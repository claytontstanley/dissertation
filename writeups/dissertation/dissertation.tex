\documentclass[man,floatsintext,donotrepeattitle]{apa6}

\usepackage[nodayofweek]{datetime}% http://ctan.org/pkg/datetime
\usepackage{xfrac}
\usepackage{textcomp}
\usepackage[american]{babel}
\usepackage[utf8]{inputenc}

% Single space after period (Mike prefers)
\frenchspacing

% http://tex.stackexchange.com/questions/36423/random-unwanted-space-between-paragraphs
\raggedbottom

% ref: http://tex.stackexchange.com/questions/28516/how-to-change-the-title-of-toc
\addto\captionsamerican{% Replace ``english'' with the language you use
  \renewcommand{\contentsname}%
  {Table of Contents}%
}

%\usepackage[compact]{titlesec}
\usepackage{tabu}
\usepackage{amssymb,amsmath}
\usepackage{setspace}

% ref: http://tex.stackexchange.com/questions/48509/insert-list-of-figures-in-the-table-of-contents
\usepackage[nottoc,numbib]{tocbibind}

\usepackage[hyphens]{url}

% ref: http://tex.stackexchange.com/questions/73862/how-can-i-make-a-clickable-table-of-contents
\usepackage{hyperref}
\hypersetup{
  colorlinks,
  citecolor=black,
  filecolor=black,
  linkcolor=black,
  urlcolor=black
}
%\hypersetup{linktocpage}

%\usepackage{breakurl}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage[group-separator={,}]{siunitx}
\RequirePackage[l2tabu, orthodox]{nag}
\graphicspath{{./figures/dissertation/}} % Specifies the directory where pictures are stored

\usepackage{csquotes}
\usepackage[style=apa,sortcites=true,sorting=nyt,backend=biber,doi=false,uniquename=false]{biblatex}
\DeclareLanguageMapping{american}{american-apa}

\addbibresource{bibliography.bib}

% Needed to ensure periods are placed at the end of every bibliography entry in the references section
\AtEveryBibitem{\clearfield{doi}}

% http://tex.stackexchange.com/questions/55780/disable-month-in-biblatex-bibliography
\AtEveryBibitem{\clearfield{month}}
\AtEveryCitekey{\clearfield{month}}

% Make sure only urls that are printed in references section are for webpages
\AtEveryBibitem{
  \ifentrytype{misc}{}{
    \clearfield{url}
  }
}

% ref: http://tex.stackexchange.com/questions/128592/remove-backslashes-from-url-fields-in-bib-entry
\DeclareSourcemap{
  \maps[datatype=bibtex]{
    \map{
      \step[fieldsource=url,
	match=\regexp{\\},
      replace=\regexp{}]
    }
  }
}

% Do not let biblatex change any of the casing in the bibliography file
\DeclareRobustCommand{\MakeSentenceCase*}[1]{{#1}}

% If the toc is too detailed, try limiting the depth of printed sections in the TOC:
\setcounter{tocdepth}{2}

\title{Comparing vector-based and ACT-R memory models using large-scale datasets:\protect\\User-customized hashtag and tag prediction on Twitter and StackOverflow}
\shorttitle{Comparing memory models of tag retrieval}
%\shorttitle{}

\author{Clayton Stanley}
\affiliation{Rice University}

\leftheader{Beitzel}

\def\committee{
  Michael D. Byrne, Chair \\
  Professor, Psychology and Computer Science \and
  Phillip Kortum \\
  Assistant Professor, Psychology \and
  David Lane \\
  Associate Professor, Psychology, Statistics, and Management \and
  Devika Subramanian \\
  Professor, Computer Science and Electrical Engineering \\
}

% Replace the title page in the apa6 style sheet with the title page from the Rice template
% The title page style was taken from Rice University LaTeX PhD template: http://scholarship.rice.edu/handle/1911/21747
% The following code for the abstract page was taken from the apa6 \maketitle command.
\makeatletter
\def\makeDisCover{
  \def\maketitle{
    \begin{titlepage}
      \singlespacing
      \null
      \vfill
      \vbox to 0pt{\vss\vbox to 8.75in{\parskip 0pt \parindent 0pt\centering
	{\uppercase\expandafter{Rice University} \par} \vskip 0pt plus 3fil
	{\bf \@title \par} \vskip 0pt plus 1.3fil
	{by \par} \vskip 0pt plus 1fil
	{\bf \@author \par} \vskip 0pt plus 3fil
	{\sc A Thesis Submitted \\
	  in Partial Fulfillment of the \\
	Requirements for the Degree \par} \vskip 0pt plus 1.1fil
	{\bf Doctor of Philosophy \par} \vskip 0pt plus 4.5fil
	\begingroup
	\leftskip .45\textwidth \rightskip 0pt plus 1fil
	{\sc Approved, Thesis Committee: \par} \vskip 2.5em
	\moveright\leftskip\vbox{\hrule width \leftskip}
	\def\and{\par \vskip 2.25em minus 1em\moveright\leftskip\vbox{\hrule width \leftskip}}
	\committee
	\par \endgroup
	\vskip 0pt plus 2fil
	{Houston, TEXAS \par} \vskip 0pt 
	{\monthnameenglish{} \number\year \par}
      }}
      \doublespacing
      \newpage
  %BDB\hyphenpenalty 10000
      \fussy
      \@ifundefined{@abstract}{}{%
	\section{\normalfont\normalsize\abstractname}% BDB
	\noindent\@abstract\par% BDB
	\@ifundefined{@keywords}{}{%
	  \setlength{\parindent}{0.4in}% BDB
	  \indent\textit{\keywordname:} \@keywords%
	}%
	\newpage
      }
      \@ifundefined{def@donotrepeattitle}{
	\section{\protect\normalfont{\@title}}
      }{}%
      \raggedright%
      \setlength{\parindent}{0.4in}%
    \end{titlepage}}
  }
\makeatother

%\makeDisCover

%\keywords{}

\begin{document}

\abstract{
  The growth of social media and user-created content on online sites provides unique opportunities to study models of declarative memory.
  The tasks of choosing a hashtag for a tweet and tagging a post on StackOverflow were framed as declarative memory retrieval problems.
  Two state-of-the-art cognitively-plausible memory models were evaluated on how accurate they predict a user's chosen tags: an ACT-R based Bayesian model and a random permutation vector-based model.
  Millions of posts and tweets were collected, and two declarative memory models were used to predict hashtags and tags that users choose on Twitter and StackOverflow.
  The results show that past user behavior of tag use is a strong predictor of future behavior, and this past behavior was successfully incorporated into the random permutation model that previously used only context.
  Also, ACT-R's attentional weight term was linked to a common entropy-weighting natural language processing method used to attenuate low-predictor words.
  Word order was not found to be strong predictor of tag use, and the random permutation model performed comparable to the Bayesian model without including word order.
  This shows that the strength of the random permutation model is not in the ability to represent word order, but rather in the way in which context information is successfully compressed.
  Finally, model accuracy was moderate to high for the tasks, which supports the theory that choosing tags on StackOverflow and Twitter is primarily a declarative memory retrieval process. 
  The results of the large-scale exploration show how the architecture of the two memory models can be modified to significantly improve accuracy,
  and may suggest task-independent general modifications that can help improve model fit to human data in a much wider range of domains.
}

\note{\today}

\maketitle

\section{Acknowledgements}

I would first like to thank my advisor, Mike Byrne, for all of his feedback and suggestions for this dissertation research
and also his excellent guidance throughout my time as a graduate student.
I would also like to thank my parents, Mike and Patsy, for the positive influence that they have provided me for my entire life.
Finally, I would like to thank my brothers Brendan and Justin, sister Kellen, extended family, and friends, for all of their support during my time in the Rice Psychology graduate program.

This work was supported in part by the Gertrude Maurin Fund.
The resources were used to purchase high-performance computing hardware and the research would not have been possible without this crucial equipment.

I would also like to mention a few open-sourced software packages that were heavily leveraged to do this research:
The majority of the modeling and analysis was done with the R statistical computing environment \parencite{RCoreTeam2014} and two important R packages:
data.table \parencite{Dowle2014} was leveraged to implement the models efficiently, and ggplot2 \parencite{Wickham2009} was used to produce the visualizations.

\newpage

\tableofcontents
\newpage

\listoftables
\newpage

\listoffigures
\newpage

\section{Chapter 1: Introduction}

The ACT-R cognitive architecture \parencite{Anderson2007} contains one of the leading theories of human cognitive processing.
The declarative memory component in ACT-R is a Bayesian statistical model.
This is the same type of co-occurrence Naive Bayes statistical model that is prominent in the more general machine learning field.
When these Bayesian models are used for machine learning, it is common to use a large number of observations to construct the co-occurrence matrix that is the foundation for the model.
The primary reason for this is that the stability of statistical models increase as sample size increases.

However, it is much less common in the cognitive modeling community to use a large number of observations to build up declarative memory when creating ACT-R models.
If a cognitive modeler is using the declarative memory component of ACT-R,
it is common to either construct the co-occurrences by hand, or use a small training set of data to initially build up the declarative memory system.
This approach can and has worked well.
Essentially the modeler is only including the observations in declarative memory that are relevant to the specific task that they are studying.

However, this approach is problematic for two main reasons:
First, it means that the declarative memory system for every developed model must be hand crafted and customized for that model, which takes enormous time and effort.
Second, it means that modelers are continuing to use small sample sizes for the declarative memory components in their models.

This has important implications for understanding declarative memory:
First, it is difficult to verify and explore modifications to the declarative memory architecture because the dataset is not large enough or rich enough to rigorously test modifications to the theory.
It also means that we do yet fully understand how well the ACT-R declarative memory theory scales when the number of chunks begins to approach the number of chunks that are actually stored in human declarative memory.

There is, however, an opportunity to change this approach.
With the massive growth of human-created content on social media sites (e.g., Twitter, StackOverflow, Wikipedia), and public access to many of those databases,
it is now possible to evaluate psychological theories of declarative memory on large-scale real-world tasks where the theory can be tested on hundreds of millions to billions of data points.
This will provide a way to test if these models scale, and evaluate the psychological validity of these models as the number of observations used increases by orders of magnitude.

A few researchers have started to use these large databases to evaluate how well declarative memory models scale,
for both ACT-R Bayesian models \parencites{Fu2007,Pirolli2003,Stanley2013,Douglass2010} and vector-based memory models \parencites{Jones2007,Rutledge2008,Recchia2010,Sahlgren2008}.

The work in this study expands on previous work that has tested retrieval models on large-scale datasets.
It focuses specifically on how two state-of-the-art declarative memory retrieval models (ACT-R Bayesian and vector-based) can be improved by testing these models on two real-world tagging tasks.
One primary objective is to compare the random permutation vector-based model to ACT-R's declarative memory theory when large datasets are used for each model. 
Another objective is to improve each model, and measure how incorporating the strengths of each model into the other (i.e., modifying model components) influences accuracy.
For example, it is currently unexplored how a vector-based model can be modified to retrieve results that are based not only on context, but also on the prior odds that a particular item in memory will be needed again.
Large-scale datasets were used throughout the model development and evaluation process.
This provides an ideal testing environment for constraints and assumptions for each theory to be rapidly tested and new model components to be hypothesized and quickly evaluated.

\subsection{The Task: Predicting User-Generated Tags}

Social media sites such as Twitter, Facebook, StackOverflow, Google+, and Yelp are composed almost entirely of human-created content.
The amount of user content on these sites is staggering, and continues to grow.
Users of Twitter's microblogging service, for example, generate half a billion tweets per day \parencite{TwitterReport2012}.
However, a single user of a social media site is most likely interested in only a small fragment of this large amount of information.
One general question to support the user in this domain is:
How can we quickly and effectively connect users to the content that they care about?

A growing number of users on social media sites are focusing on and looking for recently created information when performing a search \parencite{Jansen2011}.
In other words, users on social media sites want the fresh \emph{stream} of information about a particular topic.
This can certainly be seen in the main user interface views that these social media sites provide:
Twitter's homepage showing followees' tweets, Facebook's homepage showing friends' posts, and StackOverflow's daily digest of posts for particular tags.

So how can we ensure that users are connected to the information streams that they care most about?
One promising approach is to model and predict the user's goals when using and searching these social media sites \parencite{Rose2004}.
If we have a better understanding of a user's goals, then we can tailor the information streams to the content that is most in line with these goals.
For example, Twitter might suggest other users and hashtags for the user to follow, Facebook could suggest pages to like, Yelp could recommend businesses to check out.
Researchers are currently looking for ways to automate the goal-identification task for more general search queries like those generated for Google \parencites{Jansen2008}{Lee2005}.

However, social media sites have an advantage over traditional search engines for identifying user goals: human-created content.
Twitter has more than just the tweet that a user is currently composing to try and identify that user's goals.
The site also has all of the user's previous content, and can use that information to provide a much better prediction of a user's goals.
Further, sites like Twitter, StackOverflow, Google+, and Facebook support ways for the user to explicitly identify their precise goals when creating content: hashtags.
These user-created hashtags are a key indicator of a user's goals on a social media site. 

These hashtags relate to a user's goals and interests because they are a form of human-based document tagging \parencite{Chang2010}.
Further, people are using hashtags as a way to create information streams on social media sites \parencite{Kwak2010}.
Twitter users, for example, commonly create and use hashtags for upcoming political events that interest them, such as debates and races \parencite{Diakopoulos2010}.
So if we can predict what hashtags a user is interested in, we have a better understanding of the information streams that they care about.
We can use that information to correctly suggest hashtags that they have previously used when they are generating new content.

Further, if a particular information stream has multiple hashtags associated with it (common for Twitter), these semantically similar hashtags can also be recommended to users.
So we can tailor the system to suggest hashtags that the user may not yet know about but are of likely interest to them and growing in popularity across the site.
We can help aid in their discovery of fresh and relevant information that matches their goals on the site.

\subsection{Research Questions}

The core research question motivating this proposal is:
What kinds of cognitively-plausible models can best predict the tags that a user on a social media site uses?
This question can be tested by generating a prediction for the most likely used hashtag whenever a user is about to generate a hashtag, and then comparing the user's chosen hashtag to the model's prediction.
This process can be framed as a memory retrieval problem.
The process of suggesting a relevant hashtag to a user can be thought of as a memory retrieval request for a hashtag, given prior hashtag use and current context.
That is, each user on a social media site has created content that contains a set of tags within the context of each post.

Co-occurrence-based modeling has been shown as a potentially useful approach when predicting Twitter hashtag use \parencite{Efron2010}.
Several memory retrieval models are based on this broad methodology, where a count is maintained of the times each contextual element (such as a word in a post) co-occur with a tag.
At least two of the current memory retrieval models are based on a co-occurrence methodology:
ACT-R's declarative memory (DM) retrieval theory and random permutation vector-based memory systems.

\subsubsection{StackOverflow and Twitter}

These two memory systems were compared across two domains where users generate tags for posts: Twitter and StackOverflow.
Twitter is a microblogging service where users create 140 character tweets and broadcast those messages out to the users who follow their posts.
A user is free to annotate important concepts or words in the tweet by preceding words with hashtags ``\#.''
After the tweet is created the hashtags become hyperlinks, and if clicked on will take a user to a summary feed of all of the tweets across Twitter that have used that hashtag.
So these user-created hashtags on Twitter help connect the tweet to other tweets about the same topic through common hashtag use.
Some recent and often used hashtags for Twitter are \emph{\#photography}, \emph{\#startup}, \emph{\#4change}, \emph{\#android}, and \emph{\#solar}.

\begin{figure}[!htbp]
  {%
    \setlength{\fboxsep}{0pt}%
    \setlength{\fboxrule}{1pt}%
    \hfill
    \subfigure[]{\fbox{\includegraphics[width=8cm]{twitterTweetExample1-crop.pdf}}}
    \hfill
    \subfigure[]{\fbox{\includegraphics[width=8cm]{twitterTweetExample3-crop.pdf}}}
    \hfill
    \vfill
    \hfill
    \subfigure[]{\fbox{\includegraphics[width=8cm]{twitterTweetExample4-crop.pdf}}}
    \hfill
    \subfigure[]{\fbox{\includegraphics[width=8cm]{twitterTweetExample5-crop.pdf}}}
    \hfill
    \caption{Example tweets on twitter.com}
    \label{figTweetExample}
  }%
\end{figure}

Some example tweets on Twitter are included in Figure \ref{figTweetExample}.
Note how users both add hashtags to the end of a tweet as a summary of the message and emphasize important concepts words in the tweet by turning them into hashtags.
That is, hashtags are both intermixed within the words of the tweet and added to the end of the message.

These memory retrieval models were also tested on tag use for StackOverflow posts.
StackOverflow is a question and answer site for computer programming where users ask programming-related questions and fellow members of the community provide answers.
A user with a programming-related question creates a post with their question and tags the post with a few specific programming-related tags. 
The tags chosen by a user on the StackOverflow site represent the primary topic of the question, such as a specific programming language, tool, software package, or framework.
Some example tags for StackOverflow are \emph{PHP}, \emph{Arrays}, \emph{MVC}, \emph{C\#}, and \emph{Common Lisp}.

\begin{figure}[!htbp]
  \scalebox{.8}{\resizebox{\linewidth}{!}{\includegraphics{stackoverflowPostExample1-crop.pdf}}}
  \caption{Example post on stackoverflow.com}
  \label{figSOExample}
\end{figure}

An example StackOverflow post is included in Figure \ref{figSOExample}.
The site requires that for each post the user must associate at least one tag with the post, and the average number of tags per post is around 3.
The author used 4 tags for this post: \emph{javascript}, \emph{html}, \emph{internet-explorer}, and \emph{xhtml}.

The StackOverflow and Twitter datasets were chosen because the human-created content between them is quite different, and I am interested in retrieval models that generalize across tasks.
However, they are also similar on several accounts:
Both domains have amassed large amounts of user-created content, users generate tags for posts when creating content on the site (hashtags for Twitter and tags for StackOverflow),
and the user data from the datasets are publicly available for analysis.

Also, studying user-based tag generation on these sites has relevant real-world application.
Models that can accurately predict the tags that users will generate on Twitter and StackOverflow can be used as the foundation for recommendations systems on these very popular sites.
These systems can also help newer users by recommending proper tags for their newest content.
On the StackOverflow site, for example, experts often subscribe to specific hashtags that interest them, and receive a daily digest of posts that are tagged with those hashtags.
Helping the user properly tag posts on StackOverflow ensures that right community sees the post, which greatly increases the chance that the question on the post will be answered quickly and correctly.
For Twitter, hashtags represent streams of information that are possibly interesting to the user.
Having a recommendation system that can suggest relevant hashtags to the user provides a way for the user to connect to and discover new information streams that interest them.

\subsubsection{Comparison Between ACT-R and Vector-Based Memory Systems}

ACT-R's declarative memory and vector-based memory systems are substantially different in their mathematical structure.
Nonetheless, both can successfully model classic behavioral patterns found in word pairing experiments (e.g., \cite{Rutledge2008} modeling the ``fan effect'').

An advantage of vector-based models is that word order can be easily incorporated into the single co-occurrence representation \parencite{Jones2007}.
Word order information on sites like Twitter may contain highly predictive pieces of information, as it is likely that specific words immediately precede specific hashtags.
Word order has been incorporated into ACT-R sentence-parsing models previously, but this has not been done using declarative memory.
Rather, production rules were defined that ensured that the model followed a set of rules (e.g., left-to-right reading) when parsing sentences \parencite{Lewis2006}.
It is certainly true that at least some production rules are needed when choosing words and creating content. 
However, it is unexplored if some of the process of sentence construction and word selection can be represented in the declarative memory system.

ACT-R's declarative memory system places strong constraints on how a user's prior knowledge and experience influences the likelihood that a particular memory item is retrieved \parencite{Anderson2007}.
This decay rate equation formalizes how recency and frequency of memory use relate to the prior probability that a memory will be retrieved.
However, it is unexplored how a user's prior knowledge should influence retrieval for vector-based models.

\subsubsection{Lifetime of a Hashtag for a Specific User}

Prior research has examined the growth and decay cycle of hashtag use across users \parencite{Tsur2012}.
However, much less is known about hashtag lifetime for individual users.
It may not necessarily follow that a hashtag's life cycle for individual users matches the life cycle across users.
Further, modeling a hashtag's life within users is much more applicable to hashtag prediction, since that model can be directly incorporated into a specific user's prior likelihood of hashtag use.

\subsubsection{User-Customized Hashtag Prediction}

The end goal of this work is to identify a memory retrieval model that can suggest relevant hashtags to users when they wish to retrieve one.
These hashtags should be customized to their specific interests and relevant to the content of the post that they are currently creating.
This will undoubtedly require a combination of two primary model components:
[1] a user's prior likelihood of choosing a particular hashtag, given their previous hashtag use, and [2] the likelihood that a particular hashtag is related to the context of the post being created. 

A user's prior hashtag use will be an essential model component for domains such as Twitter, where the number of possible hashtags is effectively infinite.
In these domains, the model can utilize a user's hashtag history as a way to prune the infinite space of possible hashtags, and generate a much smaller and more manageable set for prediction.
However, not only should the model properly take into account a user's prior hashtag use, but it should also generate new hashtag predictions when a user is most likely choosing a hashtag they have never used before.
The end goal is to have a model that can properly balance the user's tag history with the contextual cues in the post in order to generate an accurate prediction.

\clearpage
\section{Chapter 2: Theoretical Background}

\subsection{ACT-R Declarative Memory Theory}

ACT-R \parencite{Anderson2007} is a cognitive architecture that formalizes how each cognitive process of the mind (e.g., memory, learning, visual and motor) interacts to produce behavior.
The declarative memory system is a component of that architecture that models the timing, learning, and forgetting processes that make up declarative memory storage and retrieval.
The equations that make up this system were derived from a rational analysis of long-term memory retrieval.
That is, given the task of retrieving a chunk of information from long-term declarative memory,
the current context (i.e., external and internal environment state), and past experience (i.e., prior memories and exposure), 
what is the optimal behavior (i.e., the optimal chunk to retrieve from memory)?
Using Bayesian reasoning, each chunk of information in declarative memory can be assigned a prior likelihood of needing retrieval again, given the prior history of exposure to the chunk.
These chunk prior probabilities are then adjusted for the current context, so that the posterior probabilities represent the likelihood that a chunk is needed, given prior odds and adjusted by current environment state.

\subsubsection{ACT-R DM Model}

A formal description of the ACT-R Declarative Memory model is included in Table \ref{tabACTRModel}.

\begin{table}[!ht]
  \caption{ACT-R declarative memory model}
  \label{tabACTRModel}
  {\tabulinesep=1.2mm
    \begin{tabu}{ll}
      \hline
      Common Name &  Equation \\
      \hline
      Activation &	 	$A_{i} = B_{i} + \sum_{j \in c}^{} W_{j} S_{ji}$ \\
      Attentional Weight &	$W_{j} = \frac{W}{n}$ \\
      Base Level & 		$B_{i} = log \sum_{j=1}^{n} {t_{j}}^{-d}$ \\
      Constant Base Level &	$B_{i} = log \frac{p(i)}{p(\overline{i})}$ \\
      Strength of Association &	$S_{ji} = log \frac{p(i|j)}{p(i|\overline{j})} \approx log \frac{p(i|j)}{p(i)} = log \frac{NN_{ji}}{N_{Row}(j)N_{Col}(i)}$ \\
      Recall Probability &	$P_{i} = \left( 1 + e^{\frac{\tau - A_{i}}{s}} \right )^{-1}$ \\
      \hline
    \end{tabu}
  }
\end{table}

The total activation ($A_{i}$) for a chunk in declarative memory is a function of two components: base-level activation ($B_{i}$) and strength of association ($S_{ji}$).
The recall probability ($P_{i})$ that a chunk will be retrieved from memory increases with total activation ($A_{i}$).

\subsubsection{Base-Level Activation}

Base-level activation reflects the log prior odds of needing an observed chunk again.
The default way to calculate these log prior odds is to use the standard base-level equation in Table \ref{tabACTRModel}.
This equation formalizes how log prior odds are a function of both frequency and recency of prior exposure to a particular chunk.
Chunks used more frequently (either through exposure or from a retrieval) are more likely to be needed for retrieval again.
However, as time progresses and a particular chunk is no longer used, the activation for that chunk decays.
In this way the standard base-level equation formalizes the time dynamics of the retrieval system, where a chunk's base-level activation evolves over time, depending on its frequency and recency of use.

For some domains it is reasonable to assume that the base-level activations of each chunk within a particular time window of interest are stable, and do not change within that window.
Programming language popularity over the past few years is a reasonable example of this.
Although the popularity of various programming languages has certainly changed slightly over the past few years (e.g., Clojure's growth), it is not the case that the changes have been drastic.
C\#, Python, and Java are still a few of the most popular languages, Common Lisp has not gained or lost much ground in popularity over the past few years.

Further, if this is a reasonable assumption to make, it greatly simplifies the computation of the base-level activation for chunks, and can turn the computation into a tractable problem for large datasets.
In these time-constant domains, one can compute the log prior odds of needing each chunk directly as the log-odds ratio of exposure to that chunk compared to exposure to all other chunks.
For example, if the StackOverflow tag \emph{PHP} has been used four times as often as the tag \emph{Common Lisp}, then the prior odds of needing \emph{PHP} again is $\frac{.8}{.2}=4$ times that of \emph{Common Lisp}.

\subsubsection{Strength of Association}

Strength of association ($S_{ji}$) reflects the amount of log-odds adjustment to the activation of a chunk, given the current context (i.e., external environment and internal state).
Context for the StackOverflow domain for example is represented as each word in the title and body of a post.
Context for the Twitter domain are the words in a tweet.
Association strength between a chunk in memory and a single contextual element can be computed directly by calculating its context-adjusted odds ratio:
the likelihood a chunk occurred with the current context ($p(i|j)$) over the likelihood that the chunk occurred in any of the other contexts ($p(i|\overline{j})$).
For large datasets, the likelihood that a chunk occurs in any particular context reaches near-zero values ($p(j) \Rightarrow 0$).
So it can be assumed that the likelihood that a chunk occurs in any context but one ($p(i|\overline{j})$) is equivalent to the likelihood that a chunk occurs in any context ($p(i)$).
This assumption was described when deriving the $S_{ji}$ equation \parencite{Anderson1989}, and used in recent research working with large-scale datasets \parencites{Stanley2013,Farahat2004,Douglass2010}.

Using this assumption for large datasets ($S_{ji} = log \frac{p(i|j)}{p(i)}$), the interpretation of the context-adjusted odds ratio becomes much simpler.
If this ratio for a particular tag and context chunk is greater than one, then the log is positive, which means that this context has been observed more often with this tag than in general.
If this ratio is less than one, then the log is negative, which provides a negative adjustment of total activation since this context has been observed more often in general than with this particular tag.

\subsubsection{Connection to Pointwise Mutual Information}

Pointwise Mutual Information (PMI) is another co-occurrence index that measures strength of association between two terms \parencite{Farahat2004}.
It is based on the co-occurrence count between pair-wise observations of terms, similar to ACT-R's strength of association.
The PMI index is presented as Equation \eqref{eqPMI}.

\begin{equation}
  \label{eqPMI}
  \mathit{PMI}(y,x) = log \frac{p(y|x)}{p(y)}
\end{equation}

Further, this PMI equation is identical to ACT-R's strength of association ($S_{ji}$) equation for large datasets.
Once the number of occurrences is large enough such that the probability of observing any particular contextual element ($p(x)$) is near zero, then ACT-R's strength of association index simplifies to the PMI equation.
That is, the PMI equation \emph{is} the simplified form of ACT-R's strength of association index ($S_{ji}$).

\subsubsection{Scaling the Equations}

The majority of tag-recommendation systems use some measure of word pair associations to compute the likelihood of a tag, given current context.
As a applied concern, it is important that this measure scales well enough to handle the large volume of information present in online social media datasets.
Large social media datasets such as StackOverflow and Twitter contain on the order of millions to hundreds of millions of unique word pair associations between terms and tags.
If one wants to capitalize on all of the information present in the database to build the most accurate measure of relatedness between words and tags, then the association measure used must scale to this large size.

However, scale is also a theoretical concern.
The declarative memory system is tasked to quickly return a single chunk of information that is stored alongside millions to billions of other unique chunks given only the current context and past history of use.
To do this task, it seems plausible that a model of declarative memory must use a representation that produces accurate retrievals even when large amounts of observations are included.
Further, it seems plausible that the time to compute an activation value by a computer is correlated with the time to compute an activation value by the brain.
So any model of declarative memory should also be reasonably computationally efficient even with large co-occurrence datasets.

ACT-R's strength of association ($S_{ji}$) and the equivalent pointwise mutual information index ($\mathit{PMI}_{ji}$) have been shown to scale well for large corpora.
\textcite{Douglass2010} implemented the large-scale version of $S_{ji}$ in Erlang.
The wall-clock time required for a declarative memory retrieval request scaled linearly with the size of declarative memory with this implementation.
Retrieval times were around one second with the largest declarative memory that contained over one million associations (co-occurrence pairs).

The SNIF-ACT framework \parencites{Fu2007,Pirolli2003} is also based on the large-scale version of $S_{ji}$, and scaled well when working with a large-scale search query database for internet search.

As one of the largest tests, \textcite{Farahat2004} used the PMI index based on a 118GB local database of 10 million pages of the Stanford WebBase Project.
Even at this large dataset size, the ACT-R declarative memory theory still scaled well enough for reasonably fast retrieval times for $\mathit{PMI}_{ji}$ values.

\subsection{Latent Semantic Analysis Memory Theory}

Latent Semantic Analysis \parencite{Landauer1997} (LSA) is an alternative representation of declarative memory.
Both ACT-R's Bayesian declarative memory model and LSA compute activations based on a co-occurrence matrix.
However, LSA performs further processing on that matrix in order to collapse words that have similar meaning.
The process starts with a set of N documents.
A word by document co-occurrence matrix is built by counting the number of times each word appears in each document.
The dimensionality of the full word frequency co-occurrence matrix is then reduced, while maintaining as much of the variance in the original full matrix as possible.
This process is similar to how factor analysis takes a data table and finds the most efficient way to represent that data with as few dimensions as possible.
The strength of association between two words is measured by the cosine of the two word vectors across the reduced dimension space.

Usually information is lost when the data are restructured into a smaller set of latent dimensions.
However, it was shown that this process actually improves prediction and reduces noise by allowing the frequency count of highly similar concepts (words) to be pooled together and represented by a single dimension.
\textcite{Landauer1997} tested the LSA model on the Test of English as a Foreign Language (TOEFL) task of choosing the best of four possible synonyms to a target word.
Model results were on par with a large sample of applicants to American universities from non-English speaking countries (\num{64.4}\% compared to \num{64.5}\%).
Model performance also improved when the original dimensionality of the data was reduced, but only up to a point.
Afterwards further reductions in the dimensionality of the data began to degrade performance.

\subsubsection{Singular Value Decomposition}

LSA uses the Singular Value Decomposition process to reduce the dimensionality of the word by document co-occurrence matrix.
The technique attempts to maximize the amount of variability in the data left after forcing the original dimensionality of the data to be reduced.
The best way to remove a dimension while maintaining the most variability is to look for words with highly similar co-occurrence patterns and deduce that these words represent the same concept.
For example, synonyms such as \emph{\#photograph} and \emph{\#photo} would appear interchangeably across documents, so these terms would be collapsed to a single dimension and frequency counts would be pooled.
By pooling co-occurrence counts the model is able to generate better predictions with less data.

\subsubsection{Word Order}

Although the LSA model has been shown to perform well as a way to measure similarity between two terms, one of the main criticisms is that it is a ``bag of words'' model.
This means that the model does not account for or pay attention to word order when building the base word by document matrix or consequently, when performing the SVD dimension reduction.
Other models that can account for word order have been shown to perform better than ``bag of words'' models on tasks that do not even appear to require grammatical structure such as word order to do well.
For example, \textcite{Jones2007} compared a vector-based model called BEAGLE directly to LSA, training and testing both models on the same corpus used in \textcite{Landauer1997}.
The context-only BEAGLE model (no word order) performed nearly equivalent to the LSA model at the task (\num{55.6}\% vs. \num{55.3}\%).
However, even in a multiple-choice domain where the task is to identify synonyms (very little grammatical structure in the task), adding word order when training the model improved task performance to \num{57.5}\%.
It seems reasonable to expect that task performance will improve much more on tasks where word order is used (e.g., generating a hashtag in the middle of a sentence when composing a tweet).

\subsubsection{Scaling Issues}

Another issue with LSA is that the SVD matrix decomposition technique is computationally very expensive.
As the size of the original word by document matrix increases, the SVD computation becomes more and more time consuming.
If the domains where LSA is applied contained small enough datasets where SVD computation could be computed in a reasonable amount of time, then this would not be much of a concern.
However, with large human-created datasets such as Twitter, StackOverflow, and Wikipedia, the total size of the dataset far exceeds the maximum size that can even be computed by an LSA approach.
\textcite{Budiu2007} used the first six million pages of the Stanford WebBase corpus to compare performance between LSA and PMI on the TOEFL, and the LSA model could not be implemented with this size training dataset.
Instead, they compared the measures on the TOEFL by training on the smaller Touchstone Applied Science Associates (TASA) corpus created from \num{60527} samples of text from high school to early college textbooks.
This is one of the available corpora on the LSA website and is intended to summarize the general reading up to the 1st year of college \parencite{Budiu2007}.

When \textcite{Budiu2007} compared the simpler PMI (i.e., $S_{ji}$) measure to LSA on smaller TASA-sized datasets, LSA performed much better (23\% vs. 60\%).
This was one of the primary motivating reasons that LSA was developed in the first place, since reduced dimensionality model outperformed the full dimensionality model on TASA-sized datasets.
However, once the dataset surpasses in size what is computationally feasible with LSA, the much simpler PMI measure can still be computed, and the performance for PMI becomes on par and even surpasses that of LSA.
\textcite{Budiu2007} also trained the PMI measure on the much larger Stanford WebBase corpus and tested the measure on the TOEFL.
This PMI model increased accuracy from 23\% to 51\% when training the dataset on the smaller TASA compared to the larger Stanford dataset.
Further, when \textcite{Turney2001} trained the PMI measure on the even larger AltaVista index of 350 million webpages, the accuracy on the TOEFL increased to \num{73.75}\%, which is 10\% higher than the TASA-trained LSA.
This suggests that simpler models may be preferred to more complex (yet more accurate) models in large-scale domains, since the simpler models scale and can utilize all available information to generate predictions.

\subsubsection{Cost of Incremental Updating}

Another issue with LSA is that adding additional observations to a previously-computed reduced representation requires rerunning the SVD \parencite{Farahat2004}.
Since computing the SVD is the most computationally expensive operation with LSA, it is difficult to see how LSA could be implemented in incremental domains where information is periodically added.
For Twitter and StackOverflow, it might certainly be useful to update the strength of association representation when new information arrives, especially for Twitter, where hashtag use and news content change rapidly.

\subsection{Vector-Based Memory Systems}

``Holographic'' memory systems \parencite{Plate1995} or vector-based memory systems represent a concept (i.e., a chunk) as a vector.
The representation of the concept is distributed across all elements of the vector.
This is somewhat similar to taking a column (i.e., a tag) on a word co-occurrence matrix and viewing the distribution of co-occurrence counts across the rows in the column as the representation of that concept.
However, vector-based memory systems represent information in a much more compact way than a full word co-occurrence matrix.

For vector-based systems, each word is represented by an environment vector $e_{i}$ that is nearly orthogonal to all other words' environment vectors.
The number of dimensions for these vectors is much less than the number of rows in a full word co-occurrence matrix, which is how vector-based systems represent information in a much more compact space.
Paired with each environment vector ($e_{i}$) is a memory vector ($m_{i}$) that contains the summed representation of all other environment vectors that have co-occurred with that $e_{i}$ environment vector.
These memory vectors can contain environment vectors from position-independent word co-occurrence information ($c_{i}$) and word order information ($o_{i}$), which can be combined into a single representation ($m_{i}$).
Over time, memory vectors accumulate a distributed representation of the most common environment vectors that co-occurred with them.

\subsubsection{Retrieval Process}

Retrieving the most likely chunk given context can be done in two ways: decoding and resonance \parencite{Jones2007}.
Decoding takes the memory vector for context and decodes it back into an environment vector.
The cosine between that decoded environment vector and all other environment vectors is computed and ranked, and the chunk with the environment vector with the highest cosine is retrieved.

Resonance is the opposite retrieval process, where a memory vector is created from the context, and then that context memory vector is compared to all memory vectors.
For example, assume that a user has written the word ``zend'' in a post, and she wants to assign a tag for the post.
``zend'' has co-occurred with the tag \emph{PHP} many times previously, so the unordered memory vector for ``PHP'' ($c_{\mathit{PHP}}$) contains the unordered environment vector for \emph{zend} ($e_{zend}$).
To determine the most correlated memory vector with context, a memory vector is created from the current context ($e_{zend}$).
The cosine between that context memory vector and the memory vector for \emph{PHP} will be high, since the \emph{PHP} memory vector ($c_{\mathit{PHP}}$) often contains the context environment vector ($e_{zend}$).
Since the cosine is high, the tag \emph{PHP} will most likely be returned as the vector that resonates highest with the current context.

The resonance and decoding processes are inversions of each other.
Usually both resonance and decoding will return very similar rank orderings of most likely chunks given context.
\textcite{Jones2007} recommends averaging the cosine values returned from each process for each chunk, and then rank ordering those averages to determine the chunk to retrieve.

\subsubsection{Addressing Word Order}

One of the strengths of vector-based systems is that word order can naturally be represented alongside word co-occurrence in these distributed representations.
Representing word order is a case where both the encoding and decoding operations are applied.
As a motivating example, say that one wants to encode the first and second words immediately preceding a hashtag in a tweet.
Simply counting co-occurrences of the two immediately preceding words and hashtags does not include the order information of the words in the representation.
With this representation, it would be impossible to tell the difference between 1-back words that immediately preceded the hashtag, and 2-back words that occurred just prior to these 1-back words.
Order information is lost if a simply co-occurrence counting technique is used.

With vector-based systems, order information is included by creating an environment vector ($e_{bind(i,\Phi)}$) that is a function of environment vectors for both the word ($e_{i}$) and location ($\Phi$).
This function has the useful property that the resulting environment vector and the location vector can be inverted to return the original word's environment vector.
Circular convolution and inverse circular convolution (i.e., circular correlation) are used by \textcite{Plate1995} and \textcite{Jones2007} to implement these operations.
This allows for retrieval requests such as ``playing \#$\Phi$,'' ``just read \#$\Phi$,'' and ``vector-based $\Phi$ systems.''

For example, suppose that a retrieval request is made for ``martin $\Phi$'' given that the model was trained on the TASA corpus.
Within this corpus, the phrase ``martin luther'' often occurred.
When this occurs, the word-order memory vector for martin ($o_{martin}$) will be updated by convolving a 1-back location vector with luther ($o_{martin} = o_{martin'} + \Phi * e_{luther}$).
Also, the word-order memory vector for luther ($o_{luther}$) will be updated by convolving martin with and a 1-forward location vector ($o_{luther} = o_{luther'} + e_{martin} * \Phi$).
The retrieval request for ``martin $\Phi$'' can be done in two ways: through decoding or resonance.

Using decoding, the word-order memory vector for martin is deconvolved to the right ($o_{martin} \circledast \Phi$) which returns the environment vector for luther ($e_{luther}$).
Using resonance, a memory vector for ``martin $\Phi$'' is constructed ($e_{martin} * \Phi$), which correlates highest with the word-order memory vector for luther ($o_{luther}$).
Either way, luther is returned as the most likely term to match the phrase ``martin $\Phi$.''

New vectors generated by circular convolution are uncorrelated with all other environment vectors and have the same length.
This allows the word order information and unordered context information for a chunk to be aggregated and represented in the same memory vector.
Quite simply, the different types of memory vectors can be summed to create a single memory vector representation for each chunk (i.e., $m_{i} = c_{i} + o_{i}$).

\subsubsection{Addressing Scalability}

Vector-based representations compute chunk likelihoods by performing a correlation operation directly on the memory vectors.
Each memory vector can be thought of as a column on a context word by tag co-occurrence matrix used for a Bayesian representation.
Further, the number of rows for a vector-based representation are fixed and set much smaller than the number of rows in the full word by tag co-occurrence matrix.
Since Bayesian representations have been shown to scale to over hundreds of millions of word by tag co-occurrences \parencite{Stanley2013}, the compressed vector-based representations should scale as well or better.

\subsubsection{BEAGLE}

\textcite{Jones2007} created the BEAGLE vector-based memory representation.
Each memory vector ($m_{i}$) is a summed representation of unordered and ordered word co-occurrences ($c_{i} + o_{i}$).
This model uses circular convolution and circular correlation as a way to encode and decode word order.
BEAGLE performed similar to LSA when trained on the TASA and tested on the TOEFL (\num{55.6}\% compared to \num{55.3}\%).
Further, it was shown that incorporating word order information into the same representation resulted in minimal data loss (less than 1 percent of total predictive variance).
This model provided an efficient algorithm for using a single representation for unordered and ordered information that showed performance similar to if not better than LSA.

\subsubsection{Random Permutation Model}

In order to represent word order information, a function must be used that converts an environment vector for a set of words and positions (e.g., ``stack $\Phi$'') into a new uncorrelated environment vector.
The BEAGLE model uses circular convolution and circular correlation as the encoding and decoding operations.
However, that is certainly not the only operation that can generate new uncorrelated vectors from a set of memory vectors and positions.
\textcite{Sahlgren2008} used a much simpler random permutation method for this operation.

A formal description of the random permutation model in \textcite{Sahlgren2008} is included in Table \ref{tabRandPermModel}.
With random permutations, each word environment vector ($e_{i}$) is a large sparse vector of zeros with a few one and negative one values in random locations. 
In order to create a new environment vector for a word that preceded another word in a sentence, that word's environment vector is shifted to the left one position.
This produces a new environment vector ($e_{i^{-1}}$) for the combination of the word and the position that is uncorrelated with the original word's environment vector and all other environment vectors.

\begin{table}[!ht]
  \caption{Random permutation model}
  \label{tabRandPermModel}
  {\tabulinesep=1.2mm
    \begin{tabu}{ll}
      \hline
      Common Name &  Equation \\
      \hline
      Activation &		$A_{i} = r(m_{C},m_{i})$ \\
      Memory Vector &		$m_{i} = \sum_{i \in all past} c_{i} + \sum_{i \in all past} \sum_{l \in locations} o_{i,l}$ \\
      Unordered Context &	$c_{i} = e_{i}$ \\
      Ordered Context &		$o_{i,l} = e_{i^{-l}}$ \\
      Context Memory Vector &	$m_{C} = \sum_{i \in C} c_{i} + \sum_{i \in C} \sum_{l \in locations} o_{i,l}$ \\
      Environment Vector & 	$e_{i} = rand$ \\
      \hline
    \end{tabu}
  }
\end{table}

For example, suppose that the words ``stack overflow'' appear in a sentence, and I want to represent that ``stack'' preceded the word ``overflow.''
The ``stack'' environment vector ($e_{stack}$) is shifted one to the left, producing an environment vector for ``stack'' preceding a word ($e_{stack^{-1}}$).
That environment vector is then added to the memory vector for ``overflow'' ($m_{overflow} = m_{overflow'} + e_{stack^{-1}}$), so that ``overflow'' remembers that ``stack'' has preceded it in a sentence.

A resonance retrieval for a word following ``stack'' (``stack $\Phi$'') is done by creating a memory vector out of all information in context.
In this case, assume only a single piece of order information is available, that ``stack'' precedes the word to retrieve.
So the memory vector for context is the environment vector for one-back ``stack'' ($m_{C} = e_{stack^{-1}}$).
The memory vector for ``overflow'' ($m_{overflow}$) will be correlated the highest with this context vector ($m_{C}$) since ``overflow'' contains this vector.

\subsubsection{Comparison between BEAGLE and Random Permutations}

Random permutations uses one of simplest operations possible to create a new environment vector.
This operation is much less costly than circular convolution used by the BEAGLE model, which allows random permutations to scale better and handle larger datasets.
Further, even though random permutations is a simpler representation than BEAGLE, it also performs better when trained on the same sized corpus. 

\textcite{Recchia2010} found that random permutations could use smaller vectors than circular convolution approaches (e.g., BEAGLE) to produce equivalent performance (better compression),
and also scale to larger datasets (better computational efficiency).
The vector length by performance tradeoff was tested on a simple paired associative retrieval task. 
Retrieval accuracy for the random permutation vector-based models remained high relative to the circular convolution model even after reducing each environment vector's length from \num{2048} to \num{1024}.
This indicates that vector-based representations using random permutations compress better than those using circular convolution.

To test computational efficiency, each model was trained on a subset of the Wikipedia corpus, and tested on the Test of English as a Foreign Language (TOEFL) assessment (among other tasks).
The random permutation model was trained on \num{2.33} GB corpus.
The BEAGLE model was unable to train on a corpus this large, so both models were also trained on an additional \num{35} MB subset of the Wikipedia corpus.
The random permutation model performed as well or slightly better than the BEAGLE model when trained on the smaller corpus.
The model's performance also significantly improved when trained on the much larger corpus that was computationally intractable for the BEAGLE model.
These results make a strong case to favor random permutations over circular convolution when implementing vector-based memory representations.

\subsubsection{Connection to LSA}

Both random permutations and LSA calculate word similarity on a reduced matrix.
For LSA, the original word by document matrix is compressed to a factor by document matrix and the number of factors are much less than the number of words.
For the StackOverflow and Twitter datasets, the number of documents are the number of different tags.

For random permutations, each tag is represented by a compressed vector with a length much less than the number of words.
Note that if the length of a word's environment vector was the number of unique words, then each word could be represented by associating it with a particular row in the column vector.
If done in this way, building a matrix of all words' environment vectors by tags would recover the original word by document matrix \parencite{Kanerva2000}.
However, each word's environment vector is compressed using a length much less than the number of unique words and represented by using a combination of a few one and negative one values at a unique combination of positions. 

Information is lost with this compression in both cases: by starting with a compressed representation with random permutations or by reducing to a compressed representation with LSA.
However, the way that the compression works in these two cases is not the same.
LSA uses SVD to find words that have highly correlated tag occurrence distributions (i.e., latent concepts), and then pools those co-occurrence counts into a single dimension.
A vector-based approach like BEAGLE or random permutations essentially randomizes the mapping of the words on the reduced set of dimensions, and does not take into account word similarity when generating the mapping.
Nonetheless, random permutations have been shown to perform similar to if not better than LSA when trained on the TASA and tested on the TOEFL \parencites{Sahlgren2008,Jones2007}.

This is a counterintuitive result.
It seems plausible that an SVD technique that takes into account word similarity when reducing the matrix should generate a reduced matrix that has more information than a randomly-compressed matrix.
This does not seem to be the case in testing however, as vector-based approaches like BEAGLE and random permutations perform just as well as the LSA when trained and tested on similar datasets.
Nonetheless, it has been suggested to perform an SVD reduction technique on a compressed random permutation vector \parencite{Kanerva2000}.
This approach might be worth investigating, as it would reduce computational requirements even further, and possibly improve performance by pooling co-occurrence counts from latent topics together.

\subsection{Comparison of Vector-Based Models and ACT-R}

Vector-based models can be thought of as a compressed representation of the full co-occurrence matrix used for the ACT-R declarative memory system.
Each model computes an activation for each word to retrieve, given context and (for ACT-R's Bayesian model) prior knowledge.
Since both models produce activations for each chunk, one can be swapped out for another and used as the declarative memory component of the ACT-R infrastructure.
For example, \textcite{Rutledge2007} used a variant of the BEAGLE vector-based model as the DM component of ACT-R.
\textcite{Rutledge2008} showed that an ACT-R theory with a vector-based memory system can successfully model the fan effect, which is a psychological phenomena that ACT-R's current Bayesian DM system can model as well.

The fan effect describes a behavior of human memory where memory items are more difficult to retrieve if they have occurred in a larger number of different contexts.
\textcite{Rutledge2008} used \textcite{Anderson1974} classic experimental paradigm to test the fan effect.
In the task, participants are exposed to a set of person and place pairs (e.g., ``the hippie is in the park'').
They are later asked to recall if a particular person and place pair was presented to them before.
The main manipulation is the ``fan'' of the person and place for each pair.
The fan is defined as the number of different contexts that each item in the pair is presented in.
For example, if items were presented to the participant where the ``hippie'' was both in the ``park'' and in the ``store,'' then ``hippie'' would have a fan of two.
Recall accuracy decrease and time to recall increase as total fan of the person and pair increases.

Both the ACT-R declarative memory theory and vector-based models produce the fan effect due to the effect that fan has on the co-occurrence matrix.
As a particular word (e.g., ``hippie'') is seen in more and more contexts, the co-occurrence count of the word across all contexts is spread across multiple contexts.
This means that when a retrieval for a context given the word occurs, multiple contexts have relatively high activation.
This makes it more difficult to test if a signal (e.g., if a word appears in a specific context) is higher than the background noise (e.g., all other contexts that the word has appeared in),
which makes the retrieval more uncertain.

Both vector-based and Bayesian models produce the fan effect since they both are based on a co-occurrence matrix.
However, vector-based models do further processing on that matrix, reduce its dimensionality, and use a correlation instead of a log odds value to compute the activation.
Nonetheless, vector-based models still produce the fan effect because at their core they are still computing activations based on co-occurrences.

\subsubsection{Base-Level Activation}

Vector-based models and ACT-R's current Bayesian DM model are not identical however, and these differences need to be addressed in order to better evaluate the relative benefits of the two models. 
In particular, the frequency and recency of retrievals for each chunk are not currently used for vector-based models when calculating activation.

The frequency and recency of how content has been used in the past can be a strong predictor of the future importance of that information. 
The recent content in tweets on a news site like Twitter should be more relevant, for example.
\textcite{Efron2011} tested this claim by evaluating a set of query likelihood models on returning relevant Twitter tweets from search queries.
They showed that a model incorporating recency information of the tweet produced more relevant results than models that did not pay attention to this information.

ACT-R's Bayesian model has a strong theory with respect to recency and frequency information.
It uses this information to calculate a chunk's prior activation (i.e., prior likelihood of being needed again, independent of current context).
Vector-based models compute activation based entirely on context, so chunks that resonate with context will be retrieved, irrespective to the frequency and recency of use of that chunk.
ACT-R's prior activation can also be computed for each user, so that hashtags that have been used frequently and recently for a particular user are rated with higher activation.
Vector-based models have focused on computing the associative strength of a chunk and specific contextual elements.
It is unexplored how accuracy changes for these models once the information about a user's prior behavior (e.g., ACT-R's base-level activation component) is added.

\subsubsection{Word Order}

One strength of a vector-based model over ACT-R's Bayesian DM model is that word order can be naturally represented alongside unordered co-occurrence information.
With the ACT-R model, for example, it is unclear how to represent that a particular word appeared just before a hashtag. 
Although there are certainly ways to incorporate word order into the ACT-R DM model.
For example, an additional dimension could be added to the co-occurrence matrix representing word position.
But adding word-order information to ACT-R's default Bayesian model is not common.

\subsubsection{Activation Calculation}

Vector-based and LSA retrieval models calculate chunk activation by correlating a representation of the current context with each memory item.
Bayesian retrieval models like ACT-R calculate activation by computing each chunk's posterior odd adjustment likelihood, given the context.
These two computations are certainly not equivalent, even though they are most likely correlated with one another.

Using a correlation can be problematic due to range effects in observed co-occurrences.
If there are particular words for a tag that co-occur much more often than all of the other words with that tag, that single large value diminishes the power of all other words to contribute to the correlation calculation.
This does not happen with Bayesian retrieval models because each pair of word and tag values is normalized by the total number of words that appeared with that tag and total number of tags that appeared with that word
(see the strength of association equation in Table \ref{tabACTRModel}).
The high-frequency words are often removed from co-occurrence matrix for vector-based models to reduce this effect.

\subsection{Recommendation Systems}

A model that predicts a user-chosen hashtag is a specific type of a recommendation system.
General recommendation systems have grown in need with the growth of the web.
Users on a site need quick access to specific pieces of information, and the site contains much more information than they can process or care about.
Recommendation systems are used to tailor the information presented to what is most relevant to the user \parencite{Pazzani2007}.

\subsubsection{The Netflix Prize}

The Netflix Prize \parencite{Bennett2007} is an example of a recommendation system where prior user behavior and current context was used to present the most relevant information to the user.
Netflix is an online subscription service where users can stream their favorite television episodes and movies.
Each user of the site is only interested in a small portion of the entire library of content that Netflix provides.
In order to provide a high quality user experience on the site, Netflix created a content recommendation service.
This tool recommends to the user movies and television episodes that most likely match their specific interests.

Netflix wanted to improve the accuracy of their recommendation system, so they held a competition where teams could try to improve on the accuracy of the service.
The winning team utilized a statistical model that performed a type of factor analysis to identify important dimensions in the data (e.g., drama, action, cult classic) and place values for each user along each dimension. 
This method used prior user behavior to classify a user along the dimensions of the reduced space, and current user behavior (e.g., just watched) to understand what they were interested in watching at that moment.
By using a factor analysis technique and combining these indicators, the model improved on Netflix's original recommendation system by 10\%.

\subsubsection{Recommending Followers on Twitter}

Recommendation systems based on the content of user-created posts have been successful in recommending people on Twitter for a user to follow.
\textcite{Hannon2010} built a Twitter follower recommendation system by creating a profile of each user based on the words they used in previous posts.
The recommendation system would then suggest other Twitter users to follow that had similar profiles to a particular user.
The profiles were created by generating a word frequency matrix of words by (user + followers + followees), and then providing that matrix as input into the Lucene text search engine framework.

Lucene \parencite{McCandless2010} is a commonly-used open source software tool that measures the similarity between a query and candidate documents in a database.
It uses the term frequency---inverse document frequency co-occurrence measure to quantify the similarity between a word in the query and a possible document to retrieve.
This measure is the product of two terms: the frequency count of the word in the document and the log inverse of the ratio of the number of documents that the word appeared in to the total number of documents.
The term frequency component measures the strength of association between a word and a document.
The inverse document frequency component measures how much information is contained within a specific word, independent of document.
ACT-R also uses two similar components to compute the activation of a word with a specific document.
The strength of association component of the ACT-R Bayesian co-occurrence model is a more sophisticated method for computing the term frequency component,
and ACT-R's attentional weight component of the declarative memory model can be used to measure the information content within each word.

When \citeauthor{Hannon2010}'s system recommended a set of individuals for a user to follow, it collected all of the words used in posts from that user, constructed a search query, and provided that query to Lucene.
Lucene then returned the most likely users that share similar words in posts within their Twitter network with the words provided in the query.
Using this simple technique based on a standard text search engine allowed the recommendation system to accurately predict the people that a particular user follows at 25\% accuracy.

\subsubsection{Hashtags for Recommendation Systems}

\textcite{Efron2010} showed how hashtags might be used as a type of recommendation system for Twitter.
Users constructed a query of text that represented a search for a topic that they were interested in.
The system then retrieved the most likely hashtags that were associated with that search query.
The model is based on building a word co-occurrence matrix of words by hashtags, and then uses that matrix to assign a similarity score for each hashtag when provided with a search query.
Participants graded how relevant the hashtags returned were to each search query.
The results suggest that hashtags contain useful information that help classify the gist of a post. 
Although this model does not predict the specific hashtags used when composing a tweet, a model like this could certainly be tailored for that purpose.

\subsection{Hashtag Prediction}

Recommendation models that attempt to predict a user's chosen hashtag have recently been created and tested for a few social media sites.
Google has even deployed a hashtag recommendation model to help users properly tag posts on their Google+ social media site \parencite{GoogleKeynote2013}.
A tag-suggestion tool has also been deployed for the \url{tex.stackexchange.org} and \url{superuser.com} StackExchange sites.
Models for Twitter and StackOverflow have been proposed and tested, but none have yet been deployed.
Recommendation models for StackOverflow, Google+, and Twitter will be described in more detail to follow.

\subsubsection{StackOverflow}

\textcite{Kuo2011} was the first to model tag use for StackOverflow posts.
His set of models were originally designed for next-word prediction for large text document collections.
\citeauthor{Kuo2011} examined the accuracy of three different commonly-used models for next-word prediction: a Bayesian model, a K-nearest-neighbors model, and a Latent Semantic Analysis (LSA) model.
In order to structure the models to work for StackOverflow tag prediction, the models would request the most-likely next word after a user created a post, and restrict the space of possible words to only tags.
He tested the set of next-word prediction models to generate predictions for the most likely tags that a user would choose when asking a question on the site.
Interestingly, the Bayesian co-occurrence model outperformed the more complicated and computationally expensive LSA (since it requires Singular Value Decomposition) and K-nearest-neighbors models.
This Bayesian model can accurately predict a user-chosen tag at 47\% accuracy.

% ref: http://tex.stackexchange.com/questions/53513/hyperref-token-not-allowed
\paragraph{\texorpdfstring{\textcite{Stanley2013}}{Stanley2013}'s ACT-R DM Model for StackOverflow}

\textcite{Stanley2013} also modeled a user's chosen tags for a post on the StackOverflow site.
They modified the standard ACT-R model in Table \ref{tabACTRModel} in order to more accurately predict tag use on StackOverflow.
A formal description of the modified ACT-R Bayesian model is included in Table \ref{tabModACTRModel}.

\begin{table}[!ht]
  \caption{\citeauthor{Stanley2013}'s StackOverflow tag prediction model}
  \label{tabModACTRModel}
  {\tabulinesep=1.2mm
    \begin{tabu}{ll}
      \hline
      Common Name &  Equation \\
      \hline
      Activation & 		$A_{i} = B_{i} + \sum_{j\in T}^{ } W_{j} S_{ji} + \sum_{j\in B}^{ } W_{j} S_{ji} - O$ \\
      Base Level & 		$B_{i} = log \frac{p_{i}}{1-p_{i}}$ \\
      Strength Assoc. &		$S_{ji} = log \frac{p(i|j)}{p(i))} = log \frac{NN_{ji}}{N_{Row}(j)N_{Col}(i)}$ \\
      Co-occurrence &		$N_{ji} = \sum_{}^{}{observed_{ji}}$ \\
      Observations &		$N = \sum_{}^{}{\sum_{}^{}{N_{ji}}}$ \\
      Attentional Weight& 	$W_{j} = W \frac{E_{j}} {\sum_{}^{} {E_{j}}} $ \\
      Scaled Entropy & 		$E_{j} = 1-\frac{H_{j}}{H_{max}}$ \\
      Entropy & 		$H_{j} = -\sum_{i=1}^{N}p(i|j)log\left (  p(i|j) \right )$ \\
      Offset & 			$O = \frac{1}{5}\sum_{i\in top 5}^{ } A_{i}$ \\
      \hline
    \end{tabu}
  }
\end{table}

The activation of each tag ($A_{i}$) is a function of four components:
That tag's base-level activation ($B_{i}$), contextual activation for the words in the title, activation for words in the body, and an offset term ($O$).
This model has been modified from the standard ACT-R model (Table \ref{tabACTRModel}) in three ways:
Two separate contextual components were used (words in title and body of post), an offset term was added, and an entropy measure was used to compute the attentional weight ($W_{j}$).

The offset term was added to normalize the mean activation across retrievals.
This allowed the use of a logistic regression statistical technique to calibrate the weights for each of the terms.
The title and body contextual components were separated since it seemed plausible that the words in the title would be better predictors of the tag used than the words in the body.
It turned out that the words in the body were a better predictor of tag use \parencite{Stanley2013}, most likely because there are many more words in the body than the title of the post.

The scaled entropy measure $E_{j}$ was chosen for the attentional weight ($W_{j}$) term because it has a strong theoretical foundation and it predicts well \parencite{Dumais1991}.
It is based on Shannon's Information Theory, which measures the amount of information content (predictiveness) of a word.
So the scaled entropy ($E_{j}$) of a context word can be thought of as the amount of predictiveness that that word has to \emph{any} tag.
This means that a stop word like ``the'' will have a scaled entropy measure close to 0, while a highly-predictive word like \emph{macro} will have a scaled entropy measure close to 1.
Looking at $W_{j}$, if $E_{j}$ is low then $W_{j}$ is low, meaning that the scaled entropy measure helps allocate limited attentional resources to predictive contextual elements.

\paragraph{Comparison Between \citeauthor{Kuo2011}'s and \citeauthor{Stanley2013}'s Models}

Since \textcite{Stanley2013} made several modifications to standard ACT-R model, it was more sophisticated than both the standard ACT-R model and the Bayesian model used by \textcite{Kuo2011}.
Also, it was unclear if \citeauthor{Kuo2011} used both the words in the title and the body of the post as separate components for his models, or if only the words in the title were used.

\citeauthor{Stanley2013}'s model also scaled much better than \citeauthor{Kuo2011}'s Bayesian co-occurrence model. 
Because of this, 100 times more posts were used to build the co-occurrence matrix (\num{1000000} versus \num{10000}), which significantly increased the model's performance.
After including the modifications and scaling up the model, the model can successfully predict a user's tag for a post at 65\% accuracy.

It is interesting that in both studies the best performing model was based on a Bayesian co-occurrence framework, since that is precisely the framework from which ACT-R's declarative memory system is based.
This suggests that more cognitively-plausible memory retrieval models may outperform more general machine learning approaches in this particular environment
(i.e., when predicting the user-chosen tags for human-created content on social media sites).

\subsubsection{StackExchange}

A tag suggestion tool has been deployed for the \url{tex.stackexchange.org} \parencite{LatexTags2013} and \url{superuser.com} \parencite{SuperUserTags2013} StackExchange sites.
Latex, SuperUser, and StackOverflow are all part of the broader StackExchange network of question and answer sites, and they provide the same user interface and workflow when asking questions.
For the two sites where a tag suggestion tool has been deployed, once a user has created a question, suggested tags appear just below the text field where the user types the tags to associate with the question.
The post author is not forced to use the suggested tags, and is free to choose tags from them or tags not represented in the suggested set.
An example of the interface for a created post on the SuperUser site is included in Figure \ref{figSuperUserSuggestion}.

\begin{figure}[!htbp]
  \scalebox{.9}{\resizebox{\linewidth}{!}{\includegraphics{superUserTagSuggestion-crop.pdf}}}
  \caption{Example of tag suggestions when creating a post on the SuperUser site}
  \label{figSuperUserSuggestion}
\end{figure}

The exact tag-recommendation algorithm used for the Latex and SuperUser sites is not public.
After exploratory testing, StackExchange appears to be using something more sophisticated than simply looking for words in the title and body of the post that are tags.
The Latex and SuperUser sites are not nearly as popular as StackOverflow currently.
The sites have roughly \num{50000} and \num{189000} questions respectively compared to \num{5000000} questions on StackOverflow.

So it stands to reason that the algorithm is relatively complex, and there could be possible scaling and efficiency concerns with the algorithm's implementation, so it has not yet been deployed to StackOverflow.
Or it is also possible that StackExchange decided to try out a tag recommendation system on a few of their smaller sites first, before deploying this type of system on their most popular StackOverflow site.
Nonetheless, it is encouraging that StackExchange finds tag recommendation models beneficial to the user and has started to add them to their question and answer sites. 

\subsubsection{Google+}

The hashtag recommendation system deployed for the Google+ microblogging service is probably the most ambitious model of user-created hashtags to date \parencite{GoogleKeynote2013}.
The user interface for this recommendation system for an example Google+ post is shown in Figure \ref{figGoogle+Hashtags}.

\begin{figure}[!htbp]
  \scalebox{.4}{\resizebox{\linewidth}{!}{\includegraphics{google+HashtagExample-crop.pdf}}}
  \caption{Example post on Google+}
  \label{figGoogle+Hashtags}
\end{figure}

The workflow and user interface provided for recommending hashtags for Google+ is similar to how a hashtag recommendation system for Twitter might look.
When generating content on Google+, the system automatically associates the most likely relevant hashtags with that post.
For the example in Figure \ref{figGoogle+Hashtags}, the system recommended \emph{\#SFGiants} and \emph{\#BusterPosey} as hashtags for the user-created post.
The user is free to remove any or all of the tags that the system has associated with the post, and can also use tags that were not included in the recommended set.

Google+'s tag-recommendation algorithm is not public, but it appears to be rather sophisticated.
Looking at the example post in Figure \ref{figGoogle+Hashtags}, \emph{\#BusterPosey} is suggested without having that specific word in the post.
Also \textcite{GoogleKeynote2013} mentioned that the algorithm uses Google's sophisticated image recognition software to suggest hashtags for images that are included in the post (e.g., \emph{\#EiffelTower}),
even when the post does not have any keywords to provide additional context.

Google is apparently highly confident in the accuracy of their system.
On the Latex site, the user interface provides subdued suggestions to the user, but requires the user to explicitly choose each tag he/she wants to associated with the post.
For Google+, the user interface automatically tags a user's post, and then requires the user to explicitly change those tags if he/she wants to use a different set.
If tag accuracy is high enough, the Google+ technique is certainly favorable since the user does not have to spend time figuring out which tags best represent this post.
However, if the model makes even just a few errors, users may become annoyed and frustrated with the recommendation system since they will have to explicitly correct it every time they generate a post.

\subsubsection{Twitter}

Several hashtag recommendation models for Twitter have been developed recently.
One of the first models used a Bayesian co-occurrence statistical technique to predict the most likely hashtag associated with a post as a function of prior hashtag use and context \parencite{Mazzia2009}.
The model presented by \textcite{Mazzia2009} is very similar to an ACT-R declarative retrieval model.
The main potential difference is that --- like \textcite{Kuo2011} and \textcite{Stanley2013} --- the global prior likelihood of a hashtag is computed without taking into account the specific user's past history.
Instead, the global prior is an overall average frequency of hashtag use across all users.

Several other models have taken a tweet-centered approach to hashtag prediction, where suggested hashtags are collected from hashtags used from similar tweets.
\textcites{Li2011, Zangerle2011, Kywe2012} all store a content vector for each tweet, and then compute a word co-occurrence-based similarity score between a composed tweet and the rest of the tweets in the database.
\textcites{Zangerle2011, Kywe2012} use the Apache Lucene infrastructure to compute the similarity score, while \textcite{Li2011} uses a custom similarity score to compute similarities.
Regardless of the method to compute the similarity between tweets,
each method collects the hashtags used from a set of the most similar tweets, then ranks the hashtags in the set, and presents the top 5-10 hashtags to the user.

This tweet-centered approach is quite different than the hashtag-centered approach presented in \textcite{Mazzia2009}.
With a hashtag-centered approach, a direct association is built between the words in a tweet and hashtags that occur in tweets.
With a tweet-centered approach, that association between words and hashtags is indirect:
The relationship is built between a tweet and its content, and then similar tweets are assumed to use similar hashtags.
One problem with the tweet-centered approach is that the storage size grows with the number of tweets, as a representation of the content in each tweet has to be maintained to later compute tweet similarity.
The storage size for a hashtag-centered approach grows with the number of hashtags, but it seems reasonable that this will grow slower than the number of tweets and asymptote after collecting a large sample of tweets.
So from the perspective of efficient information storage and retrieval, it seems more likely that the hashtag-centered approach used in \textcite{Mazzia2009} will scale as Twitter grows in size and use.

\textcite{Kywe2012} also customized their tag prediction model to the user's past hashtag use, and was one of the first to do so when developing a hashtag-prediction model for Twitter.
The set of recommended hashtags was the union of hashtags used in similar tweets and hashtags used by similar users.
This was done by storing a content vector of hashtag use for each user (user-centered approach) alongside the content vectors of hashtag use for each tweet (tweet-centered approach).
To generate recommended hashtags, the model would return the top-ranked hashtags from the hashtags used in 0-50 similar tweets and 0-4 similar users.
Prediction accuracy improved when the model combined recommendations based on hashtags in both similar tweets and the top few similar users compared to basing recommendations on similar tweets alone. 

A topics-based approach was used by \textcite{Godin2013}, where the model predicted the most likely set of topics that were associated with a tweet, and tested if these topics matched the tweet by using human raters.
The latent topics for the collection of tweets were generated using a Latent Dirichlet Allocation (LDA) statistical methodology.
This is similar to a factor analysis approach in that the most likely hidden dimensions (i.e., topics) of the data are discovered by searching for reduced representations of the data that cover most of the variance. 
Human graders rated how well the models generated topics for a tweet related to the content in the tweet.
The LDA model performed well, and generated relevant topics.
When the model produced five most-likely hashtags for each tweet, 80\% of the time at least one of those hashtags was suitable (as determined by human raters).

However, a topic-generating model is not performing the same task as a hashtag-prediction model.
The authors argued that these general topics could be used to categorize the post, in addition to the hashtags already used by the author in the post.
It is unclear though if a user would want to label their tweet with these general topics.
Perhaps they would rather use more specific hashtags to be more precise in how they label the tweet and how that tweet is associated with tweets within the rest of the community.
In fact, they have already chosen to be more precise in the specific labels that they use for the tweet, since they chose to label the tweet with specific hashtags instead of general topics.
Now it is certainly the case that the topics generated by an LDA model may be similar to the hashtags used in the post.
However, it seems a bit indirect to build a model that predicts general topics, since embedded in the data are the exact ``topics'' (i.e., hashtags) that each user chose to use for each tweet.
On the other hand, a topics-based approach is more general than a hashtags-based approach, since topics can be suggested for posts that do not use hashtags (which is quite common across Twitter).

\subsubsection{State of Research on StackOverflow and Twitter}

\textcite{Stanley2013} used an ACT-R based Bayesian memory model to predict human tagging on the StackOverflow site, but to our knowledge no vector-based memory model has been tested on the site.
Both types of models have been successfully incorporated as the declarative memory component of the ACT-R cognitive architecture \parencite{Rutledge2007},
so it should be straightforward to test a vector-based model on the StackOverflow site.

Also, a vector-based retrieval model has not yet been tested on Twitter.
One of the advantages of vector-based models is that word order can be easily incorporated into the single co-occurrence representation \parencite{Jones2007}.
It seems quite likely that words used just before a hashtag in a tweet (combined with their position) will be highly predictive of the hashtag chosen.
To our knowledge, word order has not yet been taken into account for any of the Twitter hashtag-prediction models that have already been tested.

There has been a mix of co-occurrence models created to predict hashtag use on Twitter, and the results are encouraging.
However, there has been very little research on how a specific user's past hashtag use should influence the model's prediction when that user is composing a tweet.
\textcite{Kywe2012} did take a user-centered view with their model, but the hashtags predicted by the user and content were analyzed separately and then the top from each group were combined for prediction.
The ACT-R declarative memory retrieval theory shows how past user behavior and current context combine to produce the most likely retrieval.
Each component is summed together to generate a total activation, and then the likelihood of chunks are ranked by that summed activation.
This technique is compensatory and allows for a more natural combination of the two components, where weights can be assigned to each component depending on how strongly each term predicts performance.
This type of weighted additive model that combines past user behavior and current tweet context to predict the most likely hashtags has not yet been explored for Twitter.

Both StackOverflow models \parencites{Stanley2013,Kuo2011} did not take into account the specific user's prior history when generating tag predictions.
Rather their models, much like \textcite{Mazzia2009}'s Twitter model, uses the aggregate tag history across all users as the model's prior activation for each tag.

As some initial research, I have developed and tested a user-customized model for StackOverflow by modifying the model in \textcite{Stanley2013}.
To incorporate the user's prior tag history into the model, the base-level activation term ($B_{i}$) was modified from the original equation in ACT-R.
$B_{i}$ is based on log-odds occurrence, which in turn is based on frequency of tag use, or probability of tag use $p_{i}$.
To incorporate a user's prior, a weighted average for $p_{i}$ was used,
where the probability of using tag $i$ ranges from the global probability ($pGlobal_{i}$) to the user's specific probability ($pUser_{i}$) as the user's total tag count increases.

The global probability is the same $p_{i}$ used in \textcite{Stanley2013}'s study.
The user's specific probability is computed the same way that the global probability is computed (i.e., the log odds that the tag occurred relative to the occurrence of all other tags).
However, each user's probability is constrained to the set of tags that the specific user has used in the past. 
When a user has no past tag history, the global prior probability is used.
Once a user has provided at least five tags, that user's specific prior probability is used.
Between zero and five tags, both terms are used but the weight of the global prior relative to the user prior decreases as the number of tags increases.

Both linear decay and exponential decay transition functions were tried.
Using either transition function, overall model accuracy improved by 5\% after incorporating the user's prior tag history into the model.

This research suggests that model fit can improve when tag predictions are customized to the specific user's past tagging history.
It also seems plausible that model accuracy can improve an even greater amount by using the time-dependent form of the base-level component in Table \ref{tabACTRModel},
in order to account for the fact that a user's tagging preferences change over time.

One of the main purposes of this research is to improve on these user-agnostic statistical models and include each specific user's past tagging history as a model component.
Using customized priors that are tailored to a user's specific tagging history should be crucial for sites like Twitter where the possible tag space is infinite,
but should also improve performance for more constrained sites like StackOverflow since each user is not interested in all of the possible tags on the site. 

\clearpage
\section{Chapter 3: Overall Method}

\subsection{Models}

Two models were evaluated for hashtag prediction on both the Twitter and StackOverflow datasets: a vector-based model and Bayesian retrieval model.
The random permutation model \parencite{Sahlgren2008} in Table \ref{tabRandPermModel} was used for the vector-based model, since it has shown to perform better than the BEAGLE model \parencite{Recchia2010}.
The modified ACT-R Bayesian model \parencite{Stanley2013} in Table \ref{tabModACTRModel} was used for the Bayesian model, since it has already been applied to tag use on StackOverflow posts.

The two memory models were evaluated on predicting user-chosen tags for Twitter tweets and StackOverflow posts.
The co-occurrence matrices used for the context component of each model were generated from a large subset of the Twitter and StackOverflow datasets.
Each model was tested by evaluating the accuracy of model-chosen tags on a fresh test subset of the datasets.
Accuracy was assessed by comparing model performance with the human data.

\subsection{Datasets}

The most recent quarterly published StackOverflow dataset \parencite{DataDump2014} was used to test the models on user chosen tags for posts.
This dataset contains around \num{5.7} million posts and \num{34377} unique tags.

Unlike StackOverflow, the entire Twitter dataset is not released to the public.
Due to this and the scale of Twitter, only the part of the Twitter dataset relevant to this research was collected.

\subsubsection{Popular Users Dataset}

A popular-users dataset was collected for both StackOverflow and Twitter.
This dataset consists of all of the tweets and posts from various samples of the top users for each site.
This dataset was collected by fetching all past tagging history for each of the top users.
It was used to explore the growth and decay characteristics of a hashtag for individual users.
The temporal dynamics of ACT-R's decay rate model for a chunk were evaluated against this data.

\subsubsection{Popular Hashtags Dataset}

A additional popular-hashtags dataset was collected for Twitter.
The purpose of this dataset is to test the accuracy of each model's predicted most likely hashtag(s) for a tweet.
The dataset contains approximately three million tweets where at least one of the 400 most popular hashtags are used.
A quota of three million tweets was selected because previously research testing Bayesian co-occurrence models on large-scale datasets has used on the order of one million documents
\parencites{Stanley2013,Douglass2010,Budiu2007,Kuo2011}.
Tweets were collected in real time until the tweet quota was reached.

Four popular-hashtags datasets were collected at four different time periods, spanning across one month.
Multiple datasets were collected to ensure that the results were not dependent on a particular sample of hashtags and would generalize across different popular-hashtag datasets and different time periods.
Each dataset required around four days to collect enough tweets to reach the quota, and one collection was done each week.

\subsubsection{Randomly Sampled Dataset}

Since the entire dataset is available for StackOverflow, randomly-sampled posts were used to test the context models rather than creating a popular-tags dataset (analogous to Twitter).
If all Twitter data were available, then the models would have been tested on a randomly-sampled dataset for Twitter as well.
However, since this is not the case for Twitter, the popular-hashtags dataset was used instead of a randomly-sampled dataset for that site,
while models for StackOverflow were tested on the more challenging dataset where posts were randomly sampled across all posts created on the site.

\subsection{Acquiring Datasets}

\subsubsection{StackOverflow}

The entire StackOverflow dataset is packaged and released quarterly to the public.
So the most recent packaged dataset was downloaded and the database tables were imported into PostgreSQL.

\subsubsection{Twitter}

Twitter provides two types of APIs for acquiring data: a query-based and streaming interface.
The Tweepy Python package provides a Python functional interface to these APIs, and this package was used to collect data from Twitter.
The query-based interface was used to collect all tweets for specific popular users.
At most \num{3000} of the most recent tweets were collected for each user, since Twitter only allows access to this number of tweets for each user.

The streaming interface was used to collect a sample of all of the tweets that included one of the top 400 hashtags at that time.
Twitter's streaming API only allows at most 400 hashtags to be monitored at a time.
Otherwise a larger number of hashtags would have been collected.
Twitter allows the streaming interface to randomly sample about 1\% of all of the real-time data that is generated on Twitter.
This sampling rate was adequate to collect around three million tweets within a few days.

\subsection{Tokenization}

A custom tokenization algorithm created by \textcite{Owoputi2013} was used to chunk the words in the text.
This algorithm was specifically created for tokenizing Twitter content.
However, during testing it was also found to be robust and well suited for StackOverflow content.
The initial model results with the new tokenizer were qualitatively very similar to the model results found in \textcite{Stanley2013} where the Python Natural Language Processing (NLP) toolkit \parencite{Bird2009} was used.
This is most likely because the language for the samples taken for both sites is informal English,
and it seams reasonable to think that tokenizing all of the informal language and characters included in Twitter is a much more difficult task than tokenizing most of the words used on StackOverflow.
So the same tokenization algorithm was used for both StackOverflow and Twitter, since performance was similar to \textcite{Stanley2013} for StackOverflow,
and it seemed beneficial to use the same algorithm for both datasets to make comparisons across datasets more equitable.

\subsection{Stemming}

\textcite{Owoputi2013}'s algorithm handles basic word stemming (e.g., removing ``ed'' and ``ing'' endings), but
with the StackOverflow dataset, it is also possible to convert synonym tags to canonical base tags, since the community maintains a tag-synonym database.
This can be thought of as semantic stemming, where two words with identical meaning are converted to the same canonical word.
So tags used for the post were converted to their root tag when possible.

\subsection{Handling Stop Words}

``Stop words'' (e.g., ``the,'' ``a,'' ``or'') are commonly used words that co-occur with all tags (i.e., have little to no discriminating power).
It is typically recommended \parencite{Bird2009} to remove stop words when building the co-occurrence matrix.
Otherwise the matrix may not scale as well since these low-predictor stop words commonly appear in the text.
Also for vector-based systems, memory vectors will become overly saturated with the environment vectors of the commonly-occurring stop words.

Stop words are usually removed by either filtering based on a predetermined list or filtering based on the high-frequency words that occur in the dataset.
\textcite{Sahlgren2008} tried both methods to remove stop words for the random permutation vector-based model, and found that the data-driven approach
(i.e., building the list based on the high-frequency words that occurred in the dataset) worked much better than using a predetermined list.

\textcite{Jones2007} also filtered the stop words for the circular convolution model,
but also mentioned that an entropy weighting technique \parencite{Dumais1991} has been used previously to attenuate (rather than remove) stop words.
This data-driven technique identifies stop words by calculating the discriminating power (i.e., entropy) of each word to predict any tag.
Each word is then weighted (not removed) based on their discriminating power.

\textcite{Stanley2013} used the entropy weighting technique recommended in \textcite{Dumais1991} for the StackOverflow dataset.
The result maps cleanly to the ACT-R DM theory by modifying each word's attentional weight ($W_{j}$) parameter.
It also seems less arbitrary than explicitly identifying a list of stop words to remove from the analysis.

All three stop word techniques (i.e., predetermined list, high-frequency filtering, and entropy weighting) were explored for this analysis.

\clearpage
\section{Chapter 4: Past User Behavior}

How does a user's prior tag use influence their likelihood of choosing that tag in the future?
This question was tested by examining how well ACT-R's base-level activation component of declarative memory predicts the tags that authors choose, given each author's past tagging behavior.
The model predicts that the more recently and frequently used tags have higher prior activations, and are more likely to be needed in the future.

\subsection{Base-level ACT-R model}

The base-level activation component from the full ACT-R declarative memory model in Table \ref{tabACTRModel} is included in Table \ref{tabACTRBLLModel}.

\begin{table}[!ht]
  \caption{Base-level component of ACT-R declarative memory}
  \label{tabACTRBLLModel}
  {\tabulinesep=1.2mm
    \begin{tabu}{ll}
      \hline
      Common Name &  Equation \\
      \hline
      Base-Level Activation &	$B_{i} = log \sum_{j=1}^{n} {t_{j}}^{-d}$ \\
      Optimized Learning &	$B_{i} = log \frac{n}{1-d} - d * log L$ \\
      \hline
    \end{tabu}
  }
\end{table}

Both terms are methods for computing the base-level activation ($B_{i}$) of a chunk in declarative memory.
The standard form has a time dependence and accounts for the fact that the rate of presentations for a given object can change over time.
The optimized learning form assumes that this rate is constant.

For both equations, $i$ represents each chunk (e.g., a word), $j$ is a specific observation for a chunk (e.g., the 2nd time that ``loop'' appears), and $n$ is the number of times each chunk has been observed.
Since the optimized learning form assumes a constant presentation rate for each chunk,
it only has to keep track of the time since the first presentation of a chunk ($L$) and the total number of instances observed for that chunk ($n$). 
It does not have to keep track of the time each specific chunk instance was observed.
Consequently, the optimized learning form of $B_{i}$ is computationally much less expensive than the standard form, and is actually set to the default in the ACT-R Common Lisp implementation.

The decay rate parameter ($d$) represents how fast the activation for an individual presentation of a chunk decays from declarative memory.
Higher values mean that chunk activation decays faster, which means that the most recently presented chunks will have the highest activation, and the total number of times that a chunk has been presented matters less.
Lower decay rate values flip the importance of frequency and recency:
Lower values mean that chunks that have occurred most frequently will have the highest activation,
and activation depends less on the specific chunks that have been observed in the recent past.

The base-level ($B_{i}$) component in Table \ref{tabACTRBLLModel} is different than the $B_{i}$ used in \textcite{Stanley2013} (Table \ref{tabModACTRModel}).
\citeauthor{Stanley2013} assumed no time dependence, no decay rate, and that the $B_{i}$ is simply a log-odds computation of the probability that each chunk is likely to be presented again, given past frequency.
If a decay rate of 0 is used on the optimized learning form of $B_{i}$, then the equation collapses to $log \left ( n_{i} \right )$,
which is a nonlinear transformation of the $B_{i}$ used by \citeauthor{Stanley2013} ($log \frac{p_{i}}{1-p_{i}}$).
Both forms maintain the same rank ordering, so they should produce similar results.
So the $B_{i}$ component used by \citeauthor{Stanley2013} can be roughly thought of as the standard form of $B_{i}$ when the decay rate is 0 (i.e., a pure frequency model).

\subsection{Method}

The StackOverflow and Twitter popular-users datasets were used to characterize tag growth and decay for a user on each site.
Each user's tag behavior from the popular-users dataset for each site was analyzed. 

Multiple samples were taken across a broad range of user types for each dataset to ensure that the model results were not dependent on specific attributes in a single sample.
For the StackOverflow popular-users dataset, a total of 12 slices of users were taken at different levels of reputation and total number of questions asked.
500 users were sampled that had a reputation higher than \num{500}, \num{1000}, \num{5000}, \num{10000}, \num{50000}, and \num{100000}.
100 users were sampled that had asked a total number of questions higher than \num{50}, \num{100}, \num{200}, \num{300}, \num{400}, \num{500}.

For the Twitter popular-users dataset, 12 slices were also taken, this time at different levels of number of followers and total number of tweets.
100 users were sampled that had a total number of followers higher than \num{1000}, \num{5000}, \num{10000}, \num{100000}, \num{1000000}, and \num{10000000},
and a total number of tweets higher than \num{100}, \num{500}, \num{1000}, \num{5000}, \num{10000}, and \num{50000}.

22 different values of the decay rate parameter were explored for each slice
(0, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.8, 2, 5, 10, 20).
The values were chosen to look at how performance changes from a pure frequency model ($d=0$), to a blend of frequency and recency ($d \approx .5$), to a pure recency model ($d=20$).
The decay rate value was varied for both the standard form and optimized learning form of $B_{i}$. 
Only values equal to or below 1 were examined for the optimized learning form, since values above 1 for this form cause a bifurcation and are not psychologically interesting.

Both the standard form and optimized learning form of $B_{i}$ were used to predict each user's specific tag history.
To do this, a user's entire tag history was collected, and then each tag instance from oldest to newest was examined.
For each of these tagging instances, the model generated an activation value for each possible tag based on that tag's frequency and recency of use. 
This means, for example, that the model always missed characterizing the first observed tag for each user, since at that point the model does not have any history of tag use for that user.
This also means that the tag space for each user is constrained and limited to the tags that each user has used in the past.

At each tagging instance, the model has activation values for a set of tags.
Model accuracy was assessed by rank ordering these activations and choosing the model's highest $N$ tags, where $N$ is the number of tags used for that specific post.
The model's chosen $N$ tags were compared to the user's chosen $N$ tags, and if a model's chosen tag was in the set of user's chosen tags, then the prediction was marked as correct.
Otherwise, it was marked as incorrect.

This process was performed across all users in each of the 12 dataset slices for both the Twitter and StackOverflow popular-users datasets.
It was repeated for each of the decay rate values and the two different forms of $B_{i}$.

\subsection{Results}

\subsubsection{Visualizing Tag Use}

To better illustrate the dynamics of each user's tagging behavior,
profile plots for a single user for both StackOverflow and Twitter are included in Figure \ref{figPriorHTByTime}.
The specific value of the hashtag used for both plots is a nominal variable and not important for this analysis.
What is interesting is the large tag space for these users.
There are bands of commonly-used tags for each user, but there are also many instances where new tags are used, and a large percentage of tags used are not in the most common bands.
For example, the StackOverflow user in Figure \ref{figPriorHTByTime} has a pronounced band where the same hashtag is chosen repeatedly over time. 
However there are many instances of other tags being used in parallel to the tag that is producing the band (that is repeatedly being used). 
The Twitter user has a few clusters where the same hashtag is chosen repeatedly but only for a brief period of time.
As time progresses, a band of choosing another hashtag is observed, and then the user switches to a new hashtag after that.
More recent posts for the Twitter user are more varied, and there is no longer a single hashtag that the user chooses repeatedly.
This helps illustrate how challenging modeling this task can be.
The model's goal is to predict at each instance what tag will be chosen, but to do so the model must predict the correct few tags from a much larger set of tags whose frequency of occurrence is changing over time.

\begin{figure}[!htbp]
  {%
    \setlength{\fboxsep}{0pt}%
    \setlength{\fboxrule}{1pt}%
    \hfill
    \subfigure[][StackOverflow]{\includegraphics[width=8cm]{HTByTime-520957-crop.pdf}}
    \hfill
    \subfigure[][Twitter]{\includegraphics[width=8cm]{HTByTime-fashionista_com-crop.pdf}}
    \caption{
      Tagging profile for a single user.
      The x-axis represents the specific hashtag a user chooses for a post and time progresses vertically.
      A point in the plot represents when a user chooses a specific hashtag for a specific post.
      A user's chosen hashtag for their oldest post that contains a hashtag is the lowest point on the plot.
    }
    \label{figPriorHTByTime}
  }%
\end{figure}

Although the tag space for the Twitter user is smaller than the StackOverflow user in these plots, this is not a consistent trend.
These plots were chosen from the set to also show that the amount of tag variation for individual users also varies: some use many tags and some use just a few.
To accurately model human performance in this domain, the model must be able to make correct predictions across all variations of user tagging behavior. 

\subsubsection{Visualizing Model Performance}

As a first method for understanding model performance, profile plots of model's predicted tags for individual users were overlaid on that user's actual tag use.
These plots for StackOverflow and Twitter are included in Figures \ref{figPriorHTMByTimeSO} and \ref{figPriorHTMByTimeT} respectively.
These are the same plots shown in Figure \ref{figPriorHTByTime} with model predictions overlaid in red. 

\begin{figure}[!htbp]
  \scalebox{.6}{\resizebox{\linewidth}{!}{\includegraphics{HTMByTime-520957-crop.pdf}}}
  \caption{Model tagging profile of a single user on StackOverflow}
  \label{figPriorHTMByTimeSO}
\end{figure}

\begin{figure}[!htbp]
  \scalebox{.6}{\resizebox{\linewidth}{!}{\includegraphics{HTMByTime-fashionista_com-crop.pdf}}}
  \caption{Model tagging profile of a single user on Twitter}
  \label{figPriorHTMByTimeT}
\end{figure}

Also shown in these figures is how model predictions change as the decay rate parameter changes.
With lower decay rate values, the model is less likely to switch from tag to tag for each post.
This makes sense because lower decay rate values lead to the model weighting frequency more than recency in the activation calculation.
These plots also suggest that model accuracy is best when a blend of frequency and recency is used (i.e., a decay rate between 0 and 20).
This will be more formally tested when model accuracy across decay rate values is analyzed across all users in each dataset slice and all dataset slices.

\subsubsection{Model Performance Across Decay Rate Values}

Moving up one level of analysis, model accuracy for all users in a single dataset slice was analyzed.
A plot of the results for one of the 12 slices, for both Twitter and StackOverflow, and for both forms of $B_{i}$ are included in Figures \ref{figPriorSOQSliceDsStd} and \ref{figPriorTwitterSliceDsStd} respectively.

\begin{figure}[!htbp]
  {%
    \setlength{\fboxsep}{0pt}%
    \setlength{\fboxrule}{1pt}%
    \hfill
    \subfigure[][Standard form]{\includegraphics[width=8cm]{visNormMean-SOQgt500r2-topHashtagPostPriorStd-crop.pdf}}
    \hfill
    \subfigure[][Optimized learning form]{\includegraphics[width=8cm]{visNormMean-SOQgt500r2-topHashtagPostPriorOL2-crop.pdf}}
    \caption{
    Model performance for a single dataset slice for StackOverflow.
  }
    \label{figPriorSOQSliceDsStd}
  }%
\end{figure}

\begin{figure}[!htbp]
  {%
    \setlength{\fboxsep}{0pt}%
    \setlength{\fboxrule}{1pt}%
    \hfill
    \subfigure[][Standard form]{\includegraphics[width=8cm]{visNormMean-TFollowgt10Mr2-topHashtagPostPriorStd-crop.pdf}}
    \hfill
    \subfigure[][Optmized learning form]{\includegraphics[width=8cm]{visNormMean-TFollowgt10Mr2-topHashtagPostPriorOL2-crop.pdf}}
    \caption{Model performance for a single dataset slice for Twitter}
    \label{figPriorTwitterSliceDsStd}
  }%
\end{figure}

Model accuracies at each decay rate value for each user in the dataset slice are included in the plots.
Each user's mean accuracy was subtracted off in order to compute a normalized mean for that user.
This was done so that the relative accuracy change for different decay rate values could be more easily visualized.
The plotting technique is a analogous to what is done when testing the main effect in a repeated measures design (i.e., each subject's mean score is subtracted off).

One large effect in all of the plots is that there is an optimal decay rate value between 0 and 20 that produces the highest model accuracy.
For the standard $B_{i}$ equation, model accuracy for a pure frequency model ($d=0$) is also worse than using a pure recency model ($d=20$), particularly for Twitter.
It makes sense that a pure recency model for Twitter is relatively more accurate than for StackOverflow, as the hashtag lifetime for Twitter is much less than a tag for a programming language on StackOverflow.

When comparing the standard and optimized learning forms of the equation, it is apparent that a pure recency model does not work very well for optimized learning.
The best-fit decay rate value for the optimized learning form is also not as clear and pronounced as it is for the standard form (less of a peak).
And the relative accuracy gained from using a pure frequency based model to a blend of frequency and recency is less for the optimized learning form than the standard form of $B_{i}$.

\subsubsection{Best-fit Decay Rate Values}

To identify the best decay rate value for the dataset slices,
each user's best decay rate value was taken and the results are plotted in histogram form in Figures \ref{figPriorSOQHistDsOL} and \ref{figPriorTwitterHistDsOL} respectively.
The best-fit values are close to the \num{0.5} default value set in ACT-R when the optimized learning form is used.
Comparing across optimized learning and the standard form, the best-fit values for the standard form are slightly higher for both the StackOverflow and Twitter dataset slices.
Comparing best-fit values will be more thoroughly tested when looking at the values across all dataset slices.

\begin{figure}[!htbp]
  {%
    \setlength{\fboxsep}{0pt}%
    \setlength{\fboxrule}{1pt}%
    \hfill
    \subfigure[][Standard form]{\includegraphics[width=8cm]{visHistD-SOQgt500r2-topHashtagPostPriorStd-crop.pdf}}
    \hfill
    \subfigure[][Optimized learning form]{\includegraphics[width=8cm]{visHistD-SOQgt500r2-topHashtagPostPriorOL2-crop.pdf}}
    \caption{Best-fit decay rate for a single dataset slice for StackOverflow}
    \label{figPriorSOQHistDsOL}
  }%
\end{figure}

\begin{figure}[!htbp]
  {%
    \setlength{\fboxsep}{0pt}%
    \setlength{\fboxrule}{1pt}%
    \hfill
    \subfigure[][Standard form]{\includegraphics[width=8cm]{visHistD-TFollowgt10Mr2-topHashtagPostPriorStd-crop.pdf}}
    \hfill
    \subfigure[][Optimized learning form]{\includegraphics[width=8cm]{visHistD-TFollowgt10Mr2-topHashtagPostPriorOL2-crop.pdf}}
    \caption{Best-fit decay rate for a single dataset slice for Twitter}
    \label{figPriorTwitterHistDsOL}
  }%
\end{figure}

\subsubsection{Aggregate Model Performance}

Aggregate best-fit decay rates were computed by taking the median of all user's best-fit decay rate values across all dataset slices.
The median was used since there were users in the popular-users dataset that did not have enough observations to generate stable predictions, and best-fit decay rate values for these users could be as high as 20.
Using the median effectively trims these unstable values from the sample, without having to define a cut point for an outlier removal process.

Aggregate best-fit decay rate values for each model are included in Figure \ref{figPriorDecay}.
Decay rate values that produced the most accurate model performance are lower for the optimized learning form (\num{0.43}) of $B_{i}$ compared to the standard form (\num{0.80}).
The optimal values for optimized learning are near \num{0.5}, which lines up with default value used in ACT-R.
So the decay rate values for the standard form are slightly higher than the default values used in ACT-R, but the values found when optimized learning is used line up nicely with the ACT-R defaults.

\begin{figure}[!htbp]
  \scalebox{.8}{\resizebox{\linewidth}{!}{\includegraphics{compareMeanDV-d--crop.pdf}}}
  \caption{
  Overall best-fit decay rate for StackOverflow and Twitter.
  Results are shown for the two Twitter and StackOverflow (SO) popular-users datasets.
  Error bars represent the 95\% bootstrapped confidence interval for the median.
}
  \label{figPriorDecay}
\end{figure}

Finally, model accuracy was analyzed across all dataset slices for StackOverflow and Twitter.
Accuracy for both sites is included in Figure \ref{figPriorAcc}. 
Aggregate accuracy was computed by taking the mean of every user's model accuracy across all users in all dataset slices.

\begin{figure}[!htbp]
  \scalebox{.8}{\resizebox{\linewidth}{!}{\includegraphics{compareMeanDV-acc--crop.pdf}}}
  \caption{
    Overall model accuracy for StackOverflow and Twitter at optimal decay-rate values.
    Best-fit decay-rate values for the standard and optimized learning model are \num{.8} and \num{.43}.
    All error bars are the 95\% bootstrapped confidence interval for the mean. 
  }
  \label{figPriorAcc}
\end{figure}

Accuracy for the standard form (\num{0.35}) is higher than the optimized learning form (\num{0.30}).
This makes sense given that the standard form does not assume equal presentation rate for each chunk, and hashtags on Twitter in particular show trends of high rate of use and then shift to low rate of use. 
The standard form of $B_{i}$ is able to more accurately account for these trends.

ACT-R sets the default decay rate value to \num{0.5}, and the best-fit value for the standard form is higher (\num{0.8}) for these datasets.
The figure shows that although model accuracy does improve for the standard form when increasing the decay rate parameter from \num{0.5} to the optimal value,
that percentage improvement is not as large as the improvement gained from using the standard form compared to the optimized learning form.
Nonetheless, model accuracy is highest when both the standard form is used and the decay rate value is increased. 

\subsection{Discussion}

Given the large tag space used by post authors on StackOverflow and Twitter, it was surprising that model performance is respectable even when only past tag history and no context is used.
This speaks to the importance of taking into account overall past tagging history, and ensuring that the prior component of activation for each tag is customized to each user's specific past tagging history.

Also apparent for these datasets is that the optimized learning form of $B_{i}$ is not as accurate as the standard form.
Further, for the optimized learning form there is not much difference between using a pure frequency-based model and a model where frequency and recency are optimally blended.
This makes sense given that the optimized learning equation in Table \ref{tabACTRBLLModel} collapses to a pure frequency model as the amount of time since the presentation of the first recorded chunk instance increases,
and also since the timespan of recorded tag use for StackOverflow and Twitter is large (years).

So it may be worthwhile to use the standard form instead of the optimized learning form when declarative memory is modeled across a broader range of tasks.
However, this should not be taken as a strong recommendation,
since the performance benefits seen in this study could simply be due to the fact that the declarative memory retrievals span across a large period of time, which is not the case for all tasks.
Nonetheless, the only real reason not to use the standard form is due to computational resource limitations.
Since this is becoming less of an issue as hardware speed increases, it may be time to reconsider the default form of base-level learning used in ACT-R.
It may be the case that the optimized learning form of the equation is no longer computationally necessary.
And if the standard form is used as the default (or simply tried in another experiment),
the results from this study suggest that the optimal decay rate value should be set slightly higher (\num{0.8}) than the default when optimized learning is used (\num{0.5}).

The decay rate parameter in $B_{i}$ determines how fast a presentation of a chunk in the past decays over time.
Higher values of $d$ mean that activation decays more quickly, which means that the chunks used more recently are the most active and the total frequency count of a chunk matters less.
Looking back at Figure \ref{figPriorDecay}, the optimal decay rate parameter when the standard form of $B_{i}$ is used is higher for Twitter than StackOverflow.
It makes sense that the lifetime of a specific hashtag on Twitter is shorter overall than the lifetime of a tag (e.g., \emph{C\#}) on StackOverflow.
This means that the decay rate for Twitter should be higher than StackOverflow, which is what is observed.

\clearpage
\section{Chapter 5: Combining Predictors}

The declarative memory models analyzed for this research consist of two main components: past behavior and context.
Past tag use was isolated in the first study to see how well ACT-R's prior component of declarative memory (i.e., base-level learning) fits each user's tag use over time.
For the rest of the studies the past behavior and context declarative memory components were combined as well as looked at in isolation.

\subsection{Overall Method}

\subsubsection{Logistic Regression}

In order to best combine these predictors, a logistic regression statistical technique similar to \textcite{Stanley2013} was used to determine the most optimal weights for each term.
Each term in the full model (see the total activation equation Table \ref{tabModACTRModel}) is weighted so that model accuracy is maximized.  
\textcite{Stanley2013} found that the optimal weights for the StackOverflow model for the prior term, title context term, and body context term were \num{.62}, \num{.93}, and \num{1.75} respectively.

The optimal weights for each model component are computed in the following way:
When a user creates a tag for a post, a retrieval request for the model is made.
For each request, the model returns a set of tags (all tags that have been observed for that user in the past) and activations associated with each tag.
An activation value is computed for each model component (e.g., prior tag use, context) for each tag.
These values are recorded, and tag instances that match what the author actually tagged are marked as a 1, while all others are marked with a 0.
This process is repeated across all posts where a user chooses a tag, and across all users in the dataset.
Once all of the results are aggregated, a logistic regression is run where the optimal weights for each model component are found that maximize the model's ability to correctly label tags as 0 or 1 based on total activation.

Model accuracy is evaluated by rank ordering all recorded tags by model activation using the optimal weights,
asking the model to tag the top N posts with a 1 (where N is the total number of recorded tagging instances in the dataset sample),
and then comparing the model's chosen tags with the used tags for each post (i.e., looking at the ``hits'').
This is equivalent to throttling the threshold in the logistic regression such that the total number of observations labeled as a 1 match the total number of recorded instances of tagging in the dataset.

For this form of model accuracy, the model labels N posts with a 1 where the labeling is not constrained in any way within each post.
That is, the labeling is relaxed across all posts, and there are no constraints where the model must label a specific number of 1s for each post based on the number of tags that the author used for that post.
So model accuracy values found with the relaxed method are actually slightly conservative, since the model may be labeling more 1s for a post than the number of tags the author used for that post.

However, this relaxed metric was used for the rest of the experiments that involve combining terms because the optimal weights found by the regression are based on using the relaxed metric.
If these weights are then used to compute the constrained form of model accuracy, biases may be introduced since the regression was not optimized to the constrained form, and weights may change as a consequence.
Also, there is no current logistic regression technique in R that can be constrained to label a specific number of 1s for each observation where a small set of 1s occurred. 
To optimize parameters based on the constrained form,
a full-enumeration numerical approach would have to be developed that systematically searches the space of coefficient weights and uses a custom objective fitting function to find the best fit.
However, this was not pursued for three reasons:
Around \num{2500} total regressions were run on these datasets,
a custom implementation in R would not likely run in compiled C,
and even if it was implemented in a compiled language it would be computationally intractable without a closed-form solution.

\subsubsection{Adding Offset Term}

This logistic regression method is a computationally efficient way to find the optimal weights for the predictors.
However, when the regression can not be constrained to label a specific number of 1s for each observation (as is the case here), to be used properly an offset term must be added as an additional model predictor.

The offset term keeps the activation for the small number of top tags for each post equal across posts.
This ensures that the logistic regression only labels a few 1s for each post.
\textcite{Stanley2013} did this by subtracting off from each post the mean total activation for the top 5 tags for that post.
However, this method is complicated since computing the total activation for a tag requires already knowing the weights for each model component that are used to compute that activation.
\citeauthor{Stanley2013} used an iterative approach to determine the weights used for the offset term, and kept iterating until those weights matched the weights returned after running the logistic regression.

A simplified version of \citeauthor{Stanley2013}'s method was used for the results for this research.
Instead of subtracting off the mean total activation for the top 5 tags for each post, the process is done in isolation for each model component.
So for the prior component, the activation value used in the logistic regression is the prior activation with the mean of the top 5 prior activations subtracted off of this value.
The same process is used for the context component.
Decoupling the offset term and computing an offset for each model component in isolation means that an iterative approach is no longer necessary,
since the optimal weights of the terms are no longer needed when computing the offset.

\subsubsection{Datasets}

Model accuracy was tested on the popular-hashtags dataset for Twitter and a random sample of posts across the entire StackOverflow dataset.
For each of the four Twitter popular-hashtags datasets, model retrievals were done across 15 runs and 500 posts within each run.
For the StackOverflow randomly-sampled dataset, 5 runs of 100 posts were used,
since the results for StackOverflow are more stable and retrievals take more wall-clock time for this dataset due to the size of the co-occurrence matrix.
All runs contained different posts, and all posts used for runs were not used to generate the co-occurrence matrix for the two retrieval models.

\subsubsection{Computing Priors}

For each randomly-sampled post for StackOverflow, the prior activation is based on that user's prior tagging history, and not on any global prior of specific tag frequency across users.
This was done so that the prior component used for StackOverflow for both the popular-users and randomly-sampled datasets was the same.

However, a custom user prior for the Twitter popular-hashtags dataset cannot be used, since the entire Twitter dataset cannot be downloaded.
The popular-hashtags dataset contains samples of a large set of various users that happened to use one of a specific set of hashtags being monitored.
So in order to generate a custom user prior for each sample in the dataset, that user's entire tagging history must be accessed through the API.
Since access rates are limited across that interface, it is infeasible to do this process for every user that appears in the Twitter popular-hashtags dataset.
So instead, the global prior tag history for all users in the Twitter popular-hashtags dataset was used to compute prior activation values for each tag.

\subsubsection{Co-occurrence Matrices}

The context component for both the random permutation and Bayesian model is based on the same co-occurrence matrix of words and observed tags.
Different size matrices were analyzed, ranging from \num{1000}, \num{10000}, \num{100000}, \num{1000000}, and \num{3000000} posts.
The matrix for \num{3000000} posts was only used for Twitter, since the number of words in a StackOverflow post is much higher than Twitter on average,
and the space and computational time required when using a co-occurrence matrix of \num{3000000} posts for StackOverflow is too high for a current high performance desktop, even with highly optimized representations.
Already \num{835500000} total co-occurring word tag observations were used to build a co-occurrence matrix of \num{1000000} StackOverflow posts.
It was also the case that \num{1000000} posts for StackOverflow was already around $1/7$ of the total posts created on the site,
so it is reasonable to assume that the results would not change greatly for StackOverflow if $3/7$ of the total posts created were used to build the co-occurrence matrix instead,
especially since \num{835500000} co-occurrences were already used with \num{1000000} posts.

\subsection{Adding Offset}

The offset component was added to both models, and model accuracy was examined before and after the addition.

\subsubsection{Method}

The four Twitter popular-hashtags datasets and the StackOverflow randomly-sampled dataset were used for the analysis.
Logistic regressions were run both with and without the offset component added to each model.
The results for each run for each dataset were collected and averaged.
The results across the four Twitter datasets were collected together, and the average was taken across all runs in all four datasets.

\subsubsection{Results}

The impact of adding the offset to each of the models for StackOverflow and Twitter is shown in Figures \ref{figContextOffsetSO} and \ref{figContextOffsetT}.
In both plots, if the model name includes ``full'' then all model components are included.
When they are enumerated, then at least one of the components is left out.
The random permutation and Bayesian models are plotted alongside each other so that comparisons across the two model types can be done easily.
So, for example, the ``standard prior model relaxed across posts without offset'' means that only the prior model component was used and the offset for that component was not subtracted off when determining activation.
As another example, the ``Bayes only orderless context'' means that only the context component of the Bayesian model was used (along with the offset), and that no order information was used for that contextual component term.

\begin{figure}[!htbp]
  \scalebox{.8}{\resizebox{\linewidth}{!}{\includegraphics{compareMeanDV-acc-OffsetSO-crop.pdf}}}
  \caption{
    Impact of adding offset component for StackOverflow.
    Error bars represent the 95\% bootstrapped confidence interval for the mean model accuracy across all runs in dataset.
    The random permutation models are represented as ``RP'' and Bayesian models as ``Bayes.''
}
  \label{figContextOffsetSO}
\end{figure}

\begin{figure}[!htbp]
  \scalebox{.8}{\resizebox{\linewidth}{!}{\includegraphics{compareMeanDV-acc-OffsetT-crop.pdf}}}
  \caption{
    Impact of adding offset component for Twitter.
    All error bars are the 95\% bootstrapped confidence interval for the mean.
  }
  \label{figContextOffsetT}
\end{figure}

Looking across the plots, including the offset significantly increases model accuracy, for all models, terms, and sites
(\num{.53} to \num{.60} and \num{.22} to \num{.45} for Bayesian and random permutation for StackOverflow, \num{.31} to \num{.29} and \num{.24} to \num{.25} for Bayesian and random permutation for Twitter),
except that the full Bayesian model without an offset was slightly more accurate on Twitter.
Accuracy is improved even when only a single term is used.
Although in this case the term is not weighted,
but using an offset still improves accuracy since the offset term ensures that only a small number of tags per posts are predicted by the model to be the chosen tags. 

The random permutation model in particular is greatly affected by the offset term, for both the StackOverflow and Twitter.
This suggests that the mean activation for the random permutation model for the top few tags in each post can vary greatly across posts.
Only if this mean activation is subtracted off will the logistic regression model generate optimal weights and predictions that are comparative to the Bayesian model when model accuracy relaxed across posts is examined.

\subsubsection{Discussion}

It is somewhat disappointing that the offset term is even needed.
If it were the case that model activation for a tag in a specific post could be interpreted as an exact activation value that did not depend on the specific post,
then the model would be able to say when, for example, the third top tag in a post is more likely to be a chosen tag than the second top tag in another post.
That is, the activation values would have meaning more than just rank ordering, and could be used to determine, for example, the number of tags to use for a specific post.
If this were the case, then adding the offset term would actually decrease accuracy, since this absolute value of the activation values would be removed.
However, since accuracy improves when adding the offset term for both models, it does not seem that absolute activation values have much meaning across posts for these datasets,
and that the relative activation values for each tag within a post is what is important.

One simple and likely explanation is that there are posts where a large set of tags have high activation,
but the author is choosing to use only a fixed small set of the top tags for each post, regardless of the activation values of the top tags.
The author determines the number of tags to use for a post based on competing goals of tagging posts an not overly saturating a post with tags.
For example, the author may be writing a question on StackOverflow about \emph{javascript}, and a large set of related tags are retrieved from declarative memory that are all above the retrieval threshold
(e.g., \emph{javascript}, \emph{dom}, \emph{svg}, \emph{html}, \emph{jquery}, \emph{css}),
but the author only chooses to use the tags \emph{javascript}, \emph{dom}, and \emph{svg}, since he has decided to tag this post with three tags, and those are the three tags with the highest activation.
Adding the offset term into the model is a way to implement a simple version of this strategy where the author is actively choosing to use only a small set of tags for each post. 

\subsection{Twitter Subsets}

Four subsets of the popular-users dataset were collected for Twitter.
This was to ensure that model results could be interpreted as effects that were not specific to a certain time period or set of popular hashtags.

\subsubsection{Method}

The four subsets were collected across a period of one month.
Each was collected sequentially, and about one subset was collected per week.
The hashtags used in each subset were chosen by scraping a site that published the current trending hashtags for each city.
Before starting to collect for each subset, the current trending popular hashtags for each US city were collected, and then 400 were randomly sampled from this set.
Only 400 were selected since this is the maximum that the Twitter API can monitor at one time.

In order to ensure that model results are not dependent on the specific Twitter subset collected,
it is important that the overall accuracy differences between the different models do not drastically shift from subset to subset.
That is, it is important that the effect size of model is much greater than the effect size of the interaction between Twitter subset and model.
A small interaction relative to the main effect means that the rank ordering of the different models remains consistent across the different Twitter subsets.
Further, a small effect size of subset relative to the effect of model means that the exact accuracy for each model remains consistent across the subsets.
To measure the effect sizes of subset, model type, and the interaction, a two-way between-subjects ANOVA was performed.

\subsubsection{Results}

Model accuracy results for components of both the random permutation and Bayesian models grouped by Twitter popular-users subset are included in Figure \ref{figContextSubsets}.
Accuracy is affected significantly by subset ($F(3, 340) = 764, p < .01$)), model ($F(4, 340) = 3617, p < .01$), and the interaction between model and subset ($F(12, 340) = 142, p < .01$).
However, the effect size of model type on accuracy is \num{8.6} times larger than the interaction term ($\eta ^ {2} = 0.770$ versus $\eta ^ {2} = 0.090$).
This shows that the performance of a model relative to the other models remains consistent across the four subsets.
Further, the effect size of model type is \num{6.3} times larger than the effect of Twitter subset ($\eta ^ {2} = 0.770$ versus $\eta ^ {2} = 0.122$).
This shows that not only is relative model performance maintained across Twitter subsets, but the exact value of model accuracy for each model remains reasonably consistent across subsets.

\begin{figure}[!htbp]
  \scalebox{.8}{\resizebox{\linewidth}{!}{\includegraphics{compareMeanDV-acc-contextStandardByGroupT-crop.pdf}}}
  \caption{Model accuracy for each of the four Twitter popular-users subsets}
  \label{figContextSubsets}
\end{figure}

\subsubsection{Discussion}

Since the effect sizes of subset and the interaction between model type and subset are minimal relative to the effect size of model,
model behavior for the popular-users subset for Twitter can be interpreted as behavior that will persist across different time periods,
if the same method of choosing popular hashtags is conducted at a later date.
It may also be the case that that model behavior does not depend on the specific method of choosing the popular hashtags, but since only a single method was used for this analysis,
the results from this study cannot by themselves be used to answer this question.

\subsection{Stop Words}

\textcite{Sahlgren2008} looked at several ways to handle the commonly-occurring stop words for the random permutation model:
A data-driven frequency-weighted approach was tried, along with using a previously-derived stoplist, and no stop word removal at all.
Removing the high-frequency words in the dataset showed the highest performance increase compared to no stop word removal, and using a previously-derived stoplist hardly increased performance.
However, \citeauthor{Sahlgren2008} did not try to weight the stop words and only tried methods to remove them.
So alongside looking at frequency weighting, previously derived, and no stop word removal, \textcite{Stanley2013}'s method of attenuating stop words based on entropy was also explored.

\subsubsection{Method}

The runs from the Twitter popular-hashtags and StackOverflow randomly-sampled datasets were used for the analysis.
Both the random permutation and Bayesian models were tested.
Activations for context were computed by using five different methods to handle stop words:
The two removal methods used in \textcite{Sahlgren2008} (i.e., removal based on frequency, removal based on the pre-determined 571-word Cornell SMART stoplist),
the entropy-weighting method used in \textcite{Stanley2013},
combining the entropy-weighting and frequency-filtering methods,
and no removal or weighting of stop words.

\paragraph{Determining Frequency Cutoff}

A cutoff point must be identified and used in order to remove the high-frequency words.
\textcite{Sahlgren2008} used a cutoff where words that occurred more than \num{15000} times were removed.
Since the StackOverflow and Twitter datasets are much larger than the datasets used by \citeauthor{Sahlgren2008}, the same cutoff could not be used.

The ideal cutoff was determined by choosing the value where the standard deviation of the counts for each word for each of the tags were all relatively small.
The plots for the standard deviations across tags that were used to identify the cutoff for StackOverflow and Twitter are included in Figures \ref{figContextCutoffSO} and \ref{figContextCutoffT}.

\begin{figure}[!htbp]
  \scalebox{.6}{\resizebox{\linewidth}{!}{\includegraphics{memMatSO-crop.pdf}}}
  \caption{Variance in observation counts of words for each tag for StackOverflow}
  \label{figContextCutoffSO}
\end{figure}

\begin{figure}[!htbp]
  \scalebox{.6}{\resizebox{\linewidth}{!}{\includegraphics{memMatT-crop.pdf}}}
  \caption{Variance in observation counts of words for each tag for Twitter}
  \label{figContextCutoffT}
\end{figure}

The original plot shows the size of the spikes when high-frequency words are not removed.
The entropy plot shows how the size decreases by more than an order of magnitude after the entropy weighting measure is used (compare the difference between the original subplot and the entropy subplot).
The cutoff for the frequency plot was chosen such that the plot looks similar to the entropy plot, with spikes around the same size.
The cutoff that produced the frequency plots in Figures \ref{figContextCutoffSO} and \ref{figContextCutoffT} is where a word represents more than \num{.04}\% of total occurrences in the dataset.
This may or may not be equal to the value for \textcite{Sahlgren2008}, since that number was expressed in total observations (not as a percentage), and the total number of words in the dataset was not given.

Note that using a predefined stoplist does not remove some of the remaining high-frequency words in these datasets, which hints that this method may not be ideal when model accuracy is examined.
Also, no strong claims are being made that the cutoff identified is the ideal frequency percentage cutoff for all datasets when a frequency cutoff is used.
The method used to identify this cutoff was simply intended to find a reasonable value where comparisons between a frequency cutoff and entropy weighting techniques would be equitable.

\subsubsection{Results for the Random Permutation Model}

Plots for the three methods of handling stop words for the random permutation model on the StackOverflow dataset are included in Figure \ref{figContextStopRPSO}.
The results show that model accuracy improves if stop words are handled in any of the three ways (predefined stoplist filtering, entropy weighting, and frequency filtering).
Using a predefined stoplist makes the smallest improvement (\num{.45} to \num{.49}), while the entropy (\num{.60}) and frequency (\num{.63}) techniques improve performance the most,
and frequency shows a slight edge over entropy.
However, using the frequency-weighting technique produces more noise in accuracy measurements compared to the other techniques, at least when tested on words in the title of StackOverflow posts.

\begin{figure}[!htbp]
  \scalebox{.8}{\resizebox{\linewidth}{!}{\includegraphics{compareMeanDV-acc-allWeightingsSOPerm-crop.pdf}}}
  \caption{
    Stop-word techniques for the random permutation model for StackOverflow.
    The five methods for handling stop words are shown both for the title model component in isolation and the full model.
    ``Stoplist'', ``freq'', ``entropy'' are the stoplist removal method, removal based on frequency count, and weighting based entropy metric.
}
  \label{figContextStopRPSO}
\end{figure}

The three stop word techniques were then tested for the random permutation model on the Twitter dataset, and the results are included in Figure \ref{figContextStopRPT}.
For the random permutation model for Twitter, the entropy weighting technique is the best method for handling stop words (\num{.25} to \num{.26}).
Removing stop-words based on a predetermined list actually decreases accuracy (\num{.25} to \num{.23}), and using data-driven frequency filtering has only a small to no effect (\num{.25} to \num{.25}).

\begin{figure}[!htbp]
  \scalebox{.8}{\resizebox{\linewidth}{!}{\includegraphics{compareMeanDV-acc-allWeightingsTPerm-crop.pdf}}}
  \caption{
  Stop-word techniques for the random permutation model for Twitter.
  The five methods for handling stop words are shown both for the context model component (i.e., no prior term) and the full model.
}
  \label{figContextStopRPT}
\end{figure}

\subsubsection{Discussion}

Comparing across Figures \ref{figContextStopRPSO} and \ref{figContextStopRPT}, entropy and frequency techniques are superior to filtering based on a predetermined list.
However, it is more difficult to choose the best approach when comparing entropy weighting and frequency filtering.
Frequency filtering is slightly better for StackOverflow, while entropy weighting is slightly better for Twitter.
However, frequency filtering does have a few drawbacks compared to entropy weighting:
It produces accuracy measurements that have higher levels of noise (see Figure \ref{figContextStopRPSO} and the frequency model for the title component in StackOverflow).
Also, the ideal cutoff threshold must be identified in order to filter based on frequency,
and identifying this cutoff requires a search, and this search must be done for each dataset that frequency filtering is used on.
This cutoff value is a free parameter in frequency filtering that is not required when weighting stop words by entropy.
Therefore, favoring parsimony, for the random permutation model, attenuating stop words based on entropy is a slightly superior method compared to removing stop words based on frequency of use.
This result is interesting since filtering stop words based on frequency is the more common default method for random permutation models \parencite{Sahlgren2008}.
This suggests that a simpler method (entropy weighting) can be used instead of the more common frequency-filtering technique to achieve the same (or better) accuracy measurements.

\subsubsection{Results for the Bayesian Model}

Since accuracy measurements when using a predetermined list to remove stop words for the random permutation model were much worse than the frequency and entropy methods, this method was not examined for the Bayesian model.
The results for the frequency and entropy techniques for the Bayesian model on the StackOverflow dataset are included in Figure \ref{figContextStopBSO}.

\begin{figure}[!htbp]
  \scalebox{.8}{\resizebox{\linewidth}{!}{\includegraphics{compareMeanDV-acc-allWeightingsSOSji-crop.pdf}}}
  \caption{Stop-word techniques for the Bayesian model for StackOverflow}
  \label{figContextStopBSO}
\end{figure}

The results show that entropy weighting produces the most accurate results (\num{.61} to \num{.63}).
Also, accuracy actually decreases when using frequency filtering (compared to no filtering) (\num{.61} to \num{.56}) for the Bayesian model.
So entropy weighting is the clear winner in this case.

Similar results are found when the Bayesian model is tested on the Twitter dataset.
Figure \ref{figContextStopBT} shows that entropy weighting is again the best performer (\num{.29} to \num{.32}), and frequency filtering decreases performance relative to no filtering (\num{.29} to \num{.28}).

\begin{figure}[!htbp]
  \scalebox{.8}{\resizebox{\linewidth}{!}{\includegraphics{compareMeanDV-acc-allWeightingsTSji-crop.pdf}}}
  \caption{Stop-word techniques for the Bayesian model for Twitter}
  \label{figContextStopBT}
\end{figure}

\subsubsection{Discussion}

One result that should not be overlooked is how poorly using a predetermined stoplist performed when compared to the two data-driven frequency and entropy approaches.
This is most likely because the StackOverflow and Twitter datasets are large and contain high-frequency words that are domain specific (e.g., emoticons) and are not included in the predetermined list.
This shows that data-driven techniques to identify stop words can be much more effective than using a predetermined list.

Another interesting result was how the Bayesian model actually performed worse when stop words were removed based on frequency compared to no stop word removal.
This was not the case for the random permutation model in this study, was not the case when \textcite{Sahlgren2008} used frequency filtering on the random permutation model,
and is most likely never the case for the random permutation model.
Model accuracy most likely decreases for the Bayesian model because this model already has a normalization component built in that the random permutation model does not have.
When computing activation for the random permutation model, the correlation is calculated directly on a matrix of counts of co-occurrences for words and tags.
For the Bayesian model this count matrix is converted to a log-odds ($S_{ji}$) matrix,
where both the number of observations for the word and the tag are normalized when computing the value for each cell (see the $S_{ji}$ computation in Table \ref{tabModACTRModel}).
So it is likely the case that a form of frequency attenuation is already built into the Bayesian model.
Therefore, adding a frequency filter on top of this does nothing to improve accuracy and only removes predictive information.

\textcite{Stanley2013} found that the entropy-weighting technique to attenuate stop words for the Bayesian model significantly improved model accuracy.
The same effect is found for the StackOverflow and Twitter datasets in this study.
Further, when this entropy-weighting technique is compared directly to the more common frequency-filtering technique, weighting by entropy produces more accurate results.
This was clearly the case for the Bayesian model.
Both frequency and entropy techniques performed equally for the random permutation model.
However, since the entropy-weighting technique is parameter free and does not require a cutoff to tune, it is also a better choice for the random permutation model for these datasets.

The effect sizes when comparing no stop-word removal technique to either the entropy or frequency techniques are quite large.
This shows the importance of correctly identifying and handling stop words when building co-occurrence matrices for large-scale natural language datasets.
For further research, it may be worthwhile to explore an even larger range of stop-word handling techniques,
since model accuracy seems to be quite sensitive to the specific method used. 
However, one conclusion that can be made from this research is that the entropy-weighting method is a contender, and should be considered much more often than it has been previously.

\subsection{Dataset Size}

The default number of documents used to build the co-occurrence matrix was \num{1000000} for StackOverflow and \num{3000000} for Twitter.
In order to see how accuracy changed as function of document size, 
co-occurrence matrices for a range of smaller document sizes were built.
Both the random permutation and Bayesian models were tested on all document sizes.

\subsubsection{Method}

For StackOverflow, the following number of posts were used to build separate co-occurrence matrices: \num{1000}, \num{10000}, \num{100000}, and the default \num{1000000}.
For Twitter, these matrix sizes were tested alongside the default \num{3000000}.
All runs from the four Twitter popular-hashtags datasets and the randomly-sampled StackOverflow dataset were used to test model accuracy.

\subsubsection{Results for StackOverflow}

Model accuracy as a function of co-occurrence matrix size for StackOverflow is included in Figure \ref{figContextDocumentSizeSO}.
Accuracy improves for both the Bayesian and random permutation models as the size of the documents used to generate the co-occurrence matrix increases.
However, model accuracy using the entropy weighting technique plateaus and slightly decreases for the largest document size for the random permutation model, while the frequency technique only begins to plateau. 
The Bayesian model does not plateau and accuracy continues to improve as document size increases.

\begin{figure}[!htbp]
  \scalebox{.8}{\resizebox{\linewidth}{!}{\includegraphics{compareMeanDV-acc-freqVsEntropyBySizeSO-crop.pdf}}}
  \caption{Effect of size of co-occurrence matrix for StackOverflow}
  \label{figContextDocumentSizeSO}
\end{figure}

\subsubsection{Discussion}

It is likely that the random permutation model is starting to saturate when entropy weighting is used for the largest document size for StackOverflow.
Saturation happens when certain words co-occur with a tag much more often than all of the other words.
This introduces range effects when the correlation is computed, which ends up driving all of the correlations near one, and consequently decreases discrimination and accuracy.
Saturation happens when entropy weighting is used because the non-predictive words are only attenuated and not removed, and this attenuation is not enough to overcome the range effects that grow with document size.

However, the effect of saturation is small compared to the large effect from using either frequency filtering or entropy weighting to handle the stop words.
The Bayesian model does not saturate when using entropy weighting because each cell in the co-occurrence matrix is normalized by word frequency and tag frequency.

\subsubsection{Results for Twitter}

The overall trend of increasing accuracy with document size is present for Twitter as well.
Both entropy weighting and frequency filtering in Figure \ref{figContextDocumentSizeT} plateau for the random permutation model with increasing document size.
Once again, the Bayesian model continues to improve with document size.

\begin{figure}[!htbp]
  \scalebox{.8}{\resizebox{\linewidth}{!}{\includegraphics{compareMeanDV-acc-freqVsEntropyBySizeT-crop.pdf}}}
  \caption{Effect of size of co-occurrence matrix for Twitter}
  \label{figContextDocumentSizeT}
\end{figure}

\subsubsection{Discussion}

It is interesting to compare model accuracy between the Bayesian and random permutation model at the smallest and largest document sizes for both StackOverflow and Twitter.
At smaller document sizes, the random permutation model outperforms the Bayesian model (slightly for Twitter, dramatically for StackOverflow), and this performance difference deteriorates as document size increases.
In other words, the Bayesian model needs larger co-occurrence matrices than the random permutation model to work properly.
As the size of the dataset increases, the compression may start to lose crucial information, and the uncompressed Bayesian model should be used.
As the size of the dataset decreases, the noise and crosstalk introduced by the compression for the random permutation model is actually beneficial, and the compressed random permutation model should be used. 
This noise is beneficial because it helps soften spurious $S_{ji}$ values when the counts for each individual cell in the co-occurrence matrix are still volatile, which happens at smaller document sizes.

This result has important implications.
The document size is rarely chosen, and more likely constrained by the number of documents in the dataset being studied.
If that dataset is relatively small, it may be better to use the compressed random permutation model than the Bayesian model, and vice-versa if the dataset is large.

However, document size is not the only factor that determines whether the Bayesian model or random permutation model will perform better.
For example, the document size where the Bayesian model begins to outperform the random permutation model is quite different for StackOverflow and Twitter.
This is because the number of observations used to generate the co-occurrence matrix is a function of four components:
document size, number of words in each document, total number of unique tags, and number of tags used per document.
The average number of tags used per post for StackOverflow and Twitter were the same order of magnitude (around 3 for StackOverflow, slightly less for Twitter).

However, the average number of words in a StackOverflow post is much greater than the 140 character limit used for tweets.
More words per post means that the number of observations per cell in the co-occurrence matrix grows more quickly with document size.
The Bayesian model is more likely to perform better with datasets like StackOverflow that have these larger document sizes.

Also, the popular-hashtags Twitter dataset only contains 400 unique tags while the randomly-sampled StackOverflow dataset has \num{34377} unique tags.
More unique tags means that the co-occurrence counts are spread across a wider range of tag columns,
which means that it takes a larger number of total documents before the Bayesian model performs better than the random permutation model.

The transition point where accuracy for the Bayesian model outperformed the random permutation model was much higher for StackOverflow than Twitter.
This is most likely because StackOverflow has a almost \num{100} times more unique tags than the Twitter popular-hashtags dataset.
The number of words in a StackOverflow post is also much higher than Twitter, which should have lowered the transition point.
However, the effect of words in a post must be less than the effect of number of unique tags, as the transition point was still much higher for StackOverflow given that it should have lowered due to post size.

The compressed random permutation model should perform better with smaller document sizes, less words per document, more unique tags, and fewer number of tags per post.
However, since there are so many factors that determine whether the Bayesian or random permutation model will perform better,
and only two different datasets were examined in this research,
it is difficult to make any specific recommendations as far as exact transition cut points (e.g., document size, words per document) where either the Bayesian or random permutation model should be used.
Also, it is likely that the specific domain being studied interacts with the factors already mentioned to determine which model is better for that domain.
Nevertheless, if a researcher is working in a domain with limited data (e.g., small document size, large tag space) that produces a sparse co-occurrence matrix,
then it may be worthwhile to experiment with using a compressed random permutation model instead of the full Bayesian model.

\subsection{Amount of Compression for Random Permutation}

Another parameter in the random permutation model is the number of rows used to represent the all of the different words.
Each word's environment vector is represented by two randomly placed ones and two negative ones across the rows.
All that is required is that the four positions and signs of each of these environment vectors is unique.
But this means multiple environment vectors can overlap when looking at a single row.
This is a lossy form of compression, and allows the number of rows to represent the words to be much less than the total number of words.
As the number of rows decreases, the amount of cross talk between the environment vectors increases, compression increases, and the amount of information stored decreases.

\subsubsection{Method}

Three different row-size values were tested for the random permutation model to see how accuracy changed as a function of space required to represent the co-occurrence matrix.
A low value (\num{100}), a more standard value (\num{2048}), and a high value (\num{10000}) of rows were tested.
A separate co-occurrence representation was built for each of these row values.
Each representation was tested on the four Twitter popular-hashtags datasets and the StackOverflow randomly-sampled dataset.
Accuracy measurements were the same as were used for previous runs of these datasets: mean accuracy for each of the 5 StackOverflow and 15 Twitter runs.

\subsubsection{Results}

Results for using different levels of compression for the random permutation model for the StackOverflow dataset are included in Figure \ref{figContextDimSO}.
In the figure, the model name that does not have the number of rows in the matrix specified is the \num{2048}-row matrix.
This was done so that names in all of the plots would remain consistent across all plots, and the \num{2048}-row matrix is the size used for the random permutation model for all other results.
The plot is also broken out for different size datasets used to build the co-occurrence matrix.

\begin{figure}[!htbp]
  \scalebox{.8}{\resizebox{\linewidth}{!}{\includegraphics{compareMeanDV-acc-dimBySizeSO-crop.pdf}}}
  \caption{Effect of compression on random permutation model for StackOverflow}
  \label{figContextDimSO}
\end{figure}

Reducing compression to only 100 rows in the representation has a strong negative effect on accuracy (\num{.60} to \num{.55} for the \num{1000000} document dataset size).
But \num{2048} rows is sufficient for these datasets,
as increasing the number of rows to almost an order of magnitude higher (\num{10000}) has little to no positive impact on accuracy (\num{.60} to \num{.60} for the \num{1000000} document size). 

Results are similar for Twitter.
Figure \ref{figContextDimT} shows the same negative impact when reducing from \num{2048} to \num{100} (\num{.26} to \num{.24} for the \num{3000000} document size),
and little to no impact when increasing from \num{2048} to \num{10000} (\num{.26} to \num{.26} at \num{3000000}).
This results are consistent across all document sizes used to build the co-occurrence matrix.

\begin{figure}[!htbp]
  \scalebox{.8}{\resizebox{\linewidth}{!}{\includegraphics{compareMeanDV-acc-dimBySizeT-crop.pdf}}}
  \caption{Effect of compression on random permutation model for Twitter}
  \label{figContextDimT}
\end{figure}

\subsubsection{Discussion}

The fact that the random permutation model only needs around \num{2048} rows to represent all of the words in the Twitter and StackOverflow datasets is impressive.
Twitter has around \num{2383000} unique words for the \num{3000000} tweets in each of the popular-users datasets.
StackOverflow has \num{5848000} unique words for the \num{1000000} posts used to generate the co-occurrence matrix.
So it is impressive that only \num{2048} rows are needed to represent most of the variance from all of the words in these co-occurrence matrices.
This is likely because word use frequency approximately follows Zipf's law \parencite{Stanley2013}.
This means that if the words are rank sorted by number of times they are used, the frequency of word use drops off sharply when moving to words that are used fewer times. 
Since these words have little to no ability to influence predictions, it makes sense that they can be safely removed from the dataset.

However, it should be pointed out that the way the random permutation model compresses the words is completely random.
It is not an LSA approach where the few ones and negative ones (i.e., the factors) for each word are chosen such to maximize the ability for the compressed matrix to represent the full matrix.
The environment vector for each word is randomly selected, and no SVD is performed to choose these in a more discriminating manner.
So it is that more impressive that even with a randomly-chosen form of compression, only \num{2048} rows are needed to represent all of the words in these datasets.

\subsection{Method to Combine Prior and Context}

The activation value for the context component of the vector-based model is a correlation, ranging from zero to one.
This is not a log-odds value.
For the previous results, prior and context for the random permutation model were combined by simply adding the terms.
However, this means that ACT-R's prior base-level learning component (which is a log-odds value) is added to a correlation value, which may not be mathematically appropriate.
It would be more appropriate to convert the correlation value to a log-odds value, and then add this context term to the log-odds prior term.

\subsubsection{Method}

For a given retrieval, the random permutation model returns a set of activations that are correlation values for each of the tags.
So a method was created to convert a distribution of correlations to a distribution of log-odds values.
To do this, the one-tailed cumulative distribution function value for each correlation in the distribution was computed.
That probability was then converted to a log-odds value in the usual way. 

This technique has a few advantages: 
It is simple and computationally cheap.
Also, it behaves properly at specific points in the distribution.
For example, if the conversion is performed on the mean correlation value in the set, the result is a log-odds value of zero.
This makes sense given that log odds can be interpreted as the amount of odds information (positive or negative) that should be adjusted to the prior log odds, given that new piece of information.
Since the mean correlation value contains no information regarding if it is better or worse than the other correlation values, it is appropriate to assign this correlation value a zero log-odds adjustment value.

The log-odds transformation technique was tested for the random permutation model on the Twitter popular-hashtags and StackOverflow randomly-sampled datasets.

\subsubsection{Results}

Model accuracy results after using the log-odds transformation are included in Figures \ref{figContextLogoddsSO} and \ref{figContextLogoddsT}.
Using the log-odds transformation technique has a small effect on model accuracy for StackOverflow (\num{.45} to \num{.47}) and minimal to no effect on Twitter (\num{.25} to \num{.25}).
When the technique is used and accuracy is examined for a model with only a single predictor, the transformation does not change accuracy.
When predictors are combined and the full model is examined, the transformation is in the direction of slightly increasing accuracy, at least for the StackOverflow dataset.
However, any improvement when using the transformation is minimal.

\begin{figure}[!htbp]
  \scalebox{.6}{\resizebox{\linewidth}{!}{\includegraphics{compareMeanDV-acc-logoddsSO-crop.pdf}}}
  \caption{Random permutation model accuracy when using log-odds transformation on context for StackOverflow}
  \label{figContextLogoddsSO}
\end{figure}

\begin{figure}[!htbp]
  \scalebox{.6}{\resizebox{\linewidth}{!}{\includegraphics{compareMeanDV-acc-logoddsT-crop.pdf}}}
  \caption{Random permutation model accuracy when using log-odds transformation on context for Twitter}
  \label{figContextLogoddsT}
\end{figure}

\subsubsection{Discussion}

Vector-based models such as BEAGLE \parencite{Jones2007} and the random permutation model \parencite{Sahlgren2008} have included only a context component in the past (i.e., no prior component).
When the prior component is included for the random permutation model, accuracy improves significantly (compare accuracy for full models to context models in Figures \ref{figContextLogoddsSO} and \ref{figContextLogoddsT}).
However, it is somewhat surprising that it does not matter much how the context and prior terms are added for the random permutation model.
Model performance is similar when the context component used for the model is based on a correlation compared to a log-odds value.

There may be a slight edge in increased accuracy when a log-odds transformation is used for the StackOverflow dataset.
However, that result is not statistically reliable for the sample size used in this analysis (confidence intervals overlap in Figure \ref{figContextLogoddsSO}),
and even if present the overall change in accuracy is small (\num{.45} to \num{.47}).
Nonetheless, it does seem appropriate to convert the context to a log-odds adjustment value when context is added as an additional term to the random permutation model.
Since performance does not decrease when the transformation is made, may slightly increase,
and leads to a more natural interpretation of each model term as a log-odds value, it is reasonable to include this step for the random permutation model.
Future research could explore more improved methods to properly combine the terms.

\subsection{Word Order}

Vector-based models like the random permutation model can easily incorporate word order when making predictions.
This is one of their primary strengths that has been discussed \parencites{Jones2007, Sahlgren2008}.
In order to thoroughly test the impact of adding word order into the model, various methods for incorporating word order were tested on the StackOverflow and Twitter datasets.

\subsubsection{Method}

Three different methods for incorporating word order in the random permutation model were tested:
taking just the sign of the word's position relative to the tag (direction method), using the position (standard method), and using the position but only including words that were used near the tag (window method).
For the standard method, all words and their respective position are used to build the representation.
For the direction method, essentially two representations are created: all co-occurrences with words to the left of the tag, and all co-occurrences with words to the right of the tag.
The window method is similar to the standard method, where each word's relative position is maintained, but it only includes words near the tag to build the representation.
\textcite{Sahlgren2008} tested similar methods, and found that accuracy was highest when a narrow window was used for the window method (only including words at most two positions to the left and right of the tag).
The same $+2$ $-2$ range was used to test the window method on the Twitter dataset.

Only the Twitter dataset was tested, since the StackOverflow dataset contains no word order information between words in the title and body of the post and the tags used for the post.
The author chooses tags after filling out the title and body forms in the post, so the tags are never intermixed with words in the post like they are for Twitter.

The same four Twitter popular-users datasets were used for testing.
Both an ordered and orderless context component were created for the random permutation models.
The orderless context component uses the same co-occurrence matrix used by the Bayesian model to create the representation, since the Bayesian model does not contain any order information.
Models were tested that used only one of the three word order methods for the ordered context component, used only the orderless context component, used a combination of the two components, and combined context with prior. 

\subsubsection{Results}

Model accuracy results after incorporating word order into the random permutation model are included in Figure \ref{figContextOrder}.
All bar plots that include an order component where ``window'' or ``direction'' is not included in the title use the standard ordering method (no direction or windowing, all position information is included) 
This is the default ordering method used, so the names for these bar plots were kept consistent to the names used in other figures so that comparisons could be made across figures if necessary
(similar to what was done for the default row size for the random permutation model).
Base models that do not use entropy weighting were also included so that effect sizes could be compared between entropy weighting and adding word order information.

\begin{figure}[!htbp]
  \scalebox{.8}{\resizebox{\linewidth}{!}{\includegraphics{compareMeanDV-acc-orderT-crop.pdf}}}
  \caption{Model accuracy for various word order methods for the random permutation model for Twitter}
  \label{figContextOrder}
\end{figure}

Looking first at models where no prior is added, the random permutation model using only orderless context performs similar to the model using only order context with direction.
All other order-only models perform worse than the order model using direction and worse than the orderless model.

Full models were also plotted against the model where only the prior and orderless context was included (i.e., no order).
This was to test how much of the variance in model accuracy is shared between the orderless and order models.
The results show that there is very little unique variance in the order-only models, so order information contributes very little to overall model performance above and beyond what an orderless model contributes.
This can be seen by comparing the two-component model with only prior and orderless context to the three-component model with prior, orderless, and any of the three methods for the order context term.

\subsubsection{Discussion}

This was a surprising result.
One of the main benefits discussed for the random permutation model is that it can easily incorporate word order information into the representation.
But including word order information does not dramatically improve performance. 
For the random permutation model, it is much more important to properly handle low-predictor stop words and include a prior component than it is to include word order information,
at least for the Twitter dataset tested in this study.

The original BEAGLE model \parencite{Jones2007} actually showed only minimal improvement when incorporating word order as well.
In absolute terms, this model only improved by 2 percentage points in accuracy when trained on the TASA and tested on the TOEFL.
\textcite{Sahlgren2008}'s results for word order for the random permutation model are similar.
At a row size of around \num{2000}, model accuracy improves by less than a percentage point when order information is incorporated.
The results for larger row sizes are somewhat inconsistent, since the full model (order and orderless) sometimes underperforms the order-only model,
which should never happen since the order-only model is a subset of the full model.
\textcite{Sahlgren2008} most likely did not use a regression technique to optimize the weights for each of the order and orderless model terms.
So although incorporating word order into the random permutation model does seem to slightly improve performance,
the strength of the model is more likely to be in how efficiently it can represent the co-occurrence matrix rather than its ability to include word order in the representation.

Also, the reason why the direction version of ordering is the best predictor of the three ordered components is most likely because order information does not have a large effect,
so the two representations for words to the left and right of a tag are highly correlated.
This means that this ordered component is essentially composed of the all co-occurrence observations in the orderless component after being randomly sampled and partitioned into two separate representations.
The co-occurrences in these representations are highly correlated with each other, and highly correlated with the orderless component,
which is why model accuracy is similar for the orderless component and direction ordered component, and why accuracy does not improve when adding the direction ordered component to the orderless component.

Since it was shown that including word order information has little to no significant improvement on model accuracy, no attempt to incorporate word order information into the Bayesian model was made.

\subsection{Coefficients for Predictors}

The logistic regression is used to determine the optimal weight for each model component.
A regression is done for each run in the dataset, and since multiple runs were used for the StackOverflow and Twitter datasets, multiple samples of coefficient weights for each model component were computed.
The variance of the weights for each component show how likely it is that the regression is overfitting the data,
and the mean shows the best estimate weight of each component relative to the other.

\subsubsection{Method}

Coefficients for all of the runs for the Twitter popular-hashtags and StackOverflow randomly-sampled datasets were collected.
This totaled 5 sets of coefficient weights for StackOverflow (5 runs) and 60 for Twitter (15 runs for each of the 4 dataset samples).

\subsubsection{Results}

Coefficients for each model component of the full models are included in Figures \ref{figContextCoeffSO} and \ref{figContextCoeffT}
Error bars represent the standard deviation of coefficient values for each component.
The standard deviation is used instead of the confidence interval of the mean so that the plots show the overall stability of each coefficient across the different runs.
The range of the distribution of coefficient values within a component is much more narrow than the range of the mean coefficient values between the components of each model.
This shows that the model is not being overfit, as the weights for each component are relatively stable across the different runs.

\begin{figure}[!htbp]
  \scalebox{.6}{\resizebox{\linewidth}{!}{\includegraphics{compareMeanDV-coeffStd-coeffSO-crop.pdf}}}
  \caption{
  Coefficient values for each component of the full models for StackOverflow.
  Models used are the best-performing Bayesian and random permutation models
  (i.e., Bayesian and RP have entropy weighting, RP converts context activation from correlation to log odds).
}
  \label{figContextCoeffSO}
\end{figure}

\begin{figure}[!htbp]
  \scalebox{.6}{\resizebox{\linewidth}{!}{\includegraphics{compareMeanDV-coeffStd-coeffT-crop.pdf}}}
  \caption{Coefficient values for each component of the full models for Twitter}
  \label{figContextCoeffT}
\end{figure}

The weight of the prior relative to the context terms is also fairly consistent across models and across the two sites.
Across the sites, context is a much stronger predictor than past behavior for StackOverflow, but past behavior is more important for Twitter. 
Within a site, both models are consistent in how they favor context or past behavior.
Also, looking at StackOverflow, both models are consistent in weighting the body more than the title, and the title more than the prior term.

\subsubsection{Discussion}

It was not surprising that the model was not overfit in this scenario, since the number of observed tags is much greater than the number of coefficients being fit.
Each model has at most three terms, those weights are being fit to 500 posts within each run for Twitter and 100 for StackOverflow, and multiple tags are used for each post (particularly for StackOverflow).
It is interesting that although the values of the weights are stable within a site (StackOverflow or Twitter), the weights change dramatically when looking across the sites.
This illustrates that the impact of a user's past history relative to the current context can be domain specific, which means that for best results the weights for each of the terms should be recalibrated in new scenarios.
However, the underlying reason that the weights change between the sites cannot be answered from this study alone.
This relative impact may be changing from either a true alteration in the underlying weight of the prior compared to context for different domains,
or simply due to the fact that the amount of information collected for each of the terms changes between the domains (less information leads to lower weighting in general). 

\clearpage
\section{Chapter 6: Combining Predictors on the Popular-Users Dataset}

\subsection{Impact of Prior vs. Context}

Adding a user's prior tag history as an additional component has been shown to improve performance, but is this always the case?
If enough context information has been collected, maybe a context-only model can perform just as well as a combined context and prior model.
If this were the case, then it may be less important that the random permutation model does not include a prior term by default.

\subsubsection{Method}

Full models were tested against context-only models for both sites. 
The four Twitter popular-hashtags datasets and the StackOverflow randomly-sampled dataset were used for the test. 

\subsubsection{Results}

The results are included in Figures \ref{figContextPriorneededSO} and \ref{figContextPriorneededT}.
For StackOverflow, accuracy remains the same when the prior component is removed for both the Bayesian (\num{.63} to \num{.63}) and random permutation (\num{.63} to \num{.63}) models.
Accuracy does dramatically decrease for Twitter when the prior is removed, however (\num{.32} to \num{.14} for Bayesian, \num{.25} to \num{.12} for random permutation).

\begin{figure}[!htbp]
  \scalebox{.6}{\resizebox{\linewidth}{!}{\includegraphics{compareMeanDV-acc-priorAddSO-crop.pdf}}}
  \caption{Model accuracy when prior is removed for StackOverflow randomly-sampled dataset}
  \label{figContextPriorneededSO}
\end{figure}

\begin{figure}[!htbp]
  \scalebox{.6}{\resizebox{\linewidth}{!}{\includegraphics{compareMeanDV-acc-priorAddT-crop.pdf}}}
  \caption{Model accuracy when prior is removed for Twitter popular-hashtags dataset}
  \label{figContextPriorneededT}
\end{figure}

\subsubsection{Discussion}

These results show that prior information is not always needed when a large amount of context is available and not much information has been collected about each user's past history.
The StackOverflow randomly-sampled dataset selects posts across the entire dataset, regardless of each user's popularity or tag history.
Across the dataset, each user only asks around five questions on average, so there is not a large amount of past tag history available for the prior component to use,
which is why this component is not a strong predictor in this case.

However, can a context-only model perform just as well as a combined context and prior model when users with the largest amount of StackOverflow history are tested?
If each user's tagging history is a useful predictor for StackOverflow,
then the prior component should at least generate unique predictive variance in the case where users with large amounts of history are tested.

\subsubsection{Method}

The same StackOverflow popular-users datasets used to test the prior component in isolation were used to test the prior component when context was added.
However, since computing activations for the context component take an order of magnitude more wall-clock time than computing the prior component, not all observations could be included for the test.
To reduce the observations, the most recent 50 posts from each user in the datasets were selected.
The total number of users sampled was also reduced to 40 users for StackOverflow and 80 users for Twitter.
Using the most recent observations for each user ensures that the observations tested have acquired the greatest amount of past tagging history in the datasets.
So if a context-only model performs just as well as the full model in this case, it would be surprising.

\subsubsection{Results}

The results comparing the full model to a context-only only model for the StackOverflow popular-users dataset are included in Figure \ref{figPUAddPrior}.
In this case, past tag history does contain unique variance for accurately predicting tag use.
Accuracy improves for both the Bayesian (\num{.51} to \num{.63}) and random permutation (\num{.47} to \num{.58}) models when the prior model component is combined with context compared to context alone.

\begin{figure}[!htbp]
  \scalebox{.8}{\resizebox{\linewidth}{!}{\includegraphics{compareMeanDV-acc-priorAddPUserSO-crop.pdf}}}
  \caption{Model accuracy when prior is removed for StackOverflow popular-users dataset}
  \label{figPUAddPrior}
\end{figure}

\subsubsection{Discussion}

This shows that incorporating past tagging history is important even for a site like StackOverflow where a large amount of context information has been collected.
However, for this type of site it only becomes important when a reasonably large amount of a user's past tagging history has been collected.
For a randomly sampled user on StackOverflow, a context-only model captures the majority of the predictive variance.

\subsection{User-customized Co-occurrence Matrix}

The co-occurrence matrix for the context component of both models has been built by aggregating across users and collecting all instances of context words and associated tags.
That is, the default co-occurrence matrix is not customized to each user, so the same matrix is used for all users.
This is commonly done mainly because there is never enough context by tag co-occurrences for a single user to generate a stable co-occurrence matrix.
If there was enough information, accuracy would most likely improve since it is not likely that every user's word by tag co-occurrence profile is exactly the same.
However, since several hundred posts for each user in the popular-users datasets are available, an analysis was tried where user-custom co-occurrence matrices were used.

\subsubsection{Method}

Separate co-occurrence matrices were built for each user in the sampled StackOverflow and Twitter popular-users dataset.
The same sampling method used to test the impact of the prior component compared to the context component for the popular-users datasets was used,
and the co-occurrence matrices were built from the oldest posts that were not included in the testing set.
So for each user, the co-occurrence matrix was created from their oldest posts, up to (and not including) their 50 newer posts that were used to test model accuracy. 

\subsubsection{Results}

Model accuracy when using a user-customized co-occurrence matrix for StackOverflow and Twitter is shown in Figures \ref{figPUCustomSO} and \ref{figPUCustomT}.
The results compare accuracy for models that use a global co-occurrence and user-specific co-occurrence matrix.
For StackOverflow, model accuracy for the user-specific co-occurrence matrix (labeled as ``user sji'') can be compared against the default global co-occurrence matrix.
Accuracy decreases dramatically when a user-specific co-occurrence matrix is used for both the Bayesian (\num{.64} to \num{.35}) and random permutation (\num{.58} to \num{.36}) models.

\begin{figure}[!htbp]
  \scalebox{.8}{\resizebox{\linewidth}{!}{\includegraphics{compareMeanDV-acc-userSjiSO-crop.pdf}}}
  \caption{Model accuracy with user-customized context component for StackOverflow}
  \label{figPUCustomSO}
\end{figure}

\begin{figure}[!htbp]
  \scalebox{.8}{\resizebox{\linewidth}{!}{\includegraphics{compareMeanDV-acc-userSjiT-crop.pdf}}}
  \caption{Model accuracy with user-customized context component for Twitter}
  \label{figPUCustomT}
\end{figure}

The results are similar for Twitter.
However, the entire dataset is not available for Twitter, so a global co-occurrence matrix condition can not be tested,
since the popular-users dataset samples across a wide range of hashtags, and due to Twitter's API restrictions it would be infeasible to collect all posts that use each of the hashtags in the dataset.
But user-specific co-occurrence matrices can be built, since the popular-users dataset does contain all context and hashtag observations for each user.

Model accuracy when only the single user-customized context predictor is used is relatively poor (\num{.17} and \num{.21} for Bayesian and random permutation),
and the prior term is a much better predictor (\num{.37}).
Further, when this component is combined with the prior component, model performance improves only slightly over performance when only the prior is used (\num{.37} to \num{.39} for both the Bayesian and random permutation).

\subsubsection{Discussion}

Both models require large numbers of context co-occurrence observations to make accurate predictions from context information.
Otherwise, the co-occurrence matrices do not have enough data to properly stabilize.
Millions of posts were used to generate the global co-occurrence matrices for Twitter and StackOverflow.
If this number of posts had been collected for a single user, then the user-customized co-occurrence matrix for that user should lead to more accurate predictions compared to using the global matrix.
However, in this study only around 100 posts were used to build the user-specific matrices for each user.

This is most likely why model accuracy was so poor when user-specific matrices were used:
Even for the large datasets analyzed in this study, there is still not enough history collected for each user to build customized co-occurrence matrices.
The prior model component can be customized for each user, since the prior is only concerned with tag use, and there are a reasonably small set of tags that the model is choosing from.
However, the context component is built from the set of all of the prior tags crossed with all of the possible words, which is a much larger space than just the set of tags.
So building a stable context component requires a much larger set of data than the prior component.

That said, if a user only chooses a small set of tags (so the tag space is small), and computing a global co-occurrence matrix is infeasible (e.g., with the Twitter popular-users dataset), 
then it may be worthwhile to add the user-custom context model component as an additional predictor.
As was shown for Twitter, model accuracy can slightly improve over a model that just uses a custom prior, even when the co-occurrence matrix only contains around 100 posts.

Further, if the number of co-occurrences is small, and a user-custom co-occurrence matrix is used, it may be advantageous to use the compressed random permutation model instead of the uncompressed Bayesian model.
For the Twitter popular-users dataset, only a user-custom co-occurrence matrix could be built, and when this matrix is used the compressed random permutation model performed better than the uncompressed Bayesian model.
When the global co-occurrence matrix was removed from the StackOverflow dataset, and a user-custom matrix was used instead, the accuracy for the random permutation model decreased less than the Bayesian model. 
This provides further evidence showing that there is an interaction between dataset size and optimal model:
At lower numbers of co-occurrences the compressed random permutation model is able to perform better than the Bayesian model,
and the Bayesian model surpasses the compressed model at higher co-occurrence counts once the cells in the full co-occurrence matrix stabilize. 

\clearpage
\section{Chapter 7: General Discussion}

\subsection{Overall Results}

Table \ref{tabOverallResults} summarizes model accuracies for full models (all components) on all of the datasets tested in the study.
After modifying the models by adding an entropy weighting mechanism to both models and a prior term to the random permutation model,
accuracy for the compressed random permutation model was nearly equal to the Bayesian model.
The largest difference between the two models was on the Twitter popular-hashtags dataset (7 percent).
Both models performed nearly equal for the StackOverflow datasets tested,
and both models were much more accurate when tested on StackOverflow compared to Twitter.

\begin{table}[!ht]
  \caption{Summary of accuracy for each model and dataset}
  \label{tabOverallResults}
  {\tabulinesep=1.2mm
    \begin{tabu}{llll}
      \hline
	Site 		& Dataset		& Model						& Accuracy \\
	\hline
	StackOverflow	& popular-users		& Bayesian with entropy				& \num{.63} \\
			& 			& RP with entropy and log odds 			& \num{.58} \\
			& randomly-sampled	& Bayesian with entropy 			& \num{.63} \\
			& 			& RP with entropy and log odds			& \num{.63} \\
	Twitter		& popular-users		& Bayesian with entropy and user sji		& \num{.39} \\
			& 			& RP with entropy, log odds, and user sji	& \num{.39} \\
			& popular-hashtags	& Bayesian with entropy				& \num{.32} \\
			& 			& RP with entropy and log odds			& \num{.25} \\
	\hline
    \end{tabu}
  }
\end{table}

Overall, both the random permutation and Bayesian declarative memory models fit the observed tagging behavior in StackOverflow and Twitter relatively well.
The fits for StackOverflow in particular are quite high, especially given the large tag space on the site.
These results provide support for the idea that choosing a hashtag when composing a tweet and tagging a StackOverflow post is a long-term memory retrieval process.
However, both models were not 100 percent accurate, and the amount of variance left to predict was much larger than what would be observed if retrieval noise was included in the model.
So it is most likely the case that the task is not done solely with declarative memory processes.

Alternatively, perhaps authors are taking advantage of context information that is not currently represented in the model.
For example, time of day might be a useful predictor of tag use on Twitter.
\textcite{Cohn2012} built a tag-prediction model for Twitter that was based primarily on temporal cycles of hashtag use.
This technique could be naturally incorporated into a co-occurrence framework by including time of day as an additional context by tag co-occurrence for each post.
This would allow time of day information to be used alongside the rest of the context information in the words of the post.
A similar technique could be used to add additional context predictors to the model.
These co-occurrences could either be added on as additional columns to the existing co-occurrence matrix,
or a separate co-occurrence matrix could be used for each additional context component, similar to how separate matrices and context predictors were created for the words in the title and body of a post. 
Including additional context predictors such as time of day may help improve model accuracy,
and perhaps it is the case that the rest of the variance left to predict is from these context components that the author is using but are not yet included in the model.

\textcite{Anderson1991} also looked at similar tagging tasks with large datasets when deriving the prior component of the declarative memory theory.
\citeauthor{Anderson1991} examined, for example, the frequency of word choice over time used for the title of articles for the New York Times,
and used the results to derive the power law equation for the prior term in the declarative memory model.
ACT-R's declarative memory theory claims that the system has evolved to make predictions that closely match the statistical structure of the environment.
Using a power-law equation to model future tag use based on past tag use is a reasonable statistical approach to the problem.
It is also the case that this same equation is an accurate model for the prior component of declarative memory, as was shown by \citeauthor{Anderson1991} in the original derivation, 
and as is supported in the results from this study.

\subsubsection{Representation}

Given that the storage space required to represent the co-occurrences for the random permutation is dramatically smaller than the uncompressed Bayesian model,
and that the compression is lossy and essentially random (i.e., no SVD),
it is quite interesting that the random permutation model performs nearly as well as the full Bayesian model for this task.

These results pose an interesting and important fundamental question about declarative memory:
How, precisely, is the neural system configured to represent co-occurrences for long-term memory?
Since at the behavioral level both models perform similarly, it is possible that the neural system is using either representation to store co-occurrences.
Vector-based models are simple enough that they can easily be implemented in a neural network.
Perhaps a form of vector-based compressed storage is being used at the neural level,
since it can be implemented in neurons and is an efficient way for the machinery to generate predictions that closely match the statistical structure of the environment
(i.e., that closely match the uncompressed Bayesian representation).

As neural imaging systems improve, this question of neural representation can be best answered by looking directly at the neural activity.
However, there are possible behavioral studies that could be done to provide evidence that may favor one representation over the other:
The largest differences between the Bayesian and vector-based models happen with small numbers of co-occurrence observations.
The vector-based model begins to generate more accurate predictions sooner than the Bayesian models,
since the noise introduced from the compression helps smooth out activation values until a larger number of co-occurrences are observed and the co-occurrence values stabilize.
For a behavioral study, novel words could be introduced alongside a corpus of known words, and the participant could be asked to make retrievals of the novel words and known words as more novel words are introduced.
Both models would make different predictions about the rate at which activation values for the novel words stabilize relative to the known words, and such make different predictions about retrieval error rates over time.
Learning a new language might be a domain where large amounts of these sorts of retrievals could be collected.
For instance, the online site Duolingo might contain enough collective data about language learning and word translation to better tease apart the two context models.

\subsubsection{Compression}

Also, the relatively strong performance of the random permutation model is not due to the model's ability to represent word order.
Performance is similar for the random permutation and Bayesian models when the co-occurrence matrix for both models is the same, and not based on word order.
Performance also does not improve by much when word order is added as the last predictor in the random permutation model.
The random permutation model seems to work so well primarily due to the way that the full co-occurrence representation is compressed.

And compression for the random permutation model is essentially random.
That is, no SVD is done to identify the most predictive components,
so the compressed representation used by the random permutation model does not contain nearly as much of the variance from the original co-occurrence matrix that it could have.
However, performing a full SVD on a co-occurrence matrix as large as the ones generated for StackOverflow and Twitter is computationally infeasible.
So, it also seems plausible that this SVD computation is infeasible at the neural level.
When deriving the LSA approach, \textcite{Landauer1997} explicitly state that they make no claims that the brain is computing an SVD at the neural level;
only that the neural machinery is set up such that the representation makes predictions that are similar to what would have been generated if the matrix decomposition for an SVD would have been computed.
Perhaps the randomly-compressed representation used for the random permutation model is the way in which the brain tries to efficiently achieve the full LSA- and SVD-like approach that \citeauthor{Landauer1997} propose.

\subsubsection{Attentional Weight}

The Bayesian declarative memory model for ACT-R has an attentional weight term $W_{j}$ that can be used to implement how stop words are handled.
The random permutation model was also easily modified to have an attentional weight term by attenuating each word's set of ones and negative ones values by that word's attentional weight.
Both a method for stop-word removal (frequency filtering) and stop-word attenuating (entropy weighting) performed well for both models.
Also, using a proper method for handling the low-predictor words produced some of the largest improvements in model accuracy compared to the other explorations
(e.g., compared to word order, properly adding terms, compression amount for random permutation representation).
Since the ACT-R declarative memory theory already has a component for attentional weight, it may be the case that the neural system performs some form of stop-word weighting when computing activations,
and this is reflected in the attentional weight term.
That is, $W_{j}$ can be defined mathematically, has the same general form across tasks, and both the frequency-filtering and entropy-weighting methods for stop-word removal are likely first candidates for the definition.
This claim, however, would require that no top-down activation is used to compute the attentional weight, which would mean that the $W_{j}$ is not under conscious control, which is not likely to be correct.
Perhaps it is a combination of top-down and bottom-up systems that determine $W_{j}$, and the entropy-weighting and frequency-filtering techniques are methods of formalizing the bottom-up component.

\subsection{Implications and Future Directions}

\subsubsection{Generalizing Results}

It seems likely that the entropy method for computing the attentional weight term $W_{j}$ may be applicable to other declarative memory tasks.
The entropy method is parameter free (unlike the frequency-filtering method), completely data driven (much like the co-occurrence matrix), and does not require tuning.
It is common to handle stop words in some manner when working with NLP and large text documents.
So, if these large natural language documents are used to test declarative memory, then the entropy method may be a more cognitively plausible way of dealing with stop words rather than 
filtering based on a predetermined list or based on the frequency count in the dataset, particularly since the entropy method substantially increases model accuracy for these datasets and since it is parameter free.

Adding a prior component to the random permutation model also increased model accuracy for both StackOverflow and Twitter.
It is unsurprising that this occurred.
However, it is quite surprising that this is the first time that the prior component has been included and tested in a random permutation model.
If the random permutation model is explored as a declarative memory theory for other datasets, it should be worthwhile to include the prior component for those explorations.
Although it did not seem to matter much how the prior component is combined with the context component in this study, 
there may be better ways than the log-odds transformation to combine the terms.
Further research could focus on exploring a larger set of methods for combining the context component of the random permutation model with the prior component from the ACT-R theory.

\subsubsection{Activation Calculation}

The Bayesian and vector-based memory models differ in how the co-occurrence matrix is represented (lossy compression for vector-based models).
They also differ in how the activation of each tag is calculated, given the different representations.
The vector-based memory models, like LSA, compute a correlation between the context and each tag, while the Bayesian models sum the log likelihoods that each word in the context occurs with each tag.
These two methods for computing the activation are correlated but not identical.
Further research could try to tease apart how model accuracy is independently influenced by the representation (compressed or not) and the activation calculation (correlation or log likelihood).
To do this, four models could be constructed (uncompressed and compressed representation crossed with correlation and log likelihood activation) compared to the two in this study,
so that the effects of representation and activation calculation could be partialed out.
This would allow, for example, the method of compression for the random permutation vector-based model to be analyzed independent of activation calculation.
The results would provide additional evidence to help determine the specific representation that the declarative memory system uses to represent co-occurrences,
and also the specific activation calculation that is carried out by the system.

\subsubsection{Future Applications}

The tag-recommendation models developed through this research should be immediately relevant to real-world tasks.
Model accuracy is high enough (particularly for StackOverflow) for the developed models to be used as a foundation for tag recommendation systems that are deployed on Twitter or StackOverflow.
Also, since the developed models are task agnostic, they can potentially be used as tag recommendation systems for other domains.
Examples of similar tagging tasks are hashtag use on Facebook, flagging (i.e., tagging) Wikipedia articles for content issues or overly opinionated material,
auto labeling incoming mail and placing it into appropriate folders, and spam filtering.
Having an accurate and efficient co-occurrence memory model that correctly retrieves relevant tags can be applied to a wide variety of tasks.

\subsubsection{Summary}

The end result of this research is [1] a better understanding of how a user's past tagging history influences future tag use,
[2] a more thorough evaluation of the strengths, weaknesses, and differences between two cognitively-plausible memory models on large-scale real-world retrieval tasks,
and [3] two efficient implementations of the models for making user-customized tag predictions for StackOverflow posts and Twitter tweets. 
This work has shown how past user history is important, and how a context-only model like the random permutation model can be modified to include the prior component.
The work has also shown how the strength of the random permutation model may not be in its ability to represent word order,
and rather in its ability to efficiently compress the full co-occurrence representation used for the Bayesian model.
Several different formalisms for ACT-R's attentional weighting term were also explored, and an entropy-weighting technique was found to perform well without requiring any parameter tuning.
Finally, this work has helped raise the question about how co-occurrence observations are represented at the neural level, since two models with very different representations performed similarly well.
The results from the study and the new research questions raised will help to better understand how imposing specific architectural constraints on memory models influence what information is retrieved.

\begingroup
\setstretch{1}
\setlength\bibitemsep{12pt}
\clearpage
\printbibliography[heading=bibintoc]
\endgroup

\clearpage
\section{Appendix 1: Support Code}

All software code created for this research is publicly available on GitHub \parencite{StanleyRepo2014}.

\subsubsection{Directory Structure}

The repository is separated into three main folders:
dissProject, firstSOProject, and writeups.
The firstSOProject folder contains the support code for the \textcite{Stanley2013} research.
The dissProject folder contains support code for this dissertation research.
The writeups folder contains all writeup projects created during the dissertation process (e.g., the LaTeX for this dissertation, Keynote presentations).

Within the dissProject folder, there are two main files that contain the majority of the code for the dissertation research: scrapeUsers.py and preProcess.R.
The python file contains the necessary code to scrape the Twitter site for tweets, import results into the Postgres database, and perform various cleanup functions that were easiest to do in Python.
The preProcess.R file contains all modeling and analysis code for the project.
Within the file, there are implementations for the Bayesian and random permutation models.
Also there is code to test the models by running each model through the various StackOverflow and Twitter test datasets.

\subsubsection{Model Implementations}

The Bayesian model is implemented using the data.table package, so that fast joins can be performed on the table when computing the strength of association values.
The random permutation model is represented as a matrix, so that correlations between the model and context vectors are fast.
The matrix is not sparse, since there are currently no sparse matrix packages for R that have a vectorized function to compute correlations between a vector and a matrix.

The Bayesian model requires slightly more time to compute activation values and also more memory space to store the representation.
This is because the random permutation model is compressed and only needs to store around \num{2048} rows in the co-occurrence matrix.
However, both models are implemented efficiently enough so that all analysis runs and activation computations can be kept in memory.
64 GB of memory was required to perform the analysis, particularly on the largest Twitter (3 million) and StackOverflow (1 million) document sizes.

\subsubsection{Design Decisions}

All model manipulations (e.g., using an entropy weighting metric, using a model with only the prior term) were expressed as part of a configuration object.
That object is a list of configuration name, value pairs.
The configuration object is passed to the analysis code, which determines the specific type of run that is performed.

This configuration design enables a large amount of code reuse and overlap.
This helps minimize the likelihood of a programming mistake, since functions are reused multiple times and overall static code length is kept to a minimum.

\subsubsection{Testing}

Each analysis run produced a set of result files.
Those result files were checked into the repository so that changes could be tracked.
Code changes were tested by looking at differences between previously checked in output files and the output files produced after a code change was made.

A small amount of unit testing was also performed.
The core behavior of each model implementation (Bayesian and random permutation) was tested in isolation to ensure that the activation values returned by the models were mathematically correct.
Unit test code is located within dissProject/test.

\subsubsection{Parallelization}

None of the analysis code is finely parallelized.
Most of the inner loop in R ends up running in compiled C, and execution within those function calls occurs on a single core.
However, since multiple models were explored, each different model configuration was executed in parallel.
This type of course-grained parallelization was achieved using R's built-in Parralel package.
Also some parallelization was done at the OS level when various make targets would run in parallel.

\subsubsection{Database}

All raw Tweet and post data were stored in Postgres tables.
Also, after the documents were tokenized, the tokenized versions of the raw data were stored in Postgres tables.
When R ran an analysis, it would query the tables to retrieve the correct set of tokenized words and tags.
So there is a lot of R code that uses the RPostgreSQL package to communicate with Postgres as the analysis is running.

\subsubsection{Running the Analysis}

All analysis runs were scripted and can be run by using the appropriate target in dissProject/Makefile.
The three main targets to run the models through the prior-only runs on the popular users dataset,
the full models on the randomly-sampled and popular-hashtags datasets,
and the full models on the popular users dataset are ``parrunPriors'', ``parrunContext'', and ``parrunPUsers'' respectively.
Each of these targets takes tens of hours to complete.
``parrunContext'' is the longest and takes around 72 hours on a desktop machine with 64 GB of memory and 24 virtual cores.

Smaller test runs were also created so that model changes could be more rapidly developed.
These typically use smaller Twitter and StackOverflow document sizes (e.g., only \num{10000} StackOverflow posts) to create the co-occurrence matrix
and a smaller number of posts to test the models on (e.g., 20 instead of 500).
Running these functions takes around a minute to complete.
To run these, the R function is called directly (i.e., not a make target) within an already-running R session.
As an example, the ``runContext20g1s6'' function takes only a few minutes to complete, runs the models on a document size of \num{1000}, with only a single run per model, and only 20 sampled posts for each run.

\end{document}

