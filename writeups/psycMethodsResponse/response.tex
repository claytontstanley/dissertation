\documentclass[answers,12pt]{exam}
\usepackage{xcolor}
\definecolor{SolutionColor}{rgb}{0.1,0.3,1}
\usepackage{lipsum}
\usepackage{enumerate}
\usepackage{alphalph}

\renewcommand{\theenumi}{\AlphAlph{\value{enumi}}}
\renewcommand{\thequestion}{\AlphAlph{\value{question}}}
\renewcommand\questionlabel{\llap{\thequestion)}}

%\pointsinrightmargin
%\boxedpoints
\unframedsolutions
\shadedsolutions
\definecolor{SolutionColor}{rgb}{0.9,0.9,1}
\renewcommand{\solutiontitle}{}

\begin{document}


Dear Editor in Chief, \bigskip

Please read below our answer about the \numquestions{}
questions made by  both refereees. I hope that you agree
with all our comments. \bigskip

Best regards,\bigskip

The author


\begin{questions}

\question This paper has great potential to serve as a meaningful contribution to the 'big data' special issue. Although there was clearly concern about the paper being a novel contribution to the field, it is reasonable to claim that the ties between clear explanation of a theory combined with clear and accessible approach to 'big data' predictive models would constitute a relatively novel contribution.

\begin{solution}
No need to comment.
\end{solution}

\question The main concern is for this paper to be highly accessible to an educated-but-untrained audience of psychological researchers -- providing clear and compelling explanation, streamlined language and accessible formulas for ease of reading, and programs and data for hands-on application. It's probably obvious that the more you can do this, the more impactful (visible, citation-worthy) your paper will be.

\begin{solution}
No need to comment directly to this.
\end{solution}

\question 2. The expert reviewers agreed that the paper was an impressive effort--and equally important is how they viewed a critical gap between the theory of ACT-R and the data here that are examined with exploratory 'big data' techniques (Rev 1, bullet 1, 3, and especially 4; Rev 2 para 2). In revision, there is opportunity to narrow this gap, such as by explaining further:

\begin{solution}
No need to comment directly to this.
\end{solution}

\question (2a) the substantive nature of 'declarative memory' (e.g., maybe you view the content, through analysis, to reflect traces of the 'declarative memory architecture' and changes thereof; Rev 2 3rd para from bottom);

\begin{solution}
Was addressed in the reviewer's comment.
\end{solution}

\question (2b) how exploratory findings inform ACT-R in its instantiations and perhaps even in qualifications/revision to the theory -- there is a balance to strike here: the paper should be a highly accessible example of combining theory with big data (continuing to clarify for the reader -- e.g., Rev 2, para 6, 8, 9), ...

\begin{solution}
Was addressed in the reviewer's comment.
\end{solution}

\question â€¦ yet technical details are also important for those with more serious investments in ACT-R; consider including technical details but relegating them to online Supplemental Material (which has no page limits, can be in color, etc.).

\begin{solution}
Provided a link to the github-hosted repository for this research.

``All software code created for this research is publicly available on GitHub (Stanley, 2014).''

\end{solution}

\question (2c) conversely, how ACT-R (which versions) guides the interpretation of exploratory findings (vs. predictive models on big data often being more black-box-like); for instance, clearer explanation of how the formulas instantiate the theory is needed (Rev 1, bullet 6)

\begin{solution}
Was addressed in reviewer's comment.
\end{solution}

\question (2d) how future studies might complement the large-scale field data here with experimental research (Rev 2, para 5)...

\begin{solution}
Was addressed in reviewer's comment.
\end{solution}

\question ...and perhaps research that combines group-level and individual phenomena in a multilevel framework of big-data modeling, analysis and inference (Rev 1, bullet 2)

\begin{solution}
Was addressed in reviewer's comment.
\end{solution}

\question 3. Closer consideration of optimized learning vs. ACT-R learning is warranted, where a reviewer suggests a hybrid approach in the literature that might suggest an important and needed shift in the framing, analysis and implications of the current work (Reviewer 1, bullet 7 -- also be sure to download Rev 1's hand-written PDF, where time is taken to suggest specific edits to the work)

\begin{solution}
Was addressed in reviewer's comment and suggested edits will be looked at and incorporated.
\end{solution}

\question Again, this paper has promise to reflect a novel and important contribution to the journal that brings together an introduction to modern ACT-R, and a strong example of tying theory to big-data models and analyses -- with accessible data sets and annotated code to readers. In doing so, we hope you will take on the challenges enumerated above and by reviewers to develop the paper further -- and in return, we will do our best in providing a more expeditious review process. Note that we reserve the right to bring in an additional expert reviewer as needed, who will have access to the other reviewers' comments.

\begin{solution}
No need to comment.
\end{solution}


Reviewer 1: 

\question I would first like to commend the authors for an ambitious project. A high degree of scholarship is apparent in their work. Please see my attached review for in-line comments and suggestions. I would like to make some general points that I would like to see.

\begin{solution}
No need to comment
\end{solution}

\question - I found that the paper proceeded too quickly through the initial explanation of ACT-R and vector-based systems. This is especially important because the default version of ACT-R (6) was not described in detail (no Table or Figure was presented)

\begin{solution}
Added table and description for default version of ACT-R 6.

Expanded the section for the vector-based model and included a description of the retrieval process for that model.

Added a section that directly compares the theory of the vector-based and ACT-R model (``Comparison of Vector-Based Models and ACT-R'').
\end{solution}

\question and no comparative figure outlining the differences in representation between ACT-R and vector-based systems was presented. I find the lack of discussion of the changes in the ACT-R architecture a major concern. The authors reference their prior work, but I believe it requires a separate section in the document to properly evaluate their current model's performance. 

\begin{solution}
Added section that outlines the differences between ACT-R and vector-based systems (``Comparison of Vector-Based Models and ACT-R'').
\end{solution}

\question - I found throughout the introduction and theory sections that the terminology seemed to waffle between discussion individual user's vs aggregate performance. It wasn't until the methods section that I realized it was due to the difference between how StackOverflow and Twitter information was obtained. I think some extra discussion on the psychological aspects of individual vs group psychology may make your argument stronger. 

\begin{solution}
 Added a paragraph in discussion that addresses this and provides one future research idea.

 ``Individual vs. Group Representation of Memory. Even with the large datasets used in this research, there was not enough data on each individual user to build up separate co-occurrence matrices for them. That is, the context component of the declarative memory models used in this research was formed as a collection of observed co-occurrences across all individuals. Nonetheless, the prior component for the models was customized for each individual (except in the Twitter popular hashtags dataset when it was
not possible). Ideally, both components would be customized to the individual, and it remains an open question how much accuracy is influenced by using an aggregate or custom context component in the models.''
\end{solution}

\question - As previously mentioned, I want to highlight that the theoretical background section is covering a lot of ground but the explanations of each are a bit terse. I think the section could be re-worded for clarity's sake.

\begin{solution}
Added table and description for default version of ACT-R 6.

Expanded the section for the vector-based model and included a description of the retrieval process for that model.

Added a section that directly compares the theory of the vector-based and ACT-R model (``Comparison of Vector-Based Models and ACT-R'').
\end{solution}

\question - As I read through the paper, I found it unclear to maintain the exact thesis of the work? What is the goal of the research. While the amount of information collection was impressive, the paper came off much like 'throwing XXXX at the wall and seeing what sticks' rather than a targeted hypothesis of what should work and why. As such, the paper is more explanatory than predictive in the traditional sense, to its decrement since the paper is using cognitively-inspired models.

\begin{solution}
Added additional paragraph in introduction.

``As a general methodology for this research, we used a cognitively-constrained
exploratory model development process. Since we worked with large-scale datasets, we had enough data to explore many different modifications simultaneously without the risk of overfitting. However, we were not interested in bringing to bear the entire collection of machine learning algorithms to this prediction task in order to completely optimize a model. Rather, we were interested in evaluating how well two cognitive models could explain the results, and then making selective modifications to them to see how performance changes. The modifications that we explored were selected such that they influenced model accuracy, could be easily added to (or removed from) the cognitive models without massive modifications, and could be considered cognitively plausible.''
\end{solution}

\question - During the interpretation of the various results, I'm concerned that the graphs do not always accord with the discussion. In some cases, I noticed a small drop in accuracy was within the 95\%CI bounds between two conditions, making the difference statistically insignificant, however, these results are reported as being lower and a reason attributed (see p35 for an example). This greatly weakens the overall argument. I recommend performing some inferential statistics on the differences in performance to be sure that all reported effects are in fact significant. Even if they only trend a given way, it's important to note the size of the effect in your discussion.

\begin{solution}
Added section in general methodology describing the method of using CIs instead of explicit p-values throughout the paper.

``In similar spirit to Cumming and Finch (2005), Masson and Loftus (2003), Tryon (2001), confidence intervals were used throughout for statistical inference instead of explicit p-values. This was done because it is a more straightforward way to visualize all of the mean differences in accuracy (i.e., bar and line plots with a visual indication of the error) for each of the models, which eases interpretation. It is also a conservative estimate of
reliability, as using confidence intervals on the means essentially ignores the fact that this is a within-subjects design, where accuracy for each model is measured for each subject in each dataset. However, we would rather err on the side of making the results easier to interpret at the cost of using conservative estimates for reliability (i.e., use error bars overlaid on a visualization of the means instead of statistical tests on every combination of difference scores). In fact, using large datasets for analysis is what empowers us to do this: With this much data, we can use straightforward (although more conservative) inferentials and still see reliable results. This allows us to concentrate more on proper data visualization of the main findings.
Nonetheless, when accuracy differences between two models are called out explicitly in the text, the confidence interval for the mean difference in accuracy is also included. Since this interval is drawn across the observed difference scores, it will be smaller than the intervals included in the plots, since these are drawn independently for each modelâ€™s overall accuracy.''

Added CI ranges over the difference scores for each of the model comparison accuracies that are written out.

e.g.: ``Accuracy for the hybrid form for the largest k (k = 10) is slightly higher than the smallest k (k = 1) (.334 vs. .329, mean difference = 0.0026, 95\% CI [0.0023, 0.0029]).''

Softened text on p35 since the fine-grained distinctions being made (although likely reliable if a more complicated and powerful statistical model was used to test them) were not necessary for the overall conclusions.

``Model accuracy as a function of co-occurrence matrix size for StackOverflow is depicted in Figure 11. Accuracy improves for both the Bayesian and random permutation models as the size of the documents used to generate the co-occurrence matrix increases. However for the random permutation model, accuracy begins to plateau for the largest corpus size for all three tested versions. The Bayesian model does not plateau nearly as much and accuracy continues to improve even at the largest measured corpus size.''

\end{solution}

\question - I found the use of your entropy measure for attentional weight to be a key thesis of your paper and was intrigued by its use. I feel that this point could/should be expanded upon. What are the behavioral/cognitive consequences, what is its actual cognitive plausibility, why did you focus on attentional weight (Wj) instead of 'fixing' the Sji equation with some associative learning or the Bi with base-level inhibition? There was not a lot of discussion of other existing changes to the ACT-R architecture/theory that could potentially account for your data. 

\begin{solution}
Expanded the discussion section for attentional weight and the entropy method	

``Attentional Weight. The Bayesian declarative memory model for ACT-R has an attentional weight term Wj that can be used to attenuate or remove stop words. The random permutation model was also easily modified to have an attentional weight term by attenuating each wordâ€™s set of ones and negative ones values by that wordâ€™s attentional weight. Both a method for stop-word removal (frequency filtering) and stop-word attenuating (entropy weighting) performed well for both models.

It is unclear at this point if this modification is cognitively plausible, even if the modification was cleanly added to the current ACT-R model. What can be said from this research is that using a proper method for handling the low-predictor words produced some of the largest improvements in model accuracy compared to the other explorations (e.g., compared to word order or properly adding terms). Consequently, it may be worthwhile to explore this area more thoroughly in the future.

Also, there may be other ways besides adjusting the attentional weight that these models can be modified to achieve the same performance improvement. The entropy weighting term is likely solving a problem that resides only in contextual activation and not prior activation, since the activation for the prior component for each word is independent of the presentation of other words, so stop words have no way to interact with tag prior activation in this case.

One alternative explanation is that the entropy weighting measure was needed because the contextual co-occurrence matrix was not yet stable, even with these large dataset sizes. If the matrix was not yet stable, then by chance certain stop words would contribute more net activation to specific tags and introduce noise into the retrieval process for that tag. So further weighting the contribution of these stop words by leveraging the attentional weight term ensured that this noise was suppressed in the system. If this is the case, it would be interesting to see how large a dataset is required to not need to modify the attentional weight term. However even if this is the case, the random permutation model would still require entropy weighting for larger dataset sizes, since this model simply adds environment vectors to produce the current context vector and has no mechanism (unlike Bayesian normalization) for weighting the contribution of an environment vector.

Another explanation for the Bayesian model is that additional attentional weighting was needed because the contextual matrix was derived only from word by tag co-occurrences and not (word or tag) by (word or tag) co-occurrences. This was done for performance reasons as the computational complexity otherwise is simply too high for current hardware. It is possible that by adding all pairwise combinations of both words and tags in the posts may soften and stabilize the contribution of stop words when computing contextual activation for a tag, given current context.

Also perhaps a different Sji model may not need entropy weighing at all (e.g., the Hebbian-inspired associative learning model described in Thomson and Lebiere, 2013). Further research could explore how entropy weighting interacts with other models of Sji, and not just the random permutation and full Bayesian model.

Nonetheless, it seems likely that the entropy method for computing the attentional weight term Wj may be applicable to other declarative memory tasks. The entropy method is parameter-free (unlike the frequency-filtering method), completely data driven (much like the co-occurrence matrix), does not require tuning, and increases model accuracy for these datasets. It is common to handle stop words in some manner when working with Natural Language Processing (NLP) and large text corpuses. So, if these large corpuses are used to test declarative memory, then the entropy method may be a more cognitively plausible way of dealing with stop words rather than filtering based on a predetermined list or the frequency count in the dataset.''

\end{solution}

\question - I was also a bit concerned with the dichotomy between optimized learning and full-on ACT-R learning. Most models that I've ever written (and everyone else I've collaborated with) use ACT-R's hybrid optimized learning equation that remembers the last 5 or 10 references and then averages the rest. This keeps the model computationally efficient while still having the benefits of local spikes in activation due to local high-frequency events (something you'd see in twitter or StackOverflow during conversations). It seems an oversight to have not tested the optimal hybrid equation. It should be relatively easy for you to run your model varying that parameter and report (briefly) on the results. This could potentially drastically change your interpretation of when to use Bayesian vs vector-based models... which was a section I found specious to begin with.

\begin{solution}
Ran the hybrid form with k=1,5,10 and incorporated the results into the past user behavior section.

``However the hybrid form with a reasonable k is just as accurate as the standard form. This is due to the fact that the majority of change in activation for a chunk occurs in the short period of time after the presentation of the chunk (a property of a power law distribution). Once a chunkâ€™s presentations decay to the flat area of the power law distribution, not much change in activation for these presentations occurs, so they can be averaged and their presentation time can be discarded without much loss in fidelity. Therefore if computational speed is a concern, one may wish to leverage the hybrid form instead of the standard form. Although qualitatively the standard form produces the cleanest and most natural effect of decay rate on accuracy, as shown in Figure 1. It also does not have a bifurcation at k = 1. Consequently, if one is a purist and computational speed is less of a concern, one may still wish to use the standard form over the hybrid form.''

\end{solution}

\question - This may be a copy-editing issue, but I think the heading levels need to be adjusted. It makes some points (see Word Order in the theoretical background) a bit unclear.

\begin{solution}
Heading level was as intended, but reworded the heading to make it more explicit that the section was a subsection of vector-based memory systems.
\end{solution}

\question - From personal preference, I find too many sentences starting with "So" when no conclusion is made. This is made worse since many sentences proceeding "So" then start with "Thus" or "Then". This is stylistic but there are numerous places where phrasing comes off as too informal or 'train-of-thought'. 

\begin{solution}
Removed most of them, particularly the ones that were used to start a new paragraph or the ones that the reviewer highlighted in the marked document. I left a few as I prefer them stylistically over overuse of â€˜consequentlyâ€™, â€˜thereforeâ€™, â€˜thusâ€™, etc.
\end{solution}

\question To summarize:
- Great Scholarship
- Focus on telling a consistent story throughout the paper
- Extend/revise theoretical discussion to include examples, justify changes to ACT-R
- Verify statistics and discussion
- More focus on relevance of attentional weight
- Set optimized learning to hybrid learning

\begin{solution}
All addressed in previous comments
\end{solution}


Reviewer 2: 

\question The authors set out to explore large scale "tagging" datasets with the motivation: "Without using large datasets it is difficult to verify and explore modifications to the declarative memory architecture because the datasets are not large enough or rich enough to rigorously test modifications to the theory."

This certainly did wet the present readers appetite, but unfortunately after reading the manuscript, still starving for clearly defined models, testable predictions, or clear conclusions. 

\begin{solution}
No need to comment
\end{solution}

\question Overall the topic: "declarative memory" and its role in message tagging should be of significant interest to the journal and its audience. However, the novelty in the present manuscript is limited. Neither the way data is used (basically the problem is framed as a supervised label assignment problem with different sub-populations of tagged messages in Twitter and StackOverflow). 

\begin{solution}
Not sure how to address this. I actually find the fact that it looks straightforward a good thing.
\end{solution}

\question The review of the literature and the two model families (ACT-R and vector spaces) is unfortunately not self-contained, rather brief and qualitative. In case the manuscript is re-submitted I suggest that the authors use the opportunity to provide a clear account -from statistical principles to implementation- of the two sets of methods. The original papers are quite frustrating in their mixture of heuristics, arbitrary choice of details, and implementation. I haven't found a paper that gives a clear self-contained account of the relevant theories.

\begin{solution}
Mention the previous additions made (description of ACT-R proper, comparison between the two models, and  additional text describing the retrieval process for vector-based systems)
\end{solution}

\question With respect to the data I think availability is a strong argument. However, there are strong limitations to observational data compared to active experiments (i.e. controlling for message differences by letting multiple users tag the same message)

\begin{solution}
Added a paragraph in the discussion section
\end{solution}

\question The design description lacks crucial details: "Each model was tested by evaluating the accuracy of model-chosen tags on a fresh test subset of the datasets." How was it (sub-)sampled? what was the size?

\begin{solution}
Addressed in the equivalent comment to follow
\end{solution}

\question More importantly I am in serious doubt of the relevance of tagging for "declarative memory". Tagging can best be seen as social behavior/communication. Hence the communication network appears crucial for understanding tag behavior. Specifically, the behavior of the recipient appears crucial to me. In StackOverflow the aim is to get an answer, hence, the main purpose is to get attention from experts, which is more related to expected behavior of the recipient rather than a memory issue. Similarly for Twitter. This issue should at least be discussed.

\begin{solution}
Added a paragraph in the discussion section
\end{solution}

\question It is also not so clear what we have learned from the small differences in performance - can it be explained by differences in model complexity or does it reflect genuine differences in the models of declarative memory?

\begin{solution}
Added a paragraph in the discussion section
\end{solution}

\question It is well known that tags follow heavy tailed (powerlaw) distributions. Hence, normalization and sampling are critical issues. Here we learn "Each model was tested by evaluating the accuracy of model-chosen tags on a fresh test subset of the datasets." How was the test data (sub-)sampled? what were their size and time ordering, how was the tag space controlled /stratified for the test sets?

\begin{solution}
Removed this text at this point, as it was leading the reader to think that I was about to provide more detail about how the sampling was done, and this detail is provided in line for each subsection. For example, much text to follow explicitly defines how each dataset was sampled, and then the methods section for combining prior and context defines how the test data are randomly sampled (including size) apart from the training data, and calls out that data in the test sets are not included to build the co-occurrence matrices
\end{solution}

\question The English is excellent, however the writing could be made considerably more precise; e.g. by clearly defining central concepts, such as "chunk".

\begin{solution}
Added a definition of â€˜chunkâ€™ prior to its first use
\end{solution}

\end{questions}

\end{document}
