\documentclass[answers,12pt]{exam}
\usepackage{xcolor}
\definecolor{SolutionColor}{rgb}{0.1,0.3,1}
\usepackage{lipsum}
\usepackage{enumerate}
\usepackage{alphalph}

\renewcommand{\theenumi}{\AlphAlph{\value{enumi}}}
\renewcommand{\thequestion}{\AlphAlph{\value{question}}}
\renewcommand\questionlabel{\llap{\thequestion)}}

%\pointsinrightmargin
%\boxedpoints
\unframedsolutions
\shadedsolutions
\definecolor{SolutionColor}{rgb}{0.9,0.9,1}
\renewcommand{\solutiontitle}{}

\begin{document}


Dear Editor in Chief, \bigskip

Please read below our answer about the \numquestions{}
questions made by  both refereees. I hope that you agree
with all our comments. \bigskip

Best regards,\bigskip

The author


\begin{questions}

\question This paper has great potential to serve as a meaningful contribution to the 'big data' special issue. Although there was clearly concern about the paper being a novel contribution to the field, it is reasonable to claim that the ties between clear explanation of a theory combined with clear and accessible approach to 'big data' predictive models would constitute a relatively novel contribution.

\begin{solution}
No need to comment
\end{solution}

\question The main concern is for this paper to be highly accessible to an educated-but-untrained audience of psychological researchers -- providing clear and compelling explanation, streamlined language and accessible formulas for ease of reading, and programs and data for hands-on application. It's probably obvious that the more you can do this, the more impactful (visible, citation-worthy) your paper will be.

\begin{solution}
No need to comment directly to this
\end{solution}

\question 2. The expert reviewers agreed that the paper was an impressive effort--and equally important is how they viewed a critical gap between the theory of ACT-R and the data here that are examined with exploratory 'big data' techniques (Rev 1, bullet 1, 3, and especially 4; Rev 2 para 2). In revision, there is opportunity to narrow this gap, such as by explaining further:

\begin{solution}
No need to comment directly to this
\end{solution}

\question (2a) the substantive nature of 'declarative memory' (e.g., maybe you view the content, through analysis, to reflect traces of the 'declarative memory architecture' and changes thereof; Rev 2 3rd para from bottom);

\begin{solution}
Will be addressed in the reviewer's comment
\end{solution}

\question (2b) how exploratory findings inform ACT-R in its instantiations and perhaps even in qualifications/revision to the theory -- there is a balance to strike here: the paper should be a highly accessible example of combining theory with big data (continuing to clarify for the reader -- e.g., Rev 2, para 6, 8, 9), ...

\begin{solution}
Will be addressed in the reviewer's comment
\end{solution}

\question â€¦ yet technical details are also important for those with more serious investments in ACT-R; consider including technical details but relegating them to online Supplemental Material (which has no page limits, can be in color, etc.).

\begin{solution}
Added a section and provided a link to the github-hosted repository for this research

\end{solution}

\question (2c) conversely, how ACT-R (which versions) guides the interpretation of exploratory findings (vs. predictive models on big data often being more black-box-like); for instance, clearer explanation of how the formulas instantiate the theory is needed (Rev 1, bullet 6)

\begin{solution}
Will be addressed in reviewer's comment
\end{solution}

\question (2d) how future studies might complement the large-scale field data here with experimental research (Rev 2, para 5)...

\begin{solution}
Will be addressed in reviewer's comment
\end{solution}

\question ...and perhaps research that combines group-level and individual phenomena in a multilevel framework of big-data modeling, analysis and inference (Rev 1, bullet 2)

\begin{solution}
Will be addressed in reviewer's comment
\end{solution}

\question 3. Closer consideration of optimized learning vs. ACT-R learning is warranted, where a reviewer suggests a hybrid approach in the literature that might suggest an important and needed shift in the framing, analysis and implications of the current work (Reviewer 1, bullet 7 -- also be sure to download Rev 1's hand-written PDF, where time is taken to suggest specific edits to the work)

\begin{solution}
Will be addressed in reviewer's comment and suggested edits will be looked at and incorporated
\end{solution}

\question Again, this paper has promise to reflect a novel and important contribution to the journal that brings together an introduction to modern ACT-R, and a strong example of tying theory to big-data models and analyses -- with accessible data sets and annotated code to readers. In doing so, we hope you will take on the challenges enumerated above and by reviewers to develop the paper further -- and in return, we will do our best in providing a more expeditious review process. Note that we reserve the right to bring in an additional expert reviewer as needed, who will have access to the other reviewers' comments.

\begin{solution}
No need to comment
\end{solution}



Reviewer 1: 

\question I would first like to commend the authors for an ambitious project. A high degree of scholarship is apparent in their work. Please see my attached review for in-line comments and suggestions. I would like to make some general points that I would like to see.

\begin{solution}
No need to comment
\end{solution}

\question - I found that the paper proceeded too quickly through the initial explanation of ACT-R and vector-based systems. This is especially important because the default version of ACT-R (6) was not described in detail (no Table or Figure was presented)

\begin{solution}
Added table and description for default version of ACT-R 6
\end{solution}

\question and no comparative figure outlining the differences in representation between ACT-R and vector-based systems was presented. I find the lack of discussion of the changes in the ACT-R architecture a major concern. The authors reference their prior work, but I believe it requires a separate section in the document to properly evaluate their current model's performance. 

\begin{solution}
Added section that outlines the differences between ACT-R and vector-based systems
\end{solution}

\question - I found throughout the introduction and theory sections that the terminology seemed to waffle between discussion individual user's vs aggregate performance. It wasn't until the methods section that I realized it was due to the difference between how StackOverflow and Twitter information was obtained. I think some extra discussion on the psychological aspects of individual vs group psychology may make your argument stronger. 

\begin{solution}
 Added a paragraph in discussion that addresses this and provides one future research idea
\end{solution}

\question - As previously mentioned, I want to highlight that the theoretical background section is covering a lot of ground but the explanations of each are a bit terse. I think the section could be re-worded for clarity's sake.

\begin{solution}
Addressed by adding previous two sections and additional text describing the retrieval process for vector-based systems
\end{solution}

\question - As I read through the paper, I found it unclear to maintain the exact thesis of the work? What is the goal of the research. While the amount of information collection was impressive, the paper came off much like 'throwing XXXX at the wall and seeing what sticks' rather than a targeted hypothesis of what should work and why. As such, the paper is more explanatory than predictive in the traditional sense, to its decrement since the paper is using cognitively-inspired models.

\begin{solution}
Added additional paragraph in intro
\end{solution}

\question - During the interpretation of the various results, I'm concerned that the graphs do not always accord with the discussion. In some cases, I noticed a small drop in accuracy was within the 95%CI bounds between two conditions, making the difference statistically insignificant, however, these results are reported as being lower and a reason attributed (see p35 for an example). This greatly weakens the overall argument. I recommend performing some inferential statistics on the differences in performance to be sure that all reported effects are in fact significant. Even if they only trend a given way, it's important to note the size of the effect in your discussion.

\begin{solution}
Added section in general methodology describing the method of using CIs instead of explicit p-values throughout the paper
Added CI ranges over the difference scores for each of the model comparison accuracies that are written out
Softened text on p35 since the fine-grained distinctions being made (although likely reliable if a more complicated and powerful statistical model was used to test them) were not necessary for the overall conclusions
\end{solution}

\question - I found the use of your entropy measure for attentional weight to be a key thesis of your paper and was intrigued by its use. I feel that this point could/should be expanded upon. What are the behavioral/cognitive consequences, what is its actual cognitive plausibility, why did you focus on attentional weight (Wj) instead of 'fixing' the Sji equation with some associative learning or the Bi with base-level inhibition? There was not a lot of discussion of other existing changes to the ACT-R architecture/theory that could potentially account for your data. 

\begin{solution}
Expanded the discussion section for attentional weight and the entropy method	
\end{solution}

\question - I was also a bit concerned with the dichotomy between optimized learning and full-on ACT-R learning. Most models that I've ever written (and everyone else I've collaborated with) use ACT-R's hybrid optimized learning equation that remembers the last 5 or 10 references and then averages the rest. This keeps the model computationally efficient while still having the benefits of local spikes in activation due to local high-frequency events (something you'd see in twitter or StackOverflow during conversations). It seems an oversight to have not tested the optimal hybrid equation. It should be relatively easy for you to run your model varying that parameter and report (briefly) on the results. This could potentially drastically change your interpretation of when to use Bayesian vs vector-based models... which was a section I found specious to begin with.

\begin{solution}
Ran the hybrid form with k=1,5,10 and incorporated the results into the past user behavior section
\end{solution}

\question - This may be a copy-editing issue, but I think the heading levels need to be adjusted. It makes some points (see Word Order in the theoretical background) a bit unclear.

\begin{solution}
Heading level was as intended, but reworded the heading to make it more explicit that the section was a subsection of vector-based memory systems
\end{solution}

\question - From personal preference, I find too many sentences starting with "So" when no conclusion is made. This is made worse since many sentences proceeding "So" then start with "Thus" or "Then". This is stylistic but there are numerous places where phrasing comes off as too informal or 'train-of-thought'. 

\begin{solution}
Removed most of them, particularly the ones that were used to start a new paragraph or the ones that the reviewer highlighted in the marked document. Left the rest as I prefer them stylistically over overuse of â€˜consequentlyâ€™, â€˜thereforeâ€™, â€˜thusâ€™, etc.
\end{solution}

\question To summarize:
- Great Scholarship
- Focus on telling a consistent story throughout the paper
- Extend/revise theoretical discussion to include examples, justify changes to ACT-R
- Verify statistics and discussion
- More focus on relevance of attentional weight
- Set optimized learning to hybrid learning

\begin{solution}
All addressed in previous comments
\end{solution}


Reviewer 2: 

\question The authors set out to explore large scale "tagging" datasets with the motivation: "Without using large datasets it is difficult to verify and explore modifications to the declarative memory architecture because the datasets are not large enough or rich enough to rigorously test modifications to the theory."

This certainly did wet the present readers appetite, but unfortunately after reading the manuscript, still starving for clearly defined models, testable predictions, or clear conclusions. 

\begin{solution}
No need to comment
\end{solution}

\question Overall the topic: "declarative memory" and its role in message tagging should be of significant interest to the journal and its audience. However, the novelty in the present manuscript is limited. Neither the way data is used (basically the problem is framed as a supervised label assignment problem with different sub-populations of tagged messages in Twitter and StackOverflow). 

\begin{solution}
Not sure how to address this. I actually find the fact that it looks straightforward a good thing.
\end{solution}

\question The review of the literature and the two model families (ACT-R and vector spaces) is unfortunately not self-contained, rather brief and qualitative. In case the manuscript is re-submitted I suggest that the authors use the opportunity to provide a clear account -from statistical principles to implementation- of the two sets of methods. The original papers are quite frustrating in their mixture of heuristics, arbitrary choice of details, and implementation. I haven't found a paper that gives a clear self-contained account of the relevant theories.

\begin{solution}
Mention the previous additions made (description of ACT-R proper, comparison between the two models, and  additional text describing the retrieval process for vector-based systems)
\end{solution}

\question With respect to the data I think availability is a strong argument. However, there are strong limitations to observational data compared to active experiments (i.e. controlling for message differences by letting multiple users tag the same message)

\begin{solution}
Added a paragraph in the discussion section
\end{solution}

\question The design description lacks crucial details: "Each model was tested by evaluating the accuracy of model-chosen tags on a fresh test subset of the datasets." How was it (sub-)sampled? what was the size?

\begin{solution}
Addressed in the equivalent comment to follow
\end{solution}

\question More importantly I am in serious doubt of the relevance of tagging for "declarative memory". Tagging can best be seen as social behavior/communication. Hence the communication network appears crucial for understanding tag behavior. Specifically, the behavior of the recipient appears crucial to me. In StackOverflow the aim is to get an answer, hence, the main purpose is to get attention from experts, which is more related to expected behavior of the recipient rather than a memory issue. Similarly for Twitter. This issue should at least be discussed.

\begin{solution}
Added a paragraph in the discussion section
\end{solution}

\question It is also not so clear what we have learned from the small differences in performance - can it be explained by differences in model complexity or does it reflect genuine differences in the models of declarative memory?

\begin{solution}
Added a paragraph in the discussion section
\end{solution}

\question It is well known that tags follow heavy tailed (powerlaw) distributions. Hence, normalization and sampling are critical issues. Here we learn "Each model was tested by evaluating the accuracy of model-chosen tags on a fresh test subset of the datasets." How was the test data (sub-)sampled? what were their size and time ordering, how was the tag space controlled /stratified for the test sets?

\begin{solution}
Removed this text at this point, as it was leading the reader to think that I was about to provide more detail about how the sampling was done, and this detail is provided in line for each subsection. For example, much text to follow explicitly defines how each dataset was sampled, and then the methods section for combining prior and context defines how the test data are randomly sampled (including size) apart from the training data, and calls out that data in the test sets are not included to build the co-occurrence matrices
\end{solution}

\question The English is excellent, however the writing could be made considerably more precise; e.g. by clearly defining central concepts, such as "chunk".

\begin{solution}
Added a definition of â€˜chunkâ€™ prior to its first use
\end{solution}

\end{questions}

\end{document}
