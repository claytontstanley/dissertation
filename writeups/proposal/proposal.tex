\documentclass[man]{apa6}

\usepackage{xfrac}
\usepackage{textcomp}
\usepackage[american]{babel}
\usepackage[utf8]{inputenc}

%\usepackage[compact]{titlesec}

\usepackage{tabu}
\usepackage{amssymb,amsmath}
\usepackage{setspace}

\usepackage[hyphens]{url}
%\usepackage{hyperref}
%\usepackage{breakurl}

\usepackage{graphicx}
\usepackage[group-separator={,}]{siunitx}
\RequirePackage[l2tabu, orthodox]{nag}
\graphicspath{{./figures/}} % Specifies the directory where pictures are stored

\usepackage{csquotes}
\usepackage[style=apa,sortcites=true,sorting=nyt,backend=biber,doi=false,uniquename=false]{biblatex}
\DeclareLanguageMapping{american}{american-apa}
\addbibresource{bibliography.bib}

% Needed to ensure periods are placed at the end of every bibliography entry in the references section
\AtEveryBibitem{\clearfield{doi}}

% Make sure only urls that are printed in references section are for webpages
\AtEveryBibitem{
  \ifentrytype{misc}{}{
    \clearfield{url}
  }
}

% ref: http://tex.stackexchange.com/questions/128592/remove-backslashes-from-url-fields-in-bib-entry
\DeclareSourcemap{
  \maps[datatype=bibtex]{
    \map{
      \step[fieldsource=url,
	match=\regexp{\\},
      replace=\regexp{}]
    }
  }
}


% If the toc is too detailed, try limiting the depth of printed sections in the TOC:
%\setcounter{tocdepth}{2}

\title{Psyc 601 Multivariate Statistics Project}
\shorttitle{Psyc 601 Project}

\author{Clayton Stanley}
\affiliation{Rice University}

\leftheader{Beitzel}

\abstract{
  The base StackOverflow tag-prediction model from the Fall '12 Psychometrics course was improved by incorporating the words in the body of the post and the post author's specific tagging history.
  We focused on hashtag creation as a specific human behavior, since understanding and modeling hashtag creation can lead to improved human-computer systems that can better identify users' goals and interests.
  We used multivariate logistic regression statistical techniques to guide the model building process, and found that adding the two model terms improved model accuracy from 56\% to 69\%.
  The model is a successful case showing that ACT-R's declarative memory retrieval equations scale, and are relevant to task domains that require large-scale knowledge stores.
}

\keywords{StackOverflow, Machine Learning, ACT-R, Tagging, Large-Scale Semantic Memory}

\begin{document}
\maketitle

\tableofcontents
\newpage

\section{Motivation}

TODO: Motivation

Social media sites such as Twitter, Facebook, StackOverflow, Google+, and Yelp, are composed almost entirely of human-generated content.
The amount of human-generated content on these sites is staggering, and continues to grow.
Users of Twitter's microblogging service, for example, generate half a billion tweets per day \parencite{TwitterReport2012}.
However, a single user of a social media site is most likely interested in only a small fragment of this large amount of information.
One general question to support the the user in this domain is:
How can we quickly and effectively connect users to the content that they care about?

% FIXME: Fix cite for Bauer2012
One traditional technique is to provide the user with a powerful search engine so that they can actively look for content that interests them.
However, users are not forming the same search queries that work so effectively on search engines like Google.
Users of social media sites are searching for information that is happening now, in real time \parencite{Bauer2012}.
In other words, users on social-media sites want the fresh \emph{stream} of information about a particular topic.
This can certainly be seen in the main user-interface views that these social media site provide:
Twitter's homepage showing your followers' tweets, Facebook's homepage showing your friends' posts, StackOverflow's daily digest of posts for particular tags, etc.

So how can we ensure that users are connected to the information streams that they care most about?
One promising approach is to model and predict the user's goals when using and searching these social media sites \parencite{Rose2004}.
If we have a better understanding of a user's goals, then we can tailor the information streams provided to the ones that are most inline with these goals.
For example, Twitter might suggest other users and hashtags for the user to follow, Facebook could suggest pages to like, Yelp could recommend businesses to check out, etc.
Researchers are currently looking for ways to automate the goal-identification task for more general search queries like those generated for Google \parencites{Jansen2008}{Lee2005}.

However, social media sites have an advantage for identifying user goals over traditional search engines:
human-generated content.
Twitter does not have only the tweet that a user is currently composing to try and identify that user's goals.
Rather, the site has all of the user's previously-generated content, and can use that information to provide a much better prediction of a user's goals.
Further, sites like Twitter, StackOverflow, Google+, and Facebook support ways for the user to explicitly identify their precise goals when creating content: Hashtags.
These user-created hashtags are a key indicator to a user's goals on a social media site. 

These hashtags relate to a user's goals and interests because they are human-generated content, and a form of human-based document tagging \parencite{Chang2010}.
Further, people are using hashtags as a way to create information streams on social media sites \parencite{Kwak2010}.
Twitter users, for example, commonly create hashtags for upcoming political events, such as debates and races \parencite{Diakopoulos2010}.
So if we can predict what hashtags a user is interested in, we have a better understanding of the information streams that they care about.
With that understanding, we can tailor the system to suggest hashtags that they may not yet know about but are of likely interest to them.
We can help aid in their discovery of fresh and relevant information that match their goals on the site.

FIXME: It is certainly the case that real-time search on these social media sites is growing \parencite{Jansen2011}.

\subsection{Research Questions}

The core research question we are interested in is:
What kinds of cognitively-plausible models can best predict the human-generated tags that a user on a social media site is interested in?
This question can be tested by generating a prediction for the most likely used hashtag whenever a user is about to generate a hashtag, and then comparing the user's chosen hashtag to the model's prediction.
This process can be framed as a memory retrieval problem.
That is, each user on a social media site has created content that contain a set of human-generated tags within the context of each post.
The process of suggesting a relevant hashtag to a user can be thought of as a memory retrieval request for a hashtag, given prior hashtag use and current context.

Co-occurrence-based modeling has been shown as a potentially useful approach when predicting Twitter hashtag use \parencite{Efron2010}.
Several memory retrieval models are based on this broad methodology, where a count is maintained of the times each contextual element (such as a word in a post) co-occur with a tag.
Two of the current state-of-the-art memory models are based on a co-occurrence methodology, and we'll be comparing these two models for hashtag prediction:
ACT-R's declarative memory retrieval theory and random permutation vector-based memory systems.

\subsubsection{StackOverflow and Twitter Task Domains}

We will compare these two memory systems across two domains where users generate tags for posts: Twitter and StackOverflow.
Twitter is a micro-blogging service where users create 140 character tweets and broadcast those messages out to the users who follow their posts.
User-created hashtags on Twitter are embedded into the tweet, and can be used to connect the tweet to other tweets about the same topic through common hashtag use.
Some recent and often-used hashtags for Twitter are \emph{\#photography}, \emph{\#startup}, \emph{\#4change}, \emph{\#android}, and \emph{\#solar}.
StackOverflow is a question and answer site for computer programming where users ask programming-related questions and fellow members of the community provide answers.
User-chosen tags on the StackOverflow site represent the primary topic of the question, such as a specific programming language, tool, software package, or framework.
Some example tags for StackOverflow are \emph{PHP}, \emph{Arrays}, \emph{MVC}, \emph{C\#}, or \emph{Common Lisp}.

These two datasets were chosen because the content of the human-generated content is quite different between them, and we are interested in retrieval models that generalize across different task domains.
However, they are also quite similar on several accounts:
Both domains have emassed large amounts of user-generated content, users generate tags for posts when creating content on the site, and the user data from the datasets are publicly available for analysis.

Also, studying user-based tag generation on these sites is a relevant task domain.
Models that can accuratly predict the tags that users will generate on Twitter and StackOverflow can be used as the foundation for recommendations systems on these very popular sites.
These systems can help newer users by recommending proper tags for their newest content.
On the StackOverflow site for example, experts often subscribe to specific hashtags that interest them, and receive a daily digest of posts that are tagged with those hashtags.
Helping the user properly tag posts on StackOverflow ensures that right community sees the post, which greatly increases the chance that the question on the post will be answered quickly and correctly.
For Twitter, hashtags represent streams of information that are possibly interesting to the user.
Having a recommendation system that can suggest relevant hashtags to the user provides a way for the user to connect to and discover new information streams.

\subsubsection{Comparison Between ACT-R and Vector-Based Memory Systems}

ACT-R's declarative memory and vector-based memory systems are substantially different in their implementation.
Nonetheless, both can successfully model the classic fan effect phenomena found in word pairing experiments \parencite{Rutledge2008}.
We will extend the work done by \textcite{Rutledge2008} by formally comparing the models on large-scale hashtag prediction tasks.
We'll also replace the vector-based model used by \textcite{Rutledge2008} with an improved and simplified random-permutation model \parencite{Sahlgren2008}.
The primary objective is to compare the random-permutation vector-based model to ACT-R's declarative memory theory.

An advantage of vector-based models is that word order can be easily incorporated into the single co-occurrence representation \parencite{Jones2007}.
Word-order information on sites like Twitter may contain highly-predictive pieces of information, as it is likely that specific words immediately precede specific hashtags.
However, it is unclear and unexplored how word order can be represented in the ACT-R declarative memory theory.
We will explore how the word-order strength of vector-based models can be incorporated into the ACT-R theory when testing the models on hashtag prediction tasks.

ACT-R's declarative memory system has a strong theory on how a user's prior knowledge and experience influences the likelihood that a particular memory item is retrieved \parencite{Anderson2004}.
However, it is unexplored how a user's prior knowledge should influence retrieval for vector-based models.
We will explore how a user's prior knowledge influences the likelihood that a particular hashtag is generated for both ACT-R's memory theory and vector-based models.

\subsubsection{Lifetime of a Hashtag for a Specific User}

Prior research has examined the growth and decay cycle of hashtag use across users \parencite{Tsur2012}.
However, much less is known about hashtag lifetime within a single user.
It may not be the case that a hashtag's lifecycle for a single user matches the lifecycle across users.
Further, modeling a hashtag's life within users is much more applicable to hashtag prediction, since that model can be directly incorporated into the a specific user's prior likelihood of hashtag use.
So we will characterize the hashtag growth and decay cycle within users.
The expectation is that this cycle will match ACT-R's decay rate used in the declarative memory system to compute a particular memory item's prior.
This decay rate equation formalizes how recency and frequency of hashtag use relate to the prior probability that a hashtag will be retrieved.

\subsubsection{User-Customized Hashtag Prediction}

The end goal of this work is to identify a memory retrieval model that can suggest relevant hashtags to users when they wish to retrieve one.
These hashtags should be customized to their specific interests and relevant to the content of the post that they are currently creating.
This will undoubtably require a combination of two primary model components:
[1] A user's prior likelihood of choosing a particular hashtag, given their prievously-generated content, and [2] the likelihood that a particular hashtag is related to the context of the post being generated. 

A user's prior hashtag use will be an essential model component for domains such as Twitter, where the number of possible hashtags is practically infinite.
In these domains, the model can utilize a user's prior hashtag use as a way to prune the infinite space of possible hashtags, and generate a much smaller and more manageable set for prediction.
However, not only should the model properly take into account a user's prior hashtag use, but it should also generate new hashtag predictions when a user is most likely choosing a hashtag they have never used before.
Calibrating the models to properly balance this exploitation of prior hashtag use vs. exploration of new hashtags will be a primary resaerch focus.
The end goal is to have a model that can properly balance the user's prior tag with the contextual cues in the post in order to generate an accurate prediciton.

\section{Prior Research}

\subsection{Recommendation Systems}

A model that predicts a user-chosen hashtag is a specific type of the more general recommendatino systems.
These systems have grown in need with the growth of the web.
Users on a site need quick access to specific pieces of information, and the site contains much more information than they can process or care about.
Recommendation systems are used to tailor the information presented to what is most relevant to the user \parencite{Pazzani2007}.

\subsubsection{The Netflix Prize}

% FIXME: Fix cite for Bennett2007
The Netflix Prize \parencite{Bennett2007} is an example of a recommendation system where prior user behavior and current context was used to present the most relevant information to the user.
Netflix is an online-subscription service where users can stream their favorite television episodes and movies.
Each user of the site is only interested in a small portion of the entire library of content that Netflix provides.
In order to provide a high-quality user experience on the site, Netflix created a content-recommendation service.
This tool recommends to the user movies and television episodes that most likely match their specific interests.
Netflix wanted to improve the accuracy of their recommendation system, so they held a competition where teams could try to improve on the accuracy of the service.

The winning team utilized a singular value decomposition (SVD) method to identify particular dimensions of the entire content library (e.g., genre, time period, previously watched) that best predicted user preferences.
This is a dimensionality-reducing technique that aims to filter out noise by forcing the data to be represented from a smaller number of dimensions than were originally present.
This method used prior user behavior to classify a user along the dimensions of the reduced space, and current user behavior (e.g., just watched) to understand what they were interested in watching at that moment.
By using an SVD technique and combining these indicators, the model improved on Netflix's original recommendation system by 10\%.

\subsubsection{Recommending Followers on Twitter}

Recommendation systems based on the content of user-generated posts have been successfull in recommending people on Twitter for a user to follow.
\textcite{Hannon2010} built a Twitter follower recommendation system by creating a profile of each user based on the words they used in previous posts.
The recommendation system would then suggest other Twitter users to follow that had similar profiles to a particular user.
The profiles were created by generating a word frequency matrix of words x (user + followers + followees), and then providing that matrix as input into the Lucine text search engine framework.
When the system needed to recommend a set of individuals for a user to follow, it would take all of the words used in posts from that user to construct a search query to be provided to Lucine.
That search query would return the most likely users that share similar words in posts within their twitter network with the words provided in the query.
Using this simple technique based on a standard text search engine allowed the recommendation system to accurately predict the people that a particular user will follow at 25\% accuracy.

\subsubsection{Hashtags for Recommendation Systems}

\textcite{Efron2010} showed how hashtags might be used as a type of recommendation system for Twitter.
Users constructed a query of text that represented a search for a topic that they were interested in.
The system then retrieved the most likely hashtags that were associated with that search query.
The model is based on building a word co-occurance matrix of words x hashtags, and then uses that matrix to assign a similarity score for each hashtag when provided with a search query.
Participants graded how relevant the hashtags returned were to each search query.
The results suggest that hashtags contain useful information that help classify the gist of a post. 
Although this model does not predict hashtags directly when composing a tweet, a model like this could certainly be tailored for that purpose.

\subsection{Hashtag Prediction}

More specific hashtag-recommendation models that attempt to predict a user's chosen hashtag have also been created and tested for a few social media sites.
Google has even deployed a hashtag-recommendation model to help users properly tag posts for their Google+ social media site \parencite{GoogleKeynote2013}.
However, to our knowledge a hashtag-recommendation model for Twitter tweets has not yet been attempted.

\subsubsection{StackOverflow}

\textcite{Kuo2011} was the first to model tag use for StackOverflow posts.
His set of models were originally designed for next-word prediction for large text document collections.
In order to structure the models to work for StackOverflow tag prediction, the models would request the most-likely next word after a user generated a post, and restrict the space of possible words to only tags.
He tested the set of next-word predictino models to generate predictions for the most likely tags that a user would choose when asking a question on the site.
Interestingly, the Bayesian co-occurrance model outperformed the more complicated and computatinoally expensive Singular Value Decomposition and K-nearest-neighbors models.
This model can accurately predict a user-chosen tag at 47\% accuracy.

\textcite{Stanley2013} also modeled a user's chosen tags for a post on the StackOverflow site.
The model was based on ACT-R's declarative memory retrieval theory, which at the core is also a Bayesian co-occurrance model.
However, this model scaled much better than the co-occurrance model presented in \textcite{Kuo2011}.
Because of this, 100 times more posts were used to build the co-occurrance matrix (\num{1000000} versus \num{10000}), which significantly increased the model's performance.
After scaling up the model, this model can successfully predict a user's tag for a post at 65\% accuracy.
It is interesting that in both research papers the best performing model was based on a Bayesian co-occurrance framework, since that is precisely the framework from which ACT-R's declarative memory system is built.
This suggests that more cognitively-plausible memory retrieval models may outperform more general machine-learning approaches in this particular task domain:
predictng the user-chosen tags for human-generated content on social media sites.

Although recent work has been done to model human tagging on the StackOverflow site, to our knowledge no vector-based holographic memory model has been tested or used.
We are curious to see the prediction difference between a vector-based models and current co-occurrance models on the StackOverflow dataset.
Both types of models are neurologically plausible, and both have been successfully incorporated as the declarative memory component of the ACT-R cognitive architecture \parencite{Rutledge2007}. 
Given that the Bayesian cognitively-plausible models have done so well to predict tag use on StackOverflow, we are curious to see how a cognitively-plausible vector-based model performs at the task.

A tag suggestion tool has also been deployed for the tex.stackexchange.org site. \parencite{LatexTags2013}
Both the \LaTeX site and StackOverflow are part of the broader StackExchange network of question and answer sites, and they provide the same user interface and workflow when asking questions.
Once a user has created a question on the Latex site, the site provides subdued text of suggested tags in the text field where the user fills in the tags to associate with the question.
The post author is not forced to use the suggested tags, and is free to choose tags from them or tags not represented in the suggested set.

The Latex site is not nearly as popular as StackOverflow currently.
The site has roughly \num{50000} questions compared to \num{5000000} questions on StackOverflow.
It may be the case that the StackExchange company decided to try out a tag recommendation system on one of their smaller sites first, before deploying this type of system on their most popular StackOverflow site.
Or it is possible that their current tag recommendation system is not efficient enough to work with the demand of the StackOverflow site.
Nonetheless, it is encouraging that StackExchange finds tag recommendation models beneficial enough to the user to start adding them to their question and answer sites. 

\subsubsection{Google+}

The hashtag recommendation system deployed for the Google+ micro-blogging service is probably the most ambitious model of user-created hashtags to date \parencite{GoogleKeynote2013}.
The workflow and user interface provided for recommending hashtags for Google+ is similar to how a hashtag recommendation system for Twitter might look.
When generating content on Google+, the system automatically associates the most likely relevant hashtags with that post.
The user is free to remove any or all of the tags that the system has associated with the post, and can also use tags that were not included in the recommended set.

Google is apparently highly confident in the accuracy of their system.
On the Latex site, the User Interface provides subdued suggestions to the user, but requires the user to explicitly choose each tag he/she wants to associated with the post.
For Google+, the User Interface automatically provides tags for a user's post, and then requires the user to explicitly change those tags if he/she wants to use a different set.
If tag accuracy is high enough, the Google+ technique is certainly favorable, since the user does not have to spend time figuring out which tags best represent this post.
However, if the model makes even just a few errors, users may become annoyed and frustrated with the recommendation system, since they will have to explicitly correct it every time they generate a post.

\subsubsection{Twitter}

Several hashtag recommendation models for Twitter have been developed recently.
One of the first models used a Bayesian co-occurrence statistical technique to predict the most likely hashtag associated with a post as a function of prior hashtag use and context. \parencite{Mazzia2009}
The model present by \textcite{Mazzia2009} is very similar to an ACT-R declarative retrieval model.
The main potential difference is that the global prior likelihood of a hashtag is computed without taking into account the specific user's past history.
Instead, the global prior is an overall average frequency of hashtag use.

Several other models have taken a tweet-centered approach to hashtag prediction, where suggested hashtags are collected from hashtags used from similar tweets.
\textcites{Li2011, Zangerle2011, Kywe2012} all store a content vector for each tweet, and then compute a word co-occurence-based similarity score between a composed tweet and the rest of the tweets in the database.
\textcites{Zangerle2011, Kywe2012} use the Apache Lucine infrastructure to compute the similarity score, while \textcite{Li2011} uses a custom similarity score derived from the WordNet database to compute similarites.
Regardless of the method to compute the similarity between tweets, each method collects the hashtags used from a set of the most similar tweets, then ranks the set, and presents the top 5-10 hashtags to the user.

This tweet-centered approach is quite different than the hashtag-centered approach presented in \textcite{Mazzia2009}.
With a hashtag-centered approach, a direct association is built between the words in a tweet and hashtags that occur in tweets.
With a tweet-centered approach, that association between words and hashtags is indirect:
The relationship is built between a tweet and its content, and then similar tweets are assumed to use similar hashtags.
One problem with the tweet-centered approach is that the storage size grows with the number of tweets, as a vector representatino of the content in each tweet has to be maintained to later compute tweet similarity.
The storage size for a hashtag-centered approach grows with the number of hashtags, but it seems reasonable that this will grow slower than the number of tweets and asymptote after collecting a large sample of tweets.
So from the perspective of efficient information storage and retrieval, it seems more likely that the hashtag-centered approach used in \textcite{Mazzia2009} will scale as Twitter grows in size and use.

\textcite{Kywe2012} also customized their tag predictino model to the user's past hashtag use, and was one of the first to do so when developing a hashtag-prediction model for Twitter.
The set of recommended hashtags was the union of hashtags used in similar tweets and hashtags used by similar users.
This was done by storing a content vector of hshtag use for each user (user-centered approach) alongside the content vectors of hashtag use for each tweet (tweet-centered approach).
To generate recommended hashtags, the model would return the top-ranked hashtags from the hashtags used in 0-50 similar tweets and 0-4 similar users.
Prediction accuracy improved when the model generated recommendations based on hashtags in both similar tweets and the top few similar users compared to basing recommendations on similar tweets alone. 

FIXME: \cite{Godin2013}
This paragraph will discuss the topics-based approach.

FIXME: Paragraph that no vector-based models have been attempted for Twitter.
Bayesian model can be improved by customizing predicitons based on past hashtag use.

FIXME: ditch this paragraph?
To our knowledge, no model of user-created hashtags when composing tweets on Twitter has yet been built.
There are two likely reasons for this:
[1] Modeling user-chosen hashtags and building hashtag-recommendation systems has only explored over the past few years, and
[2] Twitter is a large database of human-generated content, spanning a wide range of topics, containing a sizeable set of user-generated hashtags, so hashtag prediction for Twitter is a relatively hard problem.
We are nonetheless curious if declarative memory retrieval models can be applied to this domain to generate useful hashtag predictions.
The domain shares many similarities with environments where hashtag-recommendation systems have already been deployed:
Google+'s microblogging service is essentially Twitter, but focused on a specific subset of users and topics.
The StackExchange sites are also built from a large database of human generated content, where users have chosen specific tags to associate with posts.
So we will evaluate how well an ACT-R-inspired Bayesian declarative memory model and a new vector-based model predict a user's chosen hashtag when composing a tweet on Twitter.

\subsection{ACT-R Declarative Memory Theory}

ACT-R \cite{Anderson2004} is a cognitive architecture that formalizes how each cognitive process of the brain (e.g., memory, learning, visual and motor) interacts to produce behavior.
The declarative memory system is a component of that architecture that models the timing, learning, and forgetting processes that make up long-term declarative memory retrieval.
The equations that make up this system can be described through a rational analysis of long-term memory retrieval.
That is, given the task of retrieving a chunk of information from long-term declarative memory,
the current context (i.e., external and internal environment state), and past experience (i.e., prior memories and exposure), 
what is the optimal behavior (i.e., the optimal chunk to retrieve from memory)?
Using Bayesian reasoning, each chunk of information in declarative memory can be assigned a prior likilihood of being retrieved again, given the prior number of times it has been needed.
Those prior probabilities for each chunk are then adjusted for the current context, so that the posterior probabilities represent the likilihood that a chunk is needed, given prior odds and current environment state.

\subsubsection{ACT-R DM Model}

\begin{table}[!ht]
  \caption{ACT-R Declarative Memory Model}
  \label{tabACT-RModel}
  {\tabulinesep=1.2mm
    \begin{tabu}{ll}
      \hline
      Common Name &  Equation \\
      \hline
      Activation &	 	$A_{i} = B_{i} + \sum_{j \in c}^{} W_{j} S_{ji}$ \\
      Attentional Weight &	$W_{j} = \frac{W}{n}$ \\
      Base Level & 		$B_{i} = log \sum_{j=1}^{n} {t_{j}}^{-d}$ \\
      Constant Base Level &	$B_{i} = log \frac{p(i)}{p(\overline{i})}$ \\
      Strength of Association &	$S_{ji} = log \frac{p(i|j)}{p(i|\overline{j})} \approx log \frac{p(i|j)}{p(i)}$ \\
      Recall Probability &	$P_{i} = \left( 1 + e^{\frac{\tau - A_{i}}{s}} \right )^{-1}$ \\
      \hline
    \end{tabu}
  }
\end{table}

A formal description of the ACT-R Declarative Memory model is included in Table \ref{tabACT-RModel}.
The total activation ($A_{i}$) for a chunk in declarative memory is a function of two components: base level activation ($B_{i}$) and strength of association ($S_{ji}$).
The recall probability that a chunk will be retrieved from memory increases with total activation ($A_{i}$).

\subsubsection{Base-Level Activation}

Base-level activation reflects the log prior odds of needing an observed chunk again.
The primary way to calculate these log prior odds is to use the standard base level equation in Table \ref{tabACT-RModel}.
This equation formalizes how log prior odds are a function of both frequency and recency of prior exposure to a particular chunk.
Chunks used more frequently (either through exposure or from a retrieval) are more likely to be needed for retrieval again.
However, as time progresses and a particular chunk is no longer used, the activation for that chunk decays.
In this way the standard base level equation formalizes the time dynamics of the retrieval system, where a chunk's base-level activation evolves over time, depending on its current frequency and recency of use.

For some domains, it is reasonable to assume that the base-level activations of each chunk within a particualar window of interest are stable, and do not change within that window.
Programming-language popularity over the past few years is a reasonable example of this.
Although the popularity of various programming languages has certainly changed slightly over the past few years (.e.g, \emph{closure}'s growth), it is not the case that the changes have been drastic.
\emph{C\#}, \emph{Python}, and \emph{Java} are still a few of the most popular languages, \emph{Common Lisp} has not gained or lost much ground in popularity over the past few years, etc.

Further, if this is a reasonable assumption to make, it greatly simplifies the computation of the base-level activation for chunks, and can turn the computation into a tractable problem for large datasets.
In these time-constant domains, one can compute the log prior odds of needing each chunk directly as the log odds ratio of being exposed to a chunk.
For example, if the StackOverflow tag \emph{PHP} has been used four times as often as the tag \emph{Common Lisp}, then the prior odds of needing \emph{PHP} again is $\frac{.8}{.2}=4$ times that of \emph{Common Lisp}.

\subsubsection{Strength of Association}

Strength of association ($S_{ji}$) reflects the amount of log odds adjustment to the activation of a chunk, given the current context (i.e., external environment and internal state).
Context for the StackOverflow domain for example, can be represented as each word in the title and body of a post.
Context for the Twitter domain would be the words in a Tweet.
Association strenght between a chunk in memory and a single contextual element can be computed directly by calculating its context-adjusted odds ratio:
The likelihood a chunk occurred with the current context ($p(i|j)$) over the likeihood that the chunk occured in any of the other contexts ($p(i|\overline{j})$).
For large datasets, the likelihood that a chunk occurs in any particular context reaches near-zero values ($p(j) \Rightarrow 0$).
So it can be assumed that the likelihood that a chunk occurs in any context but one ($p(i|\overline{j})$) is equivalent to the likelihood that a chunk occurs in any context ($p(i)$).
This assumption has been both mentioned and used when deriving the $S_{ji}$ equation \parencite{Anderson1989}, as well as in recent research working with large-scale datasets \parencite{Stanley2013}.

Using this assumption for large datasets, the interpretation of the context-adusted odds ratio becomes much simpler.
If this ratio for a particular tag and context chunk is greater than one, then the log is positive, which means that this context has been observed more often with this tag than in general.
If this ratio is less than one, then the log is negative, which provides a negative adjustment of total activation since this context has been observed more often in general than with this particular tag.

\subsubsection{Connection to Pointwise Mutual Information}

TODO: Introduce PMI \cite{Farahat2004}

TODO: Comparison to ACT-R: Virtually equivalent for large corpora \cite{Farahat2004}

\subsubsection{Scaling the Equations}

\cite{Douglass2010}

TODO: SNIF-ACT \cite{Fu2007} \cite{Pirolli2003}

\subsection{Latent Semantic Analysis Memory Theory}

TODO: Summarize \cite{Landauer1997}

\subsubsection{Singular Value Decomposition}

\subsubsection{Word Order}

\subsubsection{Scaling Issues}

TODO: Compare with PMI, performance and issue of scale. \cite{Budiu2007}

\subsection{Vector-Based Memory Systems}

TODO: Summarize \cite{Plate1995}

\subsubsection{Addressing Word Order}

\subsubsection{Addressing Scalability}

\subsubsection{BEAGLE}

TODO: Sumarize Jones' model \cite{Jones2007}

\subsubsection{Random Permutations}

TODO: Summarize \cite{Sahlgren2008}

\subsubsection{Comparison between BEAGLE and Random Permutations}

TODO: Summarize \cite{Recchia2010}

\subsubsection{Connection to LSA}

TODO: Summarize \cite{Kanerva2000}

\subsection{Comparison of Vector-Based Models and ACT-R}

TODO: Summarize MALTA \cite{Rutledge2007}

TODO: This approach can be used to model the fan effect \cite{Rutledge2008} 

TODO: However, there are open issues that need to be addressed:
TODO: Can user prior be represented in vector-based memory systems?
TODO: Can word order be represented in ACT-R?
TODO: Do they both scale?

\subsection{Incorporating Hashtag Recency}

\subsubsection{ACT-R}

TODO: ACT-R has a strong theory here.
TODO: IR literature for Twitter domain uses a model similar to ACT-R \cite{Efron2011}

\subsubsection{Vector-Based Models}

TODO: Unclear how recency will fit in with random permutation vector-based model.

\section{Models}

TODO: Formal description of each model.

\section{Methods}

\subsection{StackOverflow Dataset}

\cite{DataDump2013}

\subsection{Twitter Dataset}

TODO: common-hashtags dataset
TODO: popular-users dataset

\subsection{Tokenizing and Lemmetizing Text}

TODO: Python NLP toolkit \cite{Bird2009}

\subsection{Stop Words and Entropy Weighting}

TODO: Explore entropy weighting instead of stop word removal \cite{Dumais1991}

\subsection{Experiments}

\subsubsection{StackOverflow Dataset Modifications}

TODO: Use newest dataset
TODO: Chunk tags

\subsubsection{Analyze Recency for User-Generated Hashtags}

TODO: Use stackoverflow and twitter popular-users dataset

\subsubsection{Incorporating Recency Into Random Permutations}

TODO: Use stackoverflow dataset only for random permutations

\subsubsection{Incorporating Word Order Into ACT-R}

TODO: Use twitter common-hashtags dataset only for ACT-R

\subsubsection{Top Hashtag Prediction}

TODO: vector-based and act-r head to head on twitter common-hashtags
TODO: head to head without word order on twitter common-hashtags
TODO: head to head on stackoverflow

\subsubsection{User-Customized Hashtag Prediction}

TODO: head to head on twitter popular-users dataset
TODO: head to head on stackoverflow

\begingroup
\setstretch{1}
\setlength\bibitemsep{12pt}
\printbibliography
\endgroup

\end{document}

