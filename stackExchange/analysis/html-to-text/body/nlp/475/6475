in
a
machine
with
aix
without
perl
i
need
to
filter
record
that
will
be
considered
duplicated
if
they
have
the
same
id
and
if
they
were
registered
between
a
period
of
four
hour
.
i
implemented
this
filter
using
awk
and
work
pretty
well
but
i
need
a
solution
much
faster
:
#
generar
lista
de
duplicados
awk
'begin
{
fs=
''
,
''
}
/ok/
{
old
[
$
8
]
=
f
[
$
8
]
;
f
[
$
8
]
=
mktime
(
$
4
,
$
3
,
$
2
,
$
5
,
$
6
,
$
7
)
;
x
[
$
8
]
++
;
}
/ok/
&
&
x
[
$
8
]
>
1
&
&
f
[
$
8
]
-old
[
$
8
]
<
14400
{
print
$
0
;
}
function
mktime
(
y
,
m
,
d
,
hh
,
mm
,
s
)
{
return
s
+
(
mm*60
)
+
(
hh*3600
)
+
(
d*86400
)
+
(
m*2592000
)
+
(
y*31536000
)
;
}
'
the.big.file.txt
any
suggestion
?
are
there
way
to
improve
the
environment
(
preloading
the
file
or
someting
like
that
)
?
the
input
file
is
already
sorted
.
with
the
correction
suggested
by
jj33
i
made
a
new
version
with
better
treatment
of
date
,
still
maintaining
a
low
profile
for
incorporating
more
operation
:
awk
'begin
{
fs=
''
,
''
;
secsperminute=60
;
secsperhour=3600
;
secsperday=86400
;
split
(
``
0
31
59
90
120
151
181
212
243
273
304
334
''
,
daystomonth
,
``
``
)
;
split
(
``
0
366
731
1096
1461
1827
2192
2557
2922
3288
3653
4018
4383
4749
5114
5479
5844
6210
6575
6940
7305
''
,
daystoyear
,
``
``
)
;
}
/ok/
{
old
[
$
8
]
=
f
[
$
8
]
;
f
[
$
8
]
=
mktime
(
$
4
,
$
3
,
$
2
,
$
5
,
$
6
,
$
7
)
;
x
[
$
8
]
++
;
}
/ok/
&
&
x
[
$
8
]
>
1
&
&
f
[
$
8
]
-old
[
$
8
]
2
)
&
&
(
(
(
y
%
4
==
0
)
&
&
(
y
%
100
!
=
0
)
)
||
(
y
%
400
==
0
)
)
)
{
d2m
=
d2m
+
1
;
}
d2y
=
daystoyear
[
y
-
1999
]
;
return
s
+
(
mm*secsperminute
)
+
(
hh*secsperour
)
+
(
d*secsperday
)
+
(
d2m*secsperday
)
+
(
d2y*secsperday
)
;
}
'