% 
% Annual Cognitive Science Conference
% Sample LaTeX Paper -- Proceedings Format
% 

% Original : Ashwin Ram (ashwin@cc.gatech.edu)       04/01/1994
% Modified : Johanna Moore (jmoore@cs.pitt.edu)      03/17/1995
% Modified : David Noelle (noelle@ucsd.edu)          03/15/1996
% Modified : Pat Langley (langley@cs.stanford.edu)   01/26/1997
% Latex2e corrections by Ramin Charles Nakisa        01/28/1997 
% Modified : Tina Eliassi-Rad (eliassi@cs.wisc.edu)  01/31/1998
% Modified : Trisha Yannuzzi (trisha@ircs.upenn.edu) 12/28/1999 (in process)
% Modified : Mary Ellen Foster (M.E.Foster@ed.ac.uk) 12/11/2000
% Modified : Ken Forbus                              01/23/2004
% Modified : Eli M. Silk (esilk@pitt.edu)            05/24/2005
% Modified: Niels Taatgen (taatgen@cmu.edu) 10/24/2006

%% Change ``a4paper'' in the following line to ``letterpaper'' if you are
%% producing a letter-format document.

\documentclass[10pt,letterpaper]{article}

\usepackage{cogsci}
\usepackage{pslatex}
\usepackage{apacite}

\usepackage[group-separator={,}]{siunitx}

\graphicspath{{./pictures/}} % Specifies the directory where pictures are stored

\title{Predicting Tags for StackOverflow Posts}

\author{{\large \bf Clayton Stanley (clayton.stanley@rice.edu)} \\
  Department of Psychology, 6100 Main Street \\
  Houston, TX 77005 USA 
  \AND {\large \bf Mike Byrne (byrne@rice.edu)} \\
  Department of Psychology and Computer Science, 6100 Main Street \\
  Houston, TX 77005 USA }

\begin{document}

\maketitle

\begin{abstract}
  A model was built for the StackOverflow dataset that can predict the tags that will be used by the author of the post, given the content of the post.
  The tag-predictor model is a cognitively-inspired Bayesian probabilistic model based on ACT-R's declarative memory retrieval mechanisms.
  Large-scale relational database techniques and efficient sparse matrix data structures were used to implement the model,
  so that the data structures require only 3GB of memory and retrieval requests with a DM dataset of \num{556677795} chunks occur in less than one second on a single processor.
  A logistic regression technique was used to calibrate model parameters and measure performance.
  Model fit was satisfactory ($R_{pseudo}^{2}=.46$, npv = .997, ppv = .70) and model performance in a real-world setting was fairly strong (67\% accuracy when asked to tag one tag per post on average).

  \textbf{Keywords:} 
  Machine Learning; ACT-R; Large-Scale Declarative Memory
\end{abstract}

\section{Introduction}

Human-based tagging of online content has increased in recent years.
A large portion of the growing number of social media sites support some sort of human-directed tagging of posts.
Additionally, an increasing number of individuals are using tags as a form of query-based search to find information
(e.g., \citeNP{Diakopoulos2010}).

Although much good work has been done to characterize the lifetime and growth characteristics of hashtags for real-time social networking sites such as Twitter
(e.g., \citeNP{Bauer2012}, \citeNP{Tsur2012}, \citeNP{Chang2010})
less is known about a person's motivation for choosing particular tags during the creation of the tweet or post.
Modeling tag creation accurately (as opposed to modeling tag growth and behavior after creation) could allow a system to predict the hashtag that a user should use, given the contextual clues available.
The benefits of this type of prediction are numerous.
For example, users could be introduced to tags (and potentially communities surrounding those tags) that are of interest to them.
Also tag cleanup systems could be implemented where mistags are identified and corrected.

A tag-prediction system was built here for the StackOverflow dataset that can solve precisely these sorts of problems.
The StackOverflow dataset was chosen because it is relatively constrained, contains a large number of posts and associated tags, and the data are easy to obtain.
It is constrained because there is a fully enumerated and limited set of tags that an author can use, as opposed to a less-constrained domain such as Twitter.
The site also makes public all posting data on a quarterly basis, which can be easily downloaded and loaded into a relational database for further analysis.

\section{Method}

A variation of ACT-R's declarative memory retrieval theory \cite{Anderson2004} was used to model tag retrievals for StackOverflow posts.
The hypothesis is that the tags used for posts are based on a tag's prior use and the strength of association between the tag and the content of words in the title and body of the post.
The strength of association between post words and tags was calculated using 2/3 of the dataset (1M posts).
A logistic regression statistical technique was used to calibrate model parameters and optimize performance.
The weights for each model component were calibrated using a sample of 1,000 posts not contained within the strength of association dataset.
The calibrated model was then tested on a new sample of 1,000 posts not contained within either previous training dataset.
Classification accuracy was used as a metric of model performance (i.e., how many of the model's predicted tags are correct).

\subsection{Stack Overflow Dataset}

The StackOverflow dataset consists of all questions and answers posted on stackoverflow.com.
The questions are all programming related and can be posted by anyone with an account, which is free to create.
Each question is tagged by the author with (presumably) the tags that are most representative of the post.
Example tags are programming languages (PHP, Common-Lisp, MySQL, C\#) and also general topics (databases, optimization, arrays).
An example post is included in Figure \ref{fig:examplePost}.
Note that the author tagged this post with javascript, firefox, dom, and svg.

\begin{figure}[ht]
  \centering
  \resizebox{\linewidth}{!}{\includegraphics{visPost-2977094-screen-cropped.pdf}}
  \caption{Example Post}
  \label{fig:examplePost}
\end{figure}


The Apr' 11 dataset \cite{DataDump2011} was downloaded and imported into the MySQL relational database system.
The dataset contains \num{4945168} posts from \num{559803} unique users.

\subsection{Measuring Co-occurrences}

Model tag predictions are based on the amount of word co-occurrences between words in the title and body of the post and tags used for that post.
To build the co-occurrence matrix between post words and tags, all title and body text and associated tags were extracted from the relational database.
The Python Natural Language Processing Toolkit \cite{Bird2009} was used to chunk (i.e., tokenize and lemmatize) the text in the post.
The tags were already chunked.
However for the tags, tag synonyms were converted to their root tag using the community-maintained StackOverflow tag synonym database.
Afterwords the processed word chunks and root tags were fed back in to the database.

A query was then used to build the post word, tag word co-occurrence matrix.
For a given word found in the body or title of a post, the query counts the number of times that that word appears with each tag.
Each of those counts is recorded as the word co-occurrence for that word and each associated tag.
This process is done across all words found in any post.
The result is a set of context word, tag, count for each of the \num{39223968} unique context word, tag combinations.

\subsection{Measuring Tag Occurrences}

Model tag predictions are also based on the frequency that each tag has been used.
More often used tags more likely to have higher activation.
A SQL query was used to build a similar set of tag use counts for the \num{26176} unique tags.
This set for example would show that C\# is the most often used tag, where about 11\% of posts were tagged with C\#.

\subsection{Generating Model Predictions}

For a given title and body of a post, the model returns the activations for each possible tag.
The tag with the highest activation is the most likely tag to be associated with that post.
Tag activation is a function of four components:
The tag's base level activation (prior odds of being used),
contextual activation for words in the title (word co-occurrence weighting),
contextual activation for words in the body,
and an offset to equalize activation values for tags across all posts.

Contextual activation is computed by comparing the words in the post with each tag's word co-occurrence vector.
If a tag's co-occurrences correlate strongly with the words in the post, then that tag's activation will be high.
If there is a low correlation between the observed co-occurrences for a tag and the observed words in the post, than that tag's activation will be low.
The equations that operationalize this process will be included in the Model section.

The R statistical programming environment was used to build the model and generate model predictions.
This was done by feeding the word co-occurrence and tag occurrence files generated by the database queries into a separate sparse matrix and sparse vector in R.
A hash was computed for each unique word, so that indices for the sparse matrix and vector could be numeric.
The dimensions for the co-occurrence sparse matrix were context word, tag, and the tag occurrence vector was 1 dimensional (tag).

1 million posts (approximately 2/3 of all posts) were fed into R to build the co-occurrences and tag occurrences.
An interface to the model was developed so that one could supply a vector of words, and the model would return a sparse vector of tag activations given that context.
In order to generate a model prediction, tag activations for title and body context were computed separately, and then added to each tag's prior base-level activation.
An offset constant (equal to the average activation of the top 5 model-predicted tags for the post) was then subtracted from the prior value.
The resulting tags were rank sorted, and the one with the highest activation would then be the model's prediction of the most likely tag associated with that post.

\subsection{Measuring Model Fit}

For a given post, the model produces a vector of tag activations.
For that post, the author's chosen tags are known (see Figure \ref{fig:examplePost}).
So model performance can be measured by comparing the model's tag activations with chosen tags for a post, and then aggregated across posts to get an overall average performance.
If the model consistently produces higher tag activations for chosen tags than non-chosen tags, the model should be able to accurately predict the chosen tags.

A logistic regression technique was used in order to operationalize this fit measure.
For each post, three sets of tag groups were combined to most accurately sample tag activations while managing computer resource constraints:
The model's tag activations for the highest 200 tags, a weighted sample of 800 tags (weighted towards higher activation), and the tag activations for all tags that were chosen by the author of the post.
Each of these observations was assigned a category of 0 or 1 depending on if the tag was a chosen tag or not for that post (0 is not chosen).
Conceptually, if the model has good categorization power, the observations assigned 1 should have higher tag activations than the observations assigned 0.

These vectors of tag activation, categorization were collected across a set of 1,000 posts.
The set of posts did not include any posts that were used to train the model on word association strength.
So a form of cross validation was used to measure model fit, although the more robust full k-fold cross validation technique was not used (due mostly to resource and time limitations).

The base R glm function (with 'binomial' argument) was used to perform the logistic regression.
The parameters were optimized to best predict categorization (chosen tag, not chosen tag) from the four predictor variables:
The tag's prior occurrence frequency, word co-occurrence activation for title and body words, and activation offset for the post.
Further discussion and operationalization of the predictor variables is included in the Model section.

\section{Model}

The tag-prediction model is a cognitively-inspired Bayesian probabilistic model, based on ACT-R's declarative memory retrieval mechanisms \cite{Anderson2004}.
These equations describe the likelihood that a chunk from memory will be retrieved, given the prior odds of retrieving the chunk and the current context.
In this way the task of retrieving the tag with the highest activation from the words in the post is similar to performing a declarative memory retrieval request for a tag, given the context of title and body words.

This model is also similar in design to the approach used by SNIF-ACT (\citeNP{Fu2007}, \citeNP{Pirolli2003}). 
SNIF-ACT can predict link traversal for search queries.
That is, given a search query (goal state), fetched results, and words included in those fetched results, the model predicts the most likely link that a person will click on.
This StackOverflow model is similar because it leverages ACT-R's declarative memory retrieval mechanisms to generate predictions (tag activations instead of link activations).
However, it is slightly different because it takes into account prior odds of tag activations (set to zero in SNIF-ACT).
This was set to zero in SNIF-ACT because each link has multiple words, so looking at word co-occurrences between the search query and link words produces a stable predictor.
It's also the case that search engines already rank order search results.
So even if prior odds for link activations were used in the SNIF-ACT model, the prior odds for the top few links would be virtually identical, so model predictions would not change much.
For the StackOverflow dataset, each tag is a single word, and those tags are not already rank ordered.
So ACT-R's prior odds component of the declarative memory retrieval equations was included for the StackOverflow model.

\subsection{Equations}

A formal description of the model's equations are included in Table \ref{sample-table}. 

\renewcommand{\arraystretch}{1.5}% Wider
\renewcommand{\tabcolsep}{.5mm}
\begin{table}[!ht]
  \begin{center} 
    \caption{The StackOverflow Tag-Prediction Model} 
    \label{sample-table} 
    \vskip 0.12in
    \begin{tabular}{ll} 
      \hline
      Common Name &  Equation \\
      \hline
      Activation & 		$A_{i} = B_{i} + \sum_{j\in T}^{ } W_{j} S_{ji} + \sum_{j\in B}^{ } W_{j} S_{ji} - O$ \\
      Base Level & 		$B_{i} = log \frac{p_{i}}{1-p_{i}}$ \\
      Strength Assoc. &		$S_{ji} = log \frac{p(i|j)}{p(i))} = log \frac{NN(j,i)}{N_{Row}(j)N_{Col}(i)}$ \\
      Attention Weight	& 	$W_{j} = WE_{j}$ \\
      Scaled Entropy & 		$E_{j} = 1-\frac{H_{j}}{H_{max}}$ \\
      Entropy & 		$H_{j} = -\sum_{i=1}^{N}p(i|j)log\left (  p(i|j) \right )$ \\
      Max Entropy & 		$H_{max} = maxH$ \\
      Offset & 			$O = \frac{1}{5}\sum_{i\in top 5}^{ } A_{i}$ \\
      \hline
    \end{tabular} 
  \end{center} 
\end{table}

Here, the $i$ subscript denotes activation for a particular tag $i$, and the $j$ subscript is for context.
$A_{i}$ is total activation for tag $i$.
Note the similarities between the StackOverflow model and ACT-R's retrieval equations (base level and contextual component, log odds of prior being adjusted by association strength with context).

\subsubsection{Base-Level Activation}

Base-level activation ($B_{i}$) is the log prior odds of observing tag $i$ in the world.
So the tag with the highest frequency (C\#) would have the highest base-level activation.
Tag frequency drops off sharply for this dataset.
The top 1\% of tags based on frequency of use account for approximately 60\% of total tag instances.
This is why accounting for tag frequency (by including ACT-R's base-level activation component) is so important for the StackOverflow dataset.
Otherwise without context, the likelihood of choosing C\# as a tag would be the same as choosing ora-00997 (one of the many tags with only a single occurrence of use).
One might argue that with context, it's unlikely that the words associated with ora-00997 will occur again, so pure context would be able to still often predict C\#.
However, the ACT-R theory asserts that pure context is often not enough to overcome high base-level activations (e.g., it can be difficult to break a bad habit).

\subsubsection{Contextual Activation}

Contextual activation can be thought of as the amount of odds adjustment to base levels, given the current context $j$.
Contextual activation has two components: strength of association ($S_{ji}$) and attentional weight ($W_{j}$).
Strength of association measures the relatedness of a contextual element $i$ (e.g., the word PHP in the title of a post) with a tag $j$ (e.g., the tag PHP5).

\subsubsection{Attentional Weighting}

The second component of contextual activation, attentional weighting ($W_{j}$), can be thought of as the amount of attentional resources that are dedicated to contextual element $j$.
Total attentional weighting ($W$) is bound, most often set to 1, and reflects the fact that our attentional resources are limited.
For the StackOverflow model, the attentional weighting was left unbound, and allowed to be calibrated by the logistic regression model.
This was done for two main reasons.
First, the researcher was curious to see what value the logistic regression model would give for $W$.
Second, a non-ACT-R standard measure of attentional weighting was used ($E_{j}$), so it seemed appropriate to allow $W$ to vary alongside exploring this new weighting measure. 

\subsubsection{Entropy}

During initial model testing, the attentional weighting for all words was equal.
That is initially, for a given post with $n$ title words, the attentional weighting $W_{j\in C}$ for all of those words was $W/n$, where the logistic regression model fit $W$ to range between 1-3.
The problem with this approach was that familiar stop words like 'the' and '?', that were observed so often and for all tags, had the same attentional weighting as highly predictive words like 'PHP'.
Granted the $S_{ji}$'s across all tags for context words like 'the' are much lower (and more evenly distributed) than the few $S_{ji}$'s that spike for 'PHP' (e.g., the tag PHP).
So the amount of adjusted log odds from stop words was low, even with equal attentional weighting (since their $S_{ji}$'s were low).
However, it still seemed problematic for two reasons.
First, intuitively it seemed that regardless of word co-occurrence association strengths, because stop words like 'the' are observed with all tags, they are not highly predictive of any tag.
So one could argue that their presence is simply adding error in the model's tag activation calculation.
Second, because $W$ is held constant across posts, any attentional resources that are allocated to stop words like 'the' and '?' are taken away from being used for highly predictive words like 'PHP'.

It's the case that there are many connections and commonalities between ACT-R's declarative memory retrieval equations and other non-ACT-R information retrieval theories,
and many of those theories use a combination of weighting schemes for contextual relevance and global importance of a particular chunk in context.
For example, \citeA{Dumais1991} shows that a measure of how likely term ${_j}$ is in document ${_i}$ is often the product of a local weighting $L_{ji}$ and global weighting $G_{j}$ measure.
This idea maps rather nicely to ACT-R's contextual weighting.
That is, terms are context words ${_j}$, documents are tags ${_i}$, local weighting is strength of association $S_{ji}$, and global weighting is attentional weighting $W_{j}$.
\citeA{Dumais1991} states that predictions tend to improve when a global weighting scheme is used, so that words that occur frequently or with many documents (tags) are not weighted as high.
He reviews four different commonly-used weighting schemes; one of which is a scaled entropy measure.

The scaled entropy measure $E_{j}$ was chosen as a global weighting measure for this dataset because it has a strong theoretical foundation and it predicts well (\citeNP{Dumais1991}).
It is based on Shannon's Information Theory, which measures the amount of information content (predictiveness) of a word.
Entropy measures that are mathematically equivalent to Shannon's formulation are used in a wide range of applications, including data compression, cryptography, and thermodynamics.
The scaled entropy ($E_{j}$) of a context word can be thought of as the amount of predictiveness that that word has to any tag.

For example, a stop word like 'the' will have an even distribution of co-occurrences across all tags.
That is, seeing the word 'the' in context is not highly predictive of any particular tag.
This means that the marginal probabilities of 'the' with all tags will be roughly equal.
It can be shown that maximum entropy $H_{j}$ is reached when all marginal probabilities are equal.
This makes intuitive sense because if all marginal probabilities are equal, there is no information gained in observing the context word.
The scaled entropy measure $E_{j}$ transforms the entropy measure to a scale that ranges from 0, 1, where maximum entropy means lowest scaled entropy.
This means that a stop word like 'the' will have a scaled entropy measure close to 0, while a highly-predictive word like 'macro' will have a scaled entropy measure close to 1.
Going back to ACT-R's attentional weighting $W_{j}$, if $E_{j}$ is low then $W_{j}$ is low, meaning that the scaled entropy measure helps allocate the limited attentional resources to predictive contextual elements.

\subsubsection{Offset}

An offset measurement (mean of the top five tags for the post) was used to equalize the top activated tags for each post.
This was done primarily to improve model fit.
To see why this is the case, suppose the following scenario:
The offset measurement is not used, and the model is asked to provide ten tags across ten posts.
The top ten tags for one of these posts have activations that are higher than activations for all other tags in the other nine posts.
The model would then select the ten tags from the single post as the most likely tags.
But this doesn't make intuitive sense, because there is a limit to the number of tags that people tend to use for each post (average number of tags per post is approximately 3).
So the offset measure aims to place all of these ten posts on the same level, so that asking the model to choose ten tags from ten posts will select roughly a single tag from each post.

\section{Results}

After training the model's spreading activation matrix and calibrating model parameters, the model can predict 67\% of tags correctly when asked to generate an average of one tag per post.

\subsection{Logistic Regression}

In order to formally test model performance and to calibrate model parameters, a calibration run using \num{1000} posts was performed.
For the calibration run, the category prediction (chosen or not chosen tag for a post) was regressed on base-level activation, spreading activation for title and body, and post offset.
Best-fit coefficients for each model component and formal statistical measurements are included in Tables \ref{tab:coeffs} and \ref{tab:fits} respectively.

\renewcommand{\arraystretch}{1}% Wider
\renewcommand{\tabcolsep}{1mm}
\begin{table}[!ht]
  \begin{center} 
    \caption{Best-fit coefficient estimates from logistic regression} 
    \label{tab:coeffs} 
    \vskip 0.12in
    \begin{tabular}{lllll} 
      \hline
      Coefficient & 	Estimate &	Std. Err. &	z value &	$Pr(>|z|)$  \\
      \hline
      (Intercept) &	-1.35 &		0.10 &		-13.59 &	\textless2e-16 *** \\
      prior &		1.08 & 		0.01 &		88.12 & 	\textless2e-16 *** \\
      sjiTitle &	1.41 &		0.02 &		57.36 &		\textless2e-16 *** \\
      sjiBody &		2.36 &		0.04 &		58.48 &		\textless2e-16 *** \\
      offset &		-0.71 &		0.02 &		-39.05 &	\textless2e-16 *** \\
      \hline
    \end{tabular} 
  \end{center} 
\end{table}

\renewcommand{\arraystretch}{1}% Wider
\renewcommand{\tabcolsep}{3mm}
\begin{table}[!ht]
  \begin{center} 
    \caption{Model fit metrics} 
    \label{tab:fits} 
    \vskip 0.12in
    \begin{tabular}{ll} 
      \hline
      Common Name &  Value	\\
      \hline
      McFadden's $R_{pseudo}^{2}$ &	.46 \\
      True Positives &			647 \\
      False Positives &			278 \\
      True Negatives &			\num{796982} \\
      False Negatives &			\num{2235} \\
      Positive Predictive Value &	0.70 \\
      Negative Predictive Value &	0.997 \\
      \hline
    \end{tabular} 
  \end{center} 
\end{table}

Note that all four coefficients for the run are strong predictors of tag use ($p<.01$), and model fit is satisfactory ($R_{pseudo}^{2}=.46$).
It's also interesting to note that although the attentional weighting term ($W$) was allowed to be calibrated by the regression, it did not stray far from one
(see coefficients for title and body in Table \ref{tab:coeffs}).
Apparently the scaled entropy measure that globally weights each cue allows for total attentional resources ($W$) to remain relatively small (around one) and not stray far from cognitively-plausible defaults (one).

A plot of model performance for the logistic regression for the calibration run is included in Figure \ref{fig:logReg}.
Tag-not-chosen observed instances are collected on the bottom and tag-chosen instances are on the top.
Looking at the histograms, note that model can not perfectly discriminate between tags that will and will not be used for a single post (overlap in distributions).
Once further improvements to the model are found, these will increase the separation between the two histograms.

\begin{figure}[ht]
  \centering
  \resizebox{\linewidth}{!}{\includegraphics{foo-cropped.pdf}}
  \caption{
    Logistic regression of tag used on post (binary) regressed on four components of model tag activation.
  }
  \label{fig:logReg}
\end{figure}

\subsection{Example Posts}

A visualization of the model's tag activation for the example post in Figure \ref{fig:examplePost} is included in Figure \ref{fig:modelPost}.

\begin{figure}[ht]
  \centering
  \resizebox{\linewidth}{!}{\includegraphics{visPost-2977094-act-cropped.pdf}}
  \caption{
    Model's prediction of top-ten tags for post in Figure \ref{fig:examplePost}.
    The title is the title text of the post.
    The attentional weight (entropy) for each word in the title is included below each word.
    The user-chosen tags are included below the attentional weights.
    The stacked bar graphs are the total activation for the top-ten predicted model tags for the post.
    Each bar graph is partitioned into the 3 model components that vary between tags within a post (not the post offset).
}
  \label{fig:modelPost}
\end{figure}

Note how the base level activation and contextual activation combine to predict tags that are both frequently used and contextually relevant.
For example, note that the 'javascript' tag leverages a combination of contextual activation and prior activation to achieve a high total activation.
Alternatively, the 'svg' tag relies almost entirely on context to achieve high activation.
And note that the model is by no means perfect.
For example, 'html' is not tagged by the author, but the model ranks it as the second highest tag.
And the author's tag 'firefox' is only ranked 8th by the model.

\subsection{Generalizability}

In order to test how well the model (and calibrated parameters) generalize to the rest of the StackOverflow dataset, the model was tested on an additional set of \num{1000} posts.
These were fresh posts, and not previously used to either train the strength of association matrix or calibrate model parameters.
The model parameters from the calibrated dataset were held constant and used on the test dataset.
Model classification accuracy as a function of number of tags predicted by the model was used as the metric to test if the model generalized.

\subsection{Classification Accuracy}

A plot of model performance for both the calibration and test dataset is included in Figure \ref{fig:ROC}.

\begin{figure}[ht]
  \centering
  \resizebox{\linewidth}{!}{\includegraphics{ROC-cropped.pdf}}
  \caption{
    Model performance across the calibration and test dataset, as well as when specific model components are removed.
    The vertical dotted line represents the point where the model is asked to tag on average one tag per post.
  }
  \label{fig:ROC}
\end{figure}

Note how model performance is almost identical for the calibration and test dataset (compare full model calibration to full model test).
This indicates that the model parameters were not overfit, and generalize well to the rest of the dataset.
It also provides evidence that \num{1000} posts is an adequate number to estimate model parameters accurately.

\subsection{Necessity Analysis}

Figure \ref{fig:ROC} also shows model performance after specific model components are removed.
Model parameters were recalibrated for each model, except for the full model test dataset which used the parameters from the calibration dataset.
These plots provide useful information regarding the relative necessity of each component in the full model.
That is, which model components contribute most strongly to increased performance, and which model components might be redundant (not necessary).
Note that it is certainly the case that each model component contributes positive and unique variance to model performance.
However, removing only one component (particularly the words in the body of the post) does not drastically reduce model performance.
It's only when groups of components are removed that model performance starts to drastically suffer.

\section{Discussion}

\subsection{Related Work}
Predicting tags for the StackOverflow dataset has been worked on by at least one prior researcher \cite{Kuo2011}.
\citeA{Kuo2011} only looks at the Stack Overflow dataset briefly at the end of the technical report.
However he does mention that the co-occurrence model he used has about a 47\% classification accuracy for predicting StackOverflow tags, when required to predict one tag per post.
The full model used here achieves 67\% classification accuracy when predicting one tag per post on average (height of full model at vertical dotted line in Figure \ref{fig:ROC}.
The increase in model performance is most likely due to FIXME.

\subsection{ACT-R and Scale}
\num{39223968} unique context word, tag combinations were contained within the \num{1000000} posts that were used to build the strength of association matrix.
Since the model is based on ACT-R's declarative memory retrieval equations, it could have been implemented in Common Lisp (programming language for ACT-R's implementation).
However, it is known that the current implementation of spreading activation (required for $S_{ji}$) in ACT-R does not scale to an $S_{ji}$ corpus over 100,000 (\citeNP{Douglass2009}, \citeNP{Douglass2010}).
So it was decided to use a one-off R implementation of ACT-R's retrieval equations for this analysis.
This is much like how \citeA{Douglass2010} implemented the equations in Erlang when using the theory to test the fan effect on large-scale declarative memory datasets.

However, it is increasingly likely that there is no fundamental reason why vanilla ACT-R (Common Lisp version) cannot be optimized to handle spreading activation calculations with datasets this large.
All that is really needed is a more appropriate data structure to handle the large scale.
This can be seen by comparing the implementation of spreading activation here to the implementation in \citeA{Douglass2010}.
Both implementations handle sji counts greater than \num{1000000}, with declarative memory retrieval times (time to calculate $A_i$'s) around 1 second at this scale.
However, the implementations could not be more different.
The Erlang implementation leverages a semantic network of millions of isolated process chunks and massive concurrency to achieve the scale.
The R implementation here is single threaded and requires only a sparse matrix implementation to achieve the scale.
Since the programming languages are different, it's not likely the case that only a specific programming language can achieve the scale.
And since the parallelism is different (single threaded vs multi threaded), it's not likely the case that achieving the scale can only be done with massively concurrent implementations.
So, it's most likely the case that using an appropriate data structure is all that is needed for ACT-R Common Lisp to handle sji counts in the millions and above.

\subsection{Entropy and Stop Words}

The Python NLTK documentation \cite{Bird2009} recommended to remove stop words from the analysis.
Stop words are low-predictor words, and the documentation listed out a short list that were commonly removed (e.g., 'the', 'and', 'or')
However it seemed somewhat arbitrary to explicitly list out a few stop words and then simply choose those as the words to remove.
Looking at the scaled entropy measure, it seems quite plausible that this measure of word predictiveness can also be used to identify stop words.
That is, words with scaled entropy close to zero are the stop words.
Looking at the lowest ten scaled-entropy words for this dataset, (does, anyone, 've, found, are, seems, has, been, for, support), using scaled entropy to determine stop words seems to work fairly well.

\subsection{Entropy and ACT-R's Attentional Weighting}

Model performance improved by using a scaled entropy measure to weight the importance of each contextual element in the body and title of a post.
However, it's uncommon for an ACT-R model that uses the declarative memory retrieval mechanisms to use anything other than equal attentional weighting for each chunk in context.
But it is much more common outside of the ACT-R modeling community to use both a local (strength of association) and global (attentional weight) term to predict document retrieval \citeA{Dumais1991}.
In fact, it's been shown that the Pointwise Mutual Information Index (PMI) (a commonly-used measure to predict document retrieval) is mathematically equivalent to ACT-R's retrieval equations when the sample size of the corpus is large (\citeNP{Budiu2007}, \citeNP{Farahat2004}).
Both global and local weightings are commonly used when the PMI measurement is used.
This suggests that using a global weighting term for chunks in context (e.g., the scaled entropy measure used here) alongside the more commonly-used local weighting (spreading activation) might be useful for other ACT-R models that are taking advantage of large stores of declarative memory datasets to build up the model's base knowledge.

\subsection{Future Work}

Classification accuracy for the model is already fairly high.
However, there is certainly room for improvement and unexplained variance left to account for.
In particular, the prior activations for each tag are not (currently) dependent on the particular user that is writing the post, and it seems plausible that prior activations for tags would change for each user.
That is, if a user has repeatedly tagged questions with Common Lisp, and never with C\#, it seems unrealistic to use the general prior activations for this case, and give C\# the highest prior, and Common Lisp a relatively low prior.
Instead, it may be worthwhile to maintain prior tag activations for each user, and combine this prior with the global prior when predicting tags for a particular post.
And this approach could be fielded as well, since each user has to sign into their account before posting a question on the site.

\section{Acknowledgments}

I would like to thank Dr. Mike Byrne and Dr. Fred Oswald for their excellent guidance and recommendations throughout the model development and data analysis process.

\bibliographystyle{apacite}
\setlength{\bibleftmargin}{.125in}
\setlength{\bibindent}{-\bibleftmargin}
\bibliography{bibliography}

\end{document}
