% 
% Annual Cognitive Science Conference
% Sample LaTeX Paper -- Proceedings Format
% 

% Original : Ashwin Ram (ashwin@cc.gatech.edu)       04/01/1994
% Modified : Johanna Moore (jmoore@cs.pitt.edu)      03/17/1995
% Modified : David Noelle (noelle@ucsd.edu)          03/15/1996
% Modified : Pat Langley (langley@cs.stanford.edu)   01/26/1997
% Latex2e corrections by Ramin Charles Nakisa        01/28/1997 
% Modified : Tina Eliassi-Rad (eliassi@cs.wisc.edu)  01/31/1998
% Modified : Trisha Yannuzzi (trisha@ircs.upenn.edu) 12/28/1999 (in process)
% Modified : Mary Ellen Foster (M.E.Foster@ed.ac.uk) 12/11/2000
% Modified : Ken Forbus                              01/23/2004
% Modified : Eli M. Silk (esilk@pitt.edu)            05/24/2005
% Modified: Niels Taatgen (taatgen@cmu.edu) 10/24/2006

%% Change ``a4paper'' in the following line to ``letterpaper'' if you are
%% producing a letter-format document.

\documentclass[10pt,letterpaper]{article}

\usepackage{cogsci}
\usepackage{pslatex}
\usepackage{apacite}
\usepackage[none]{hyphenat}

\usepackage[group-separator={,}]{siunitx}

\graphicspath{{./pictures/}} % Specifies the directory where pictures are stored

\title{Predicting Tags for StackOverflow Posts}

\author{{\large \bf Clayton Stanley (clayton.stanley@rice.edu)} \\
  Department of Psychology, 6100 Main Street \\
  Houston, TX 77005 USA 
  \AND {\large \bf Michael D. Byrne (byrne@rice.edu)} \\
  Department of Psychology and Computer Science, 6100 Main Street \\
  Houston, TX 77005 USA }

\begin{document}

\maketitle

\frenchspacing

\begin{abstract}
  A model was built for the StackOverflow dataset that can predict the tags that will be used by the author of the post, given the content of the post.
  The model is a cognitively-inspired Bayesian probabilistic model based on ACT-R's declarative memory retrieval mechanisms.
  Large-scale relational database techniques and efficient sparse matrix data structures were used for implementation,
  so that the data structures require only 3GB of memory and retrieval requests with a DM dataset of \num{556677795} chunks occur in less than one second on a single processor.
  A logistic regression technique was used to calibrate model parameters and measure performance.
  Model fit is satisfactory (McFadden's pseudo $R_{}^{2}=.46$, npv = .997, ppv = .70) and model performance on the test dataset is fairly strong (67\% accuracy when asked to tag one tag per post on average).

  \textbf{Keywords:} 
  Machine Learning; ACT-R; Large-Scale Declarative Memory
\end{abstract}

\section{Introduction}

Human-based tagging of online content has increased in recent years.
A few examples of human-based tagging of information are hashtags for Twitter tweets and Facebook posts, as well as keywords for research papers.
A large portion of the growing number of social media sites support some sort of human-directed tagging of posts.
Additionally, an increasing number of individuals are using tags as a form of query-based search to find information
(e.g., \citeNP{Diakopoulos2010}).

There is a growing body of research that aims to characterize the lifetime and growth characteristics of hashtags for real-time social networking sites such as Twitter
(e.g., \citeNP{Bauer2012, Tsur2012, Chang2010}).
However, less is known about the underlying process when choosing particular tags during the creation of the tweet or post.
Modeling tag creation accurately (as opposed to modeling tag growth and behavior after creation) could allow a system to predict the hashtag that a user should use, given the contextual clues available.
There are numerous benefits to this type of tag prediction.
For example, users could be introduced to tags (and potentially communities surrounding those tags) that are of interest to them.
Also tag cleanup systems could be implemented where mistags are identified and corrected.

We built a tag prediction system for the StackOverflow dataset that can solve precisely these sorts of problems.
The StackOverflow dataset was chosen because it is relatively constrained, contains a large number of posts and associated tags, and the data are easy to obtain.
It is constrained because there is a fully enumerated and limited set of tags that an author can use, as opposed to a less-constrained domain such as Twitter.
The site also makes public all post data on a quarterly basis, which can be downloaded and loaded into a relational database for further analysis.

\subsection{Stack Overflow Dataset}

The StackOverflow dataset consists of all questions and answers posted on stackoverflow.com.
The questions are all related to computer programming and can be posted by anyone with an account, which is free to create.
Each question is tagged by the author with (presumably) the tags that are most representative of the post.
Example tags are programming languages (\emph{PHP}, \emph{Common-Lisp}, \emph{MySQL}, \emph{C\#}) and also general topics (\emph{databases}, \emph{optimization}, \emph{arrays}).
An example post is included in Figure \ref{fig:examplePost}.
Note that the author tagged this post with \emph{javascript}, \emph{firefox}, \emph{dom}, and \emph{svg}.

\begin{figure}[ht]
  \centering
  \resizebox{\linewidth}{!}{\includegraphics{visPost-2977094-screen-cropped.pdf}}
  \caption{Example post on stackoverflow.com}
  \label{fig:examplePost}
\end{figure}

The Apr' 11 StackOverflow dataset \cite{DataDump2011} was used for this analysis.
The dataset contains \num{4945168} posts from \num{559803} unique users and \num{26176} unique tags.
The dataset is the entire collection of posts dated between Jul' 2008 (StackOverflow's creation) and Apr' 11.
Post authors used approximately 3 tags on average per post. 

\section{Model}

The tag prediction model is a cognitively-inspired Bayesian probabilistic model based on ACT-R's declarative memory retrieval mechanisms \cite{Anderson2004}.
These equations describe the likelihood that a chunk from memory will be retrieved, given the prior odds of retrieving the chunk and the current context.
In this way the task of retrieving the tag with the highest activation from the words in the post is similar to performing a declarative memory retrieval request for a tag, given the context of title and body words.

This model is also similar in design to the approach used by SNIF-ACT \cite{Fu2007, Pirolli2003}. 
SNIF-ACT can predict link traversal for search queries.
That is, given a search query (goal state), fetched results, and words included in those fetched results, the model predicts the most likely link that a person will click on.
This StackOverflow model is similar because it leverages ACT-R's declarative memory retrieval mechanisms to generate predictions (tag activations instead of link activations).
However, it is slightly different because it takes into account prior odds of tag activations which wasn't necessary in SNIF-ACT.

\subsection{Equations}

A formal description of the model's equations are included in Table \ref{sample-table}. 

\renewcommand{\arraystretch}{1.5}% Wider
\renewcommand{\tabcolsep}{.5mm}
\begin{table}[!ht]
  \begin{center} 
    \caption{The StackOverflow tag prediction model} 
    \label{sample-table} 
    \vskip 0.12in
    \begin{tabular}{ll} 
      \hline
      Common Name &  Equation \\
      \hline
      Activation & 		$A_{i} = B_{i} + \sum_{j\in T}^{ } W_{j} S_{ji} + \sum_{j\in B}^{ } W_{j} S_{ji} - O$ \\
      Base Level & 		$B_{i} = log \frac{p_{i}}{1-p_{i}}$ \\
      Strength Assoc. &		$S_{ji} = log \frac{p(i|j)}{p(i))} = log \frac{NN(j,i)}{N_{Row}(j)N_{Col}(i)}$ \\
      Attention Weight	& 	$W_{j} = WE_{j}$ \\
      Scaled Entropy & 		$E_{j} = 1-\frac{H_{j}}{H_{max}}$ \\
      Entropy & 		$H_{j} = -\sum_{i=1}^{N}p(i|j)log\left (  p(i|j) \right )$ \\
      Offset & 			$O = \frac{1}{5}\sum_{i\in top 5}^{ } A_{i}$ \\
      \hline
    \end{tabular} 
  \end{center} 
\end{table}

Here, the $i$ subscript denotes activation for a particular tag $i$, and the $j$ subscript is for context.
$A_{i}$ is total activation for tag $i$.
Note the similarities between the StackOverflow model and ACT-R's retrieval equations (base level and contextual component, log odds of prior being adjusted by association strength with context).

\subsubsection{Base-Level Activation}

Base-level activation ($B_{i}$) is the log prior odds of observing tag $i$ in the world.
The log prior odds are based on both frequency and recency of use.
However we are are assuming that frequency of tag use does not change greatly between Jul' 08 and Apr' 11 (time period of dataset).
This allows us to compute log odds based on frequency only, as is indicated in Table \ref{sample-table}.
So the tag with the highest frequency (\emph{C\#}) would have the highest base-level activation.
Tag frequency drops off sharply for this dataset.
The top 1\% of tags based on frequency of use account for approximately 60\% of total tag instances.
This is why accounting for tag frequency (by including ACT-R's base-level activation component) is so important for the StackOverflow dataset.
Otherwise without context, the likelihood of choosing \emph{C\#} as a tag would be the same as choosing \emph{ora-00997} (one of the many tags with only a single occurrence of use).
One might argue that with context, it's unlikely that the words associated with \emph{ora-00997} will occur again, so pure context would be able to still often predict \emph{C\#}.
However, the ACT-R theory asserts that pure context is often not enough to overcome high base-level activations (e.g., it can be difficult to break a bad habit).

\subsubsection{Contextual Activation}

Contextual activation can be thought of as the amount of odds adjustment to base levels, given the current context $j$.
Contextual activation has two components: strength of association ($S_{ji}$) and attentional weight ($W_{j}$).
Strength of association measures the relatedness of a contextual element $j$ (e.g., the word PHP in the title of a post) with a tag $i$ (e.g., the tag \emph{PHP5}).

\subsubsection{Attentional Weighting}

Attentional weighting ($W_{j}$), can be thought of as the amount of attentional resources that are dedicated to contextual element $j$.
Total attentional weighting ($W$) is bound, most often set to 1, and reflects the fact that attentional resources are limited.
For the StackOverflow model, the attentional weighting was left unbound and allowed to be calibrated by the logistic regression model.
This was done for two reasons:
First, the we were curious to see what value the logistic regression model would give for $W$.
Second, a non-ACT-R standard measure of attentional weighting was used ($E_{j}$), so it seemed appropriate to allow $W$ to vary alongside exploring this new weighting measure. 

\subsubsection{Entropy}

During initial model testing, the attentional weighting for all words was equal.
That is initially, for a given post with $n$ title words, the attentional weighting $W_{j\in C}$ for all of those words was $W/n$.
The problem with this approach is that familiar stop words like ``the'' and ``?'', that are often observed across all tags have the same attentional weighting as highly predictive words like ``PHP''.
Granted the $S_{ji}$'s across all tags for context words like ``the'' are much lower (and more evenly distributed) than the few $S_{ji}$'s that spike for ``PHP'' (e.g., the tag \emph{PHP}).
So the amount of adjusted log odds from stop words is low even with equal attentional weighting (since their $S_{ji}$'s are low).
However, this still seemed problematic for two reasons:
First, because stop words like ``the'' are observed with all tags, they are not highly predictive of any tag.
So one could argue that their presence is simply adding error in the model's tag activation calculation.
Second, because $W$ is held constant across posts, any attentional resources that are allocated to stop words like ``the'' and ``?'' are taken away from being used for highly predictive words like ``PHP''.

It's also the case that there are many commonalities between ACT-R's declarative memory retrieval equations and other non-ACT-R information retrieval theories,
and many of those theories use a combination of weighting schemes for contextual relevance and global importance of a particular chunk in context.
For example, \citeA{Dumais1991} shows that a measure of how likely term $j$ is in document $i$ is often the product of a local weighting $L_{ji}$ and global weighting $G_{j}$ measure.
This idea maps rather nicely to ACT-R's contextual weighting.
That is, terms are context words $j$, documents are tags $i$, local weighting is strength of association $S_{ji}$, and global weighting is attentional weighting $W_{j}$.

So the scaled entropy measure $E_{j}$ was chosen as a global weighting measure for this dataset because it has a strong theoretical foundation and it predicts well \cite{Dumais1991}.
It is based on Shannon's Information Theory, which measures the amount of information content (predictiveness) of a word.
The scaled entropy ($E_{j}$) of a context word can be thought of as the amount of predictiveness that that word has to any tag.

For example, a stop word like ``the'' will have an even distribution of co-occurrences across all tags.
This means that the marginal probabilities of ``the'' with all tags will be roughly equal.
It can be shown that maximum entropy $H_{j}$ is reached when all marginal probabilities are equal.
This makes intuitive sense because if all marginal probabilities are equal, there is no information gained in observing the context word.
The scaled entropy measure $E_{j}$ transforms the entropy measure to a scale that ranges from 0 to 1, where maximum entropy means lowest scaled entropy.
This means that a stop word like ``the'' will have a scaled entropy measure close to 0, while a highly-predictive word like \emph{macro} will have a scaled entropy measure close to 1.
Going back to ACT-R's attentional weighting $W_{j}$, if $E_{j}$ is low then $W_{j}$ is low, meaning that the scaled entropy measure helps allocate the limited attentional resources to predictive contextual elements.

\subsubsection{Offset}

An offset measurement (mean of the top five tags for the post) was used to equalize the top activated tags for each post.
This was done primarily to improve model fit.
To see why this is the case, suppose the following scenario:
The offset measurement is not used, and the model is asked to provide ten tags across ten posts.
The top ten tags for one of these posts have activations that are higher than activations for all other tags in the other nine posts.
The model would then select the ten tags from the single post as the most likely tags.
But this doesn't make intuitive sense, because there is a limit to the number of tags that people tend to use for each post.
So the offset measure aims to place all of these ten posts on the same level, so that asking the model to choose ten tags from ten posts will select roughly a single tag from each post.

\section{Method}

\subsubsection{Summary}

A variation of ACT-R's declarative memory retrieval theory \cite{Anderson2004} was used to model tag retrievals for StackOverflow posts.
The hypothesis is that the tags used for posts are based on a tag's history of prior use and the strength of association between the tag and the content of words in the title and body of the post.
The strength of association between post words and tags was calculated using 2/3 of the dataset (one million posts).
A logistic regression statistical technique was used to calibrate model parameters and optimize performance.
The weights for each model component were calibrated using a sample of \num{1000} posts not contained within the strength of association dataset.
The calibrated model was then tested on a new sample of \num{1000} posts not contained within either previous training dataset.
Classification accuracy was used as a metric of model performance (i.e., how many of the model's predicted tags are correct).

FIXME. The Apr' 11 StackOverflow dataset \cite{DataDump2011} was downloaded and imported into the MySQL relational database system.

\subsection{Generating Model Predictions}

For a given title and body of a post, the model returns the activations for each possible tag.
The tag with the highest activation is the most likely tag to be associated with that post.
Tag activation is a function of four components:
The tag's base level activation (prior odds of being used),
contextual activation for words in the title (word co-occurrence weighting),
contextual activation for words in the body,
and an offset to equalize activation values for tags across all posts.

The R statistical programming environment was used to build the model and generate model predictions.
One million posts (approximately 2/3 of all posts) were loaded into R to build the co-occurrences and tag occurrences.
These were represented as a sparse matrix of co-occurrences and a sparse vector of tag occurrences.
In order to generate a model prediction, tag activations for title and body context were computed separately, and then added to each tag's prior base-level activation.
An offset constant (equal to the average activation of the top 5 model-predicted tags for the post) was then subtracted from the prior value.
The resulting tags were rank sorted, and the one with the highest activation is the model's prediction of the most likely tag associated with that post.

\subsection{Measuring Co-occurrences}

Model tag predictions are based on the amount of word co-occurrences between words in the title and body of the post and tags used for that post.
To build the co-occurrence matrix between post words and tags, all title and body text and associated tags were extracted from the relational database.
The Python Natural Language Processing Toolkit \cite{Bird2009} was used to chunk (i.e., tokenize and lemmatize) the text in the post.
The tags were already chunked, but tag synonyms were converted to their root tag using the community-maintained StackOverflow tag synonym database.
Afterwords the processed word chunks and root tags were imported into a PostgreSQL database table.

A query was then used to build the post word, tag word co-occurrence matrix.
There are \num{39223968} unique post word, tag word combinations.
The result is a table of co-occurence counts for each of the combinations.

\subsection{Measuring Tag Occurrences}

Model tag predictions are also based on the frequency that each tag has been used.
More often used tags more likely to have higher activation.
A SQL query was used to build a similar set of tag use counts for the \num{26176} unique tags.
This set for example would show that \emph{C\#} is the most often used tag, where about 11\% of posts were tagged with \emph{C\#}.

\subsection{Measuring Model Fit}

For a given post, the model produces a vector of tag activations.
For that post, the author's chosen tags are known (see Figure \ref{fig:examplePost}).
So model performance can be measured by comparing the model's tag activations with chosen tags for a post, and then aggregated across posts to get an overall average performance.
If the model consistently produces higher tag activations for chosen tags than non-chosen tags, the model should be able to accurately predict the chosen tags.

A logistic regression technique was used in order to operationalize this fit measure.
For each post, three sets of tag groups were combined to most accurately sample tag activations while managing computer resource constraints:
The model's tag activations for the highest 200 tags, a weighted sample of 800 tags (weighted towards higher activation), and the tag activations for all tags that were chosen by the author of the post.
Each of these observations was assigned a category of 0 or 1 depending on if the tag was a chosen tag or not for that post (0 is not chosen).
Conceptually, if the model has good categorization power, the observations assigned 1 should have higher tag activations than the observations assigned 0.

These vectors of tag activation, categorization were collected across a set of \num{1000} fresh posts that were not used to train the model on word association strength.
So a form of cross validation was used to measure model fit, although the more robust full k-fold cross validation technique was not used (due mostly to resource and time limitations).

The base R glm function (with ``binomial'' argument) was used to perform the logistic regression.
The parameters were optimized to best predict categorization (chosen tag, not chosen tag) from the four predictor variables:
The tag's prior occurrence frequency, word co-occurrence activation for title and body words, and activation offset for the post.
Further discussion and operationalization of the predictor variables is included in the Model section.

\section{Results}

\subsection{Logistic Regression}

In order to formally test model performance and to calibrate model parameters, a calibration run using \num{1000} posts was performed.
For the calibration run, the category prediction (chosen or not chosen tag for a post) was regressed on base-level activation, spreading activation for title and body, and post offset.
However using a single run of the \num{1000} posts was insufficient because there are two model components that require prior estimates of coefficients:
The offset measure to determine the top-five tags and associated activation and the sampling technique used to pick a representative set of tags for each post.
So model calibration was bootstrapped, where the coefficients from the previous run were used as estimates of coefficients for the next run.
The coefficients converged on best-fit values quickly (only required two sets of four iterations), and did not show any signs of instability.

The final best-fit coefficients for each model component and formal statistical measurements are included in Tables \ref{tab:coeffs} and \ref{tab:fits} respectively.

\renewcommand{\arraystretch}{1}% Wider
\renewcommand{\tabcolsep}{1mm}
\begin{table}[!ht]
  \begin{center} 
    \caption{Best-fit coefficient estimates from logistic regression} 
    \label{tab:coeffs} 
    \vskip 0.12in
    \begin{tabular}{lllll} 
      \hline
      Coefficient & 	Estimate &	Std. Err. &	z value &	$Pr(>|z|)$  \\
      \hline
      (Intercept) &	-1.35 &		0.10 &		-13.59 &	\textless2e-16 *** \\
      prior &		1.08 & 		0.01 &		88.12 & 	\textless2e-16 *** \\
      sjiTitle &	1.41 &		0.02 &		57.36 &		\textless2e-16 *** \\
      sjiBody &		2.36 &		0.04 &		58.48 &		\textless2e-16 *** \\
      offset &		-0.71 &		0.02 &		-39.05 &	\textless2e-16 *** \\
      \hline
    \end{tabular} 
  \end{center} 
\end{table}

\vspace{-2em}

\renewcommand{\arraystretch}{1}% Wider
\renewcommand{\tabcolsep}{3mm}
\begin{table}[!ht]
  \begin{center} 
    \caption{Model fit metrics} 
    \label{tab:fits} 
    \vskip 0.12in
    \begin{tabular}{ll} 
      \hline
      Common Name &  Value	\\
      \hline
      McFadden's Pseudo $R_{}^{2}$ &	.46 \\
      True Positives &			647 \\
      False Positives &			278 \\
      True Negatives &			\num{796982} \\
      False Negatives &			\num{2235} \\
      Positive Predictive Value &	0.70 \\
      Negative Predictive Value &	0.997 \\
      \hline
    \end{tabular} 
  \end{center} 
\end{table}

Note that all four coefficients for the run are strong predictors of tag use ($p<.01$), and model fit is satisfactory (McFadden's pseudo $R_{}^{2}=.46$, npv = .997, ppv = .70).
It's also interesting to note that although the attentional weighting terms ($W_{title}, W_{body}$) were calibrated by the regression, they did not stray far from cognitively-plausible defaults
(see coefficients for title and body in Table \ref{tab:coeffs}).

\subsection{Example Posts}

A visualization of the model's tag activation for the example post in Figure \ref{fig:examplePost} is included in Figure \ref{fig:modelPost}.

\begin{figure}[ht]
  \centering
  \resizebox{\linewidth}{!}{\includegraphics{visPost-2977094-act-cropped.pdf}}
  \caption{
    Model's prediction of top-ten tags for post in Figure \ref{fig:examplePost}.
    The title is the title text of the post.
    The attentional weight (scaled entropy) for each word in the title is included below each word.
    The user-chosen tags are included below the attentional weights.
    The stacked bar graphs are the total activation for the top-ten predicted model tags for the post.
    Each bar graph is partitioned into the three model components that vary across the tags in a single post (not the offset).
}
  \label{fig:modelPost}
\end{figure}

Note how the base level activation and contextual activation combine to predict tags that are both frequently used and contextually relevant.
For example, note that the \emph{javascript} tag leverages a combination of contextual activation and prior activation to achieve a high total activation.
Alternatively, the \emph{svg} tag relies almost entirely on context to achieve high activation.
And note that the model is by no means perfect.
For example, \emph{html} is not tagged by the author, but the model ranks it as the second highest tag.
And the author's tag \emph{firefox} is only ranked 8th by the model.

\subsection{Generalizability}

In order to test how well the model (and calibrated parameters) generalize to the rest of the StackOverflow dataset, the model was tested on an additional set of \num{1000} posts.
These were fresh posts, and not previously used to either train the strength of association matrix or calibrate model parameters.
The model parameters from the calibrated dataset were held constant and used on the test dataset.
Model classification accuracy as a function of number of tags predicted by the model was used as the metric to test if the model generalized.

\subsection{Classification Accuracy}

A plot of model performance for both the calibration and test dataset is included in Figure \ref{fig:ROC}.

\begin{figure}[ht]
  \centering
  \resizebox{\linewidth}{!}{\includegraphics{ROC-cropped.pdf}}
  \caption{
    Model performance across the calibration and test dataset, as well as when specific model components are removed.
    The y-axis represents model performance (one is perfect), and the x-axis is the proportion of tags that the model generates compared to the number of tags used by the authors of the posts (one is equal number).
    The vertical dotted line represents the point where the model is asked to tag on average one tag per post.
  }
  \label{fig:ROC}
\end{figure}

Note how model performance is almost identical for the calibration and test dataset (compare full model calibration to full model test).
This indicates that the model parameters were not overfit, and generalize well to the rest of the dataset.
It also provides evidence that \num{1000} posts is an adequate number to estimate model parameters accurately.

\subsection{Necessity Analysis}

Figure \ref{fig:ROC} also shows model performance after specific model components are removed.
Model parameters were recalibrated for each model, except for the full model test dataset that used the parameters from the calibration dataset.
These plots provide useful information regarding the relative necessity of each component in the full model.
That is, which model components contribute most strongly to increased performance, and which model components might be redundant (not necessary).
Note that it is certainly the case that each model component contributes positive and unique variance to model performance.
However, removing only one component (particularly the words in the body of the post) does not drastically reduce model performance.
It's only when groups of components are removed that model performance starts to drastically suffer.

\section{Discussion}

\subsection{Related Work}
Predicting tags for the StackOverflow dataset has been worked on by at least one prior researcher \cite{Kuo2011}.
\citeA{Kuo2011} only looks at the Stack Overflow dataset briefly at the end of the technical report.
However he does mention that the co-occurrence model he used has about a 47\% classification accuracy for predicting StackOverflow tags when required to predict one tag per post.
The full model used here achieves 67\% classification accuracy when predicting one tag per post on average
(height of full model at vertical dotted line in Figure \ref{fig:ROC}).
The increase in model performance here is most likely due to the fact that \citeA{Kuo2011} only used \num{10000} posts to generate the co-occurrence matrix ($S_{ji}$).
One million posts (two orders of magnitude higher) were used here.
Also the model here has some additional components: The offset in particular and possibly the body term as well,
as it was unclear from \citeA{Kuo2011} if the author used context from both the body and title of a post or just the title. 
Nonetheless, there are certainly similarities in the approach:
Co-occurrence count was used as the core measure of similarity, and both local (strength of association) and global (idf or entropy) measures were used to compute tag likelihood, given context.

\subsection{ACT-R and Scale}
\num{39223968} unique context word, tag combinations were contained within the one million posts that were used to build the strength of association matrix.
Since the model is based on ACT-R's declarative memory retrieval equations, it could have been implemented in Common Lisp (programming language for ACT-R's implementation).
However, it is known that the current implementation of spreading activation (required for $S_{ji}$) in ACT-R does not scale to an $S_{ji}$ corpus over 100,000 \cite{Douglass2010, Douglass2009}.
So it was decided to use a one-off R implementation of ACT-R's retrieval equations for this analysis.
This is much like how \citeA{Douglass2010} implemented the equations in Erlang when using the theory to test the fan effect on large-scale declarative memory datasets.

However, it is increasingly likely that there is no fundamental reason why vanilla ACT-R (Common Lisp version) cannot be optimized to handle spreading activation calculations with datasets this large.
All that is really needed is a more appropriate data structure to handle the large scale.
This can be seen by comparing the implementation of spreading activation here to the implementation in \citeA{Douglass2010}.
Both implementations handle sji counts greater than one million, with declarative memory retrieval times (time to calculate $A_{i}$'s) around one second at this scale.
However, the implementations could not be more different.
The Erlang implementation leverages a semantic network of millions of isolated process chunks and massive concurrency to achieve the scale.
The R implementation here is single threaded and requires only a sparse matrix implementation to achieve the scale.
Since the programming languages are different, it's not likely the case that only a specific programming language can achieve the scale.
And since the parallelism is different (single threaded vs multi threaded), it's not likely the case that achieving the scale can only be done with massively concurrent implementations.
So, it's most likely the case that using an appropriate data structure is all that is needed for ACT-R Common Lisp to handle sji counts in the millions and above.

\subsection{Entropy and Stop Words}

The Python NLTK documentation \cite{Bird2009} recommended to remove stop words from the analysis.
Stop words are low-predictor words, and the documentation listed out a short list that were commonly removed (e.g., ``the'', ``and'', ``or'')
However it seemed somewhat arbitrary to explicitly list out a few stop words and then simply choose those as the words to remove.
Looking at the scaled entropy measure, it seems quite plausible that this measure of word predictiveness can also be used to identify stop words.
That is, words with scaled entropy close to zero are the stop words.
Looking at the lowest ten scaled-entropy words for this dataset, (``does'', ``anyone'', ``'ve'', ``found'', ``are'', ``seems'', ``has'', ``been'', ``for'', ``support''),
using scaled entropy to determine stop words seems to work fairly well.

\subsection{Entropy and ACT-R's Attentional Weighting}

Model performance improved by using a scaled entropy measure to weight the importance of each contextual element in the body and title of a post.
However, it's uncommon for an ACT-R model that uses the declarative memory retrieval mechanisms to use anything other than equal attentional weighting for each chunk in context.
But it is much more common outside of the ACT-R modeling community to use both a local (strength of association) and global (attentional weight) term to predict document retrieval \citeA{Dumais1991}.
In fact, it's been shown that the Pointwise Mutual Information Index (PMI) (a commonly-used measure to predict document retrieval)
is mathematically equivalent to ACT-R's retrieval equations when the sample size of the corpus is large \cite{Budiu2007, Farahat2004},
and both global and local weightings are commonly used when the PMI measurement is used.
This suggests that using a global weighting term for chunks in context (e.g., the scaled entropy measure used here) alongside the more commonly-used local weighting (spreading activation) might be useful for other ACT-R models that are taking advantage of large declarative memory datasets to build up the model's base knowledge.

\section{Acknowledgments}

I would like to thank Dr. Mike Byrne and Dr. Fred Oswald for their excellent guidance and recommendations throughout the model development and data analysis process.

\bibliographystyle{apacite}
\setlength{\bibleftmargin}{.125in}
\setlength{\bibindent}{-\bibleftmargin}
\bibliography{bibliography}

\end{document}
