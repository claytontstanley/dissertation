% 
% Annual Cognitive Science Conference
% Sample LaTeX Paper -- Proceedings Format
% 

% Original : Ashwin Ram (ashwin@cc.gatech.edu)       04/01/1994
% Modified : Johanna Moore (jmoore@cs.pitt.edu)      03/17/1995
% Modified : David Noelle (noelle@ucsd.edu)          03/15/1996
% Modified : Pat Langley (langley@cs.stanford.edu)   01/26/1997
% Latex2e corrections by Ramin Charles Nakisa        01/28/1997 
% Modified : Tina Eliassi-Rad (eliassi@cs.wisc.edu)  01/31/1998
% Modified : Trisha Yannuzzi (trisha@ircs.upenn.edu) 12/28/1999 (in process)
% Modified : Mary Ellen Foster (M.E.Foster@ed.ac.uk) 12/11/2000
% Modified : Ken Forbus                              01/23/2004
% Modified : Eli M. Silk (esilk@pitt.edu)            05/24/2005
% Modified: Niels Taatgen (taatgen@cmu.edu) 10/24/2006

%% Change ``a4paper'' in the following line to ``letterpaper'' if you are
%% producing a letter-format document.

\documentclass[10pt,letterpaper]{article}

\usepackage{cogsci}
\usepackage{pslatex}
\usepackage{apacite}
\usepackage[group-separator={,}]{siunitx}

\graphicspath{{./pictures/}} % Specifies the directory where pictures are stored

\title{FIXME}
 
\author{{\large \bf Clayton Stanley (clayton.stanley@rice.edu)} \\
  Department of Psychology, 6100 Main Street \\
  Houston, TX 77005 USA 
  \AND {\large \bf Mike Byrne (byrne@rice.edu)} \\
  Department of Psychology and Computer Science, 6100 Main Street \\
  Houston, TX 77005 USA }

\begin{document}

\maketitle

\begin{abstract}
The abstract should be one paragraph, indented 1/8~inch on both sides,
in 9~point font with single spacing. The heading {\bf Abstract} should
be 10~point, bold, centered, with one line space below it. This
one-paragraph abstract section is required only for standard spoken
papers and standard posters (i.e., those presentations that will be
represented by six page papers in the Proceedings).

\textbf{Keywords:} 
Machine Learning; ACT-R; Large-Scale Declarative Memory
\end{abstract}

\section{Introduction}

Human-based tagging of online content has increased in recent years (\cite{Chang2010}).
A large portion of the growing number of social media sites support some sort of human-directed tagging of posts.
Additionally, an increasing number of individuals are using tags as a form of query-based search to find information
(e.g., \cite{Diakopoulos2010}).

Although much good work has been done to characterize the lifetime and growth characteristics of hashtags for real-time social networking sites such as Twitter
(e.g., \cite{Bauer2012}, \cite{Tsur2012}, \cite{Chang2010})
less is known about a person's motivation for choosing particular tags during the creation of the tweet, post, etc.
Modeling tag creation accurately (as opposed to modeing tag growth and behavior after creation) could allow a system to predict the hashtag that a user should use, given the contextual clues available.
The benefits of this type of prediction are numerous.
For example, users could be introduced to tags (and potentially comunities surrounding those tags) that are of interest to them.
Also tag cleanup systems could be implemented where mistags are identified and corrected.

A tag-prediction system was built here for the StackOverflow dataset that can solve precisely these sorts of problems.
The StackOverflow dataset was chosen because it is relatively constrained, contains a large number of posts and associated tags, and the data are easy to obtain.
It is constrained because there is a fully enumerated and limited set of tags that a poster can use, as opposed to a less-constrained domain such as Twitter.
The site also makes public all posting data on a quarterly basis, which can be easily downloaded and loaded into a relational database for further analysis.

\section{Method}

A variation of ACT-R's declarative memory retrieval theory was used to model tag retrievals for StackOverflow posts.
The hypothesis is that the tags used for posts are based on a tag's prior use and the strength of association between the tag and the content of words in the title and body of the post.
The strength of association between post words and tags was calculated using 2/3 of the dataset (1M posts).
A logistic regression statistical technique was used to calibrate model parameters and optimize performance.
The weights for each model component were calibrated using a sample of 1,000 posts not contained within the strength of association dataset.
Finally, the calibrated model was tested on a new sample of 1,000 posts not contained within either previous training dataset.
Classification accuracy was used as a metric of model performance (i.e., how many of the model's predicted tags are correct).

\subsection{Stack Overflow Dataset}

The StackOverflow dataset consists of all questions and answers posted on stackoverflow.com.
The questions are all programming related and can be posted by anyone with an account, which is free to create.
Each question is tagged by the poster with (presumably) the tags that are most representative of the post.
Example tags are programming languages (PHP, Common-Lisp, MySQL, C\#) and also general topics (databases, optimization, arrays).
An Example question from the stackoverflow.com site is included in Figure \ref{fig:examplePost}.
Note that the author tagged this post with java, twitter, twitter4j, and twitterâˆ’oauth.

\begin{figure}[ht]
  \centering
  \resizebox{\linewidth}{!}{\includegraphics{visPost-4148010-act.pdf}}
  \caption{Example Post}
  \label{fig:examplePost}
\end{figure}


The Sep' 11 dataset (\cite{DataDump2011}) was downloaded and imported into the MySQL relational database system.
The dataset contains \num{4945168} posts from \num{559803} unique users.

\subsection{Measuring Co-occurrences}

Model tag predictions are based on the amount of word co-occurrences between words in the title and tags used for that post.
To build the co-occurrence matrix between post words and tags, all title and body text and associated tags were extracted from the relational database.
The Python Natural Language Processing Toolkit (\cite{Bird2009}) was used to chunk (i.e., tokenize and lemmetize) the text in the post.
The tags were already chunked so those words were not processed any further.
Afterwords the processed word chunks were fed back in to the database.

A query was then used to build the post word, tag word co-occurrence matrix.
For a given word found in the body or title of a post, the query counts the number of times that that word appears with each tag.
Each of those counts is recorded as the word co-occurrence for that word and each associated tag.
This process is done across all words found in any post.
The result is a comma-separated value (csv) file of post-word, tag, count for each combination.

\subsection{Measuring Tag Occurrences}

Model tag predictions are also based on the frequency that each tag has been used.
More often used tags more likely to have higher activation.
A SQL query was used to build a similar csv for tag occurrences.
This csv for example would show that C\# is the most often used tag, where about 11\% of posts were tagged with C\#.

\subsection{Generating Model Predictions}

For a given title in a post, the model returns the activations for each possible tag.
The tag with the highest activation is the most likely tag to be associated with that post.
Tag activation is a function of four components:
The tag's base level activation (prior odds of being used),
contextual activation for words in the title (word co-occurrence weighting),
contextual activation for words in the body,
and an offset to equalize activation values for tags across all posts.

Contextual activation is computed by comparing the words in the post with each tag's word co-occurrence vector.
If a tag's co-occurrences correlate strongly with the words in the title, then that tag's activation will be high.
If there is a low correlation between the observed co-occurrences for a tag and the observed words in the title, than that tag's activation will be low.
The equations that operationalize this process will be included in the Model section.

The R statistical programming environment was used to build the model and generate model predictions.
This was done by feeding the word co-occurrence and tag occurrence csv's into a separate sparse matrix and sparse vector in R.
A hash was computed for each unique word, so that indices for the sparse matrix and vector could be numeric.
The dimensions for the co-occurrence sparse matrix were context-word, tag, and the tag occurrence vector was 1 dimensional (tag).

1 million posts (approximately 2/3 of all posts) were fed into R to build the co-occurrences and tag occurrences.
An interface to the model was developed so that one could supply a vector of words, and the model would return a sparse vector of tag activations given that context.
In order to generate a model prediction, tag activations for title and body context were computed separately, and then added to each tag's prior base-level activation.
An offset constant (equal to the average activation of the top 5 model-predicted tags for the post) was then subtracted from the prior value.
The resulting tags were rank sorted, and the one with the highest activation would then be the model's prediction of the most likely tag associated with that post.

\subsection{Measuring Model Fit}

For a given post, the model produces a vector of tag activations.
For that post, the poster's chosen tags are known (see Figure \ref{fig:examplePost}).
So model performance can be measured by comparing the model's tag activations with chosen tags for a post, and then aggregated across posts to get an overall average performance.
If the model consistently produces higher tag activations for chosen tags than non-chosen tags, the model should be able to accurately predict the chosen tags.

A logistic regression technique was used in order to operationalize this fit measure.
For each post, the model's tag activations for the highest 100 tags were recorded, as well as the tag activations for all tags that were chosen by the poster.
Each of these observations was assigned a category of 0 or 1 depending on if the tag was a chosen tag or not for that post (0 is not chosen).
Conceptually, if the model has good categorization power, the observations assigned 1 should have higher tag activations than the observations assigned 0.

These vectors of tag activation, categorization were collected across a set of 1,000 posts.
The set of posts did not include any posts that were used to train the model on word association strength.
So a form of cross validation was used to measure model fit, although the more robust full k-fold cross validation technique was not used (due mostly to resource and time limitations).

The base R glm function (with 'binomial' argument) was used to perform the logistic regression.
The parameters were optimized to best predict categorization (chosen tag, not chosen tag) from the four predictor variables:
prior tag occurance, word co-occurrence activation for title and body words, and activation offset for the post.
Further discussion and operationalization of the predictor variables is included in the Model section.

\section{Model}

The tag-prediction model is a cognitively-inspired Bayesian probabilistic model, based on ACT-R's declarative memory retrieval mechanisms (\cite{Anderson2004}).
These equations describe the likelihood that a chunk from memory will be retrieved, given the prior odds of retrieving the chunk and the current context.
In this way the task of retrieving the tag with the highest activation from the words in the title is similar to performing a declarative memory retrieval request for a tag, given the context of title and body words.

This model is also similar in design to the approach used by SNIF-ACT (\cite{Fu2007}, \cite{Pirolli2003}). 
SNIF-ACT can predict link traversal for search queries.
That is, given a search query (goal state), fetched results, and words included in those fetched results, the model predicts the most likely link that a person will click on.
This StackOverflow model is similar because it leverages ACT-R's declarative memory retrieval mechanisms to generate predictions (tag activations instead of link activations).
However, it is slightly different because it takes into account prior odds of tag activations (set to zero in SNIF-ACT).
This was set to zero in SNIF-ACT because each link has multiple words, so looking at word co-occurrences between the search query and link words produces a stable predictor.
It's also the case that search engines already rank order search results.
So even if prior odds for link activations were used in the SNIF-ACT model, the prior odds for the top few links would be virtually identical, so model predictions would not change much.
For the StackOverflow dataset, each tag is a single word, and those tags are not already rank ordered.
So ACT-R's prior odds component of the declarative memory retrieval equations was included for the StackOverflow model.

\subsection{Equations}

A formal description of the model's equations are included below:

\begin{table}[!ht]
  \begin{center} 
    \caption{Sample table title.} 
    \label{sample-table} 
    \vskip 0.12in
    \begin{tabular}{ll} 
      \hline
      Common Name &  Equation \\
      \hline
      Activation & 		$A_{i} = B_{i} + \sum_{j\in T}^{ } W_{j} S_{ji} + \sum_{j\in B}^{ } W_{j} S_{ji} + O$ \\
      Base Level & 		$B_{i} = log \frac{p_{i}}{1-p_{i}}$ \\
      Offset & 			$O = \frac{1}{5}\sum_{i\in top 5}^{ } A_{i}$ \\
      Strength Assoc. &		$S_{ji} = log \frac{p(i|j)}{p(i))} = log \frac{p(j,i)}{p(j)p(i)}$ \\
      & 			$= log \frac{NN(j,i)}{N_{Row}(j)N_{Col}(i)}$ \\
      Attentional Weight	& $W_{j} = WE_{j}$ \\
      Chunk Entropy & 		$E_{j} = 1-\frac{H_{j}}{H_{max}}$ \\
      Entropy & 		$H_{j} = -\sum_{i=1}^{N}p(i|j)log\left (  p(i|j) \right )$ \\
      Max Entropy & 		$H_{max} = maxH$ \\
      \hline
    \end{tabular} 
  \end{center} 
\end{table}

Here, the $_{i}$ subscript denotes activation for a particular tag i, and the $_{j}$ subscript is for context.
$A_{i}$ is total activation for tag i.
Note the similarities between the StackOverflow model and ACT-R's retrieval equations (base level and contextual component, log odds of prior being adjusted by association strength with context).

\subsubsection{Base-Level Activation}

Base-level activation ($B_{i}$) is the log prior odds of observing tag i in the world.
So the tag with the highest frequency (C\#) would have the highest base-level activation.
Tag frequency drops off sharply for this dataset.
The top 1\% of tags based on frequency of use account for approximately 60\% of total tag instances.
This is why accounting for tag frequency (by including ACT-R's base-level activation component) is so important for the StackOverflow dataset.
Otherwise without context, the likelihood of choosing C\# as a tag would be the same as choosing ora-00997 (one of the many tags with only a single occurrence of use).
One might argue that with context, it's unlikely that the words associated with ora-00997 will occur again, so pure context would be able to still often predict C\#.
However, the ACT-R theory asserts that pure context is often not enough to overcome high base-level activations (e.g., it can be difficult to break a bad habit).

\subsubsection{Contextual Activation}

Contextual activation ($S_{ji}$) can be thought of as the amount of odds adjustment to base levels, given the current context $_{j}$.
For example, if the words in the title contained only 'PHP', 'PHP5', and 'PHPUnit', then the number of $_{j}$'s is three.
And for the 'PHP' tag, those three words are often in the title of PHP-tagged posts.
So the sum of the three $S_{ji}$'s for the tag PHP will be high, meaning that the odds adjustment for PHP given the current context will be high.
Since PHP already has a high base-level activation (frequently used tag), the model should produce a high total activation for the tag PHP, given the context.

\subsubsection{Attentional Weighting}

Attentional weighting ($W_{j}$) can be thought of as the amount of attentional resources that are dedicated to contextual element $_{j}$.
Total attentional weighting ($W$) is bound, most often set to 1, and reflects the fact that our attentional resources are limited.
For the StackOverflow model, the attentional weighting was left unbound, and allowed to be calibrated by the logistic regression model.
This was done for two main reasons.
First, the researcher was curious to see what value the logistic regression model would give for $W$.
Second, a non-ACT-R standard measure of attentional weighting was used ($E_{j}$), so it seemed appropriate to allow $W$ to vary alongside exploring this new weighting measure. 

\subsection{Entropy}

During initial model testing, the attentional weighting for all words was equal.
That is initially, for a given post with $n$ title words, the attentional weighting $W_{j\in C}$ for all of those words was $W/n$, where the logistic regression model fit $W$ to range between 1-3.
The problem with this approach was that familiar stop words like 'the' and '?', that were observed so often and for all tags, had the same attentional weighting as highly predictive words like 'PHP'.
Granted the $S_{ji}$'s across all tags for context words like 'the' are much lower (and more evenly distributed) than the few $S_{ji}$'s that spike for 'PHP' (e.g., the tag PHP).
So the amount of adjusted log odds from stop words was low, even with equal attentional weighting (since their $S_{ji}$'s were low).
However, it still seemed problematic for two reasons.
First, intuitively it seemed that regardless of word co-occurrence association strengths, because stop words like 'the' are observed with all tags, they are not highly predictive of any tag.
So one could argue that their presence is simply adding error in the model's tag activation calculation.
Second, because $W$ is held constant across posts, any attentional resources that are allocated to stop words like 'the' and '?' are taken away from being used for highly predictive words like 'PHP'.

It turns out that there are many connections between ACT-R's declarative memory retrieval equations and other non-ACT-R information retrieval theories,
and many of those theories use a combination of weighting schemes for contextual relevance and global importance of a particular chunk in context.
For example, \cite{Dumais1991} shows that a measure of how likely term ${_j}$ is in document ${_i}$ is often the product of a local weighting $L_{ji}$ and global weighting $G_{j}$ measure.
This idea maps rather nicely to ACT-R's contextual weighting.
That is, terms are context words ${_j}$, documents are tags ${_i}$, local weighting is strength of association $S_{ji}$, and global weighting is attentional weighting $W_{j}$.

\cite{Dumais1991} states that predictions tend to improve when a global weighting scheme is used, so that words that occur frequently or with many documents (tags) are not weighted as high.
He reviews four different commonly-used weighting schemes; one of which is a scaled entropy measure.

The scaled entropy measure $E_{j}$ was chosen as a global weighting measure for this dataset because it has a strong theoretical foundation and it predicts well (e.g., \cite{Dumais1991}).
It is based on Shannon's Information Theory, which measures the amount of information content (predictiveness) of a word.
Entropy measures that are mathematically equivalent to Shannon's formulation are used in data compression applications, cryptography, as well as thermodynamic systems.
The scaled entropy ($E_{j}$) of a context word can be thought of as the amount of predictiveness that that word has to any tag.

For example, a stop word like 'the' will have an even distribution of co-occurrences across all tags.
That is, seeing the word 'the' in context is not highly predictive of any particular tag.
This means that the marginal probabilities of 'the' with all tags will be roughly equal.
It can be shown that maximum entropy $H_{j}$ is reached when all marginal probabilities are equal.
This makes intuitive sense because if all marginal probabilities are equal, there is no information gained in observing the context word.
The scaled entropy measure $E_{j}$ transforms the entropy measure to a scale that ranges from 0, 1, where maximum entropy means lowest scaled entropy.
This means that a stop word like 'the' will have a scaled entropy measure close to 0, while a highly-predictive word like 'Macro' will have a scaled entropy measure close to 1.
Going back to ACT-R's attentional weighting $W_{j}$, if $E_{j}$ is low then $W_{j}$ is low, meaning that the scaled entropy measure helps allocate the limited attentional resources to predictive contextual elements.

\subsection{PMI}

ACT-R's declarative memory retrieval equations appear in other information retrieval literature that use word co-occurrence counts, although it's usually called the Pointwise Mutual Information Index.
For example, \cite{Fu2007} and \cite{Farahat2004} have shown that ACT-R's retrieval equations are mathematically equivalent to the Pointwise Mutual Information (PMI) index when the sample size of the corpus is large.
\cite{Budiu2007} and \cite{Farahat2004} have shown that the PMI index for word co-occurrences is a strong predictor of word similarity and information scent.
And on large corpora \cite{Budiu2007} has shown that PMI is the best predictor of word pair similarity (compared to LSA and generalized LSA).
It is interesting to note that ACT-R's declarative memory retrieval theory not only fits human performance data well, but is also a useful and competitive best predictor of word pair similarity in the information retrieval literature.

\subsection{Implementation}

\subsubsection{NLP}

Python's Natural Language Processing toolkit was chosen for word tokenizing and lemmatization because it seemed to be the most widely used tool for this type of natural language processing.
The tool wasn't tweaked from base settings or customized for the particular domain.

\subsubsection{Spreading Activation}

Since the model is based on ACT-R's declarative memory retrieval equations, it could have been implemented in Common Lisp (programming language for ACT-R's implementation).
However, it is known that the current implementation of spreading activation (required for $S_{ji}$) in ACT-R does not scale to an $S_{ji}$ corpus over 100,000 (\cite{Douglass1998}, \cite{Douglass2007}).
So it was decided to use a one-off implementation of ACT-R's retrieval equations for this analysis.
This is much like how \cite{Douglass2007} implemented the equations in Erlang when using the theory to test the fan effect on large-scale declarative memory datasets.

R was chosen as the implementation language for two main reasons.
First, it has sparse matrix support and vectorization routines, so that the model could scale to millions of $S_{jis}$'s.
Second, it has excellent data visualization and analysis capabilities. 
This way statistical data analysis and visualization could be done within the same R process that was running the model. 

\section{Results}

After training the model's spreading activation matrix and calibrating model parameters, the model can predict roughly 70\% of tags correctly when asked to generate one tag per post on average.

\subsection{Example Posts}

A visualization of the model's tag activation for the example post in Figure \ref{fig:examplePost} is included in Figure \ref{fig:modelPost}.

\begin{figure}[ht]
  \centering
  \resizebox{\linewidth}{!}{\includegraphics{visPost-4148010-act.pdf}}
  \caption{
    Model's prediction of top-ten tags for post in Figure \ref{fig:examplePost}.
    The title is the title text of the post.
    The attentional weight (entropy) for each word in the title is included below each word.
    The user-chosen tags are included below the attentional weights.
    The stacked bar graphs are the total activation for the top-ten predicted model tags for the post.
    Each bar graph is partitioned into the 3 model components that vary between tags within a post (not the post offset).
}
  \label{fig:modelPost}
\end{figure}

Note how the base level activation and contextual activation combine to predict tags that are both frequently used and contextually relevant.
For example, note that the 'twitter' tag leverages a combination of contextual activation and prior activation to achieve a high total activation.
Alternatively, the 'twitter-4j' tag relies almost entirely on context to achieve high activation.
And note that the model is by no means perfect.
For example, the 'java' tag was used by the post author, and it does have a high prior activation, but context in this particular post was not associated too highly with java, so the model rank of java suffered.

\subsection{Logistic Regression}

In order to formally test model performance, and to calibrate model parameters, a calibration run using \num{1000} posts were performed.
For the calibration run, the category prediction (used/not-used tag for post) was regressed on base-level activation, spreading activation for title and body, and post offset.
Best-fit coefficients for each model component and formal statistical measurements are included in Table \ref{tab:coeffs}.

Note that all four coefficients for the run are strong predictors of tag use ($p<.01$), and model fit is satisfactory ($R_{pseudo}^{2}=FIXME$).
It's also interesting to note that although the attentional weighting term ($W$) was allowed to be calibrated by the regression, it did not stray far from one.
Apparently the scaled entropy measure that globally weights each cue allows for total attentional resources ($W$) to remain relatively small (around one) and not stray far from cognitively-plausible defaults (one).

A plot of model performance for the logistic regression for the calibration run is included in Figure \ref{fig:runCalLogReg}.
Not-tagged observed instances are collected on the bottom and tagged instances are on the top.
Looking at the histograms, note that model can not perfectly discriminate between tags that will and will not be used for a single post (overlap in distributions).
If further improvements to the model were found, these would increase the separation between the two histograms.

\subsection{Classification Accuracy}

In order to assess model performance in a dep

The McFadden $R_{pseudo}^{2}$ statistic was chosen as the primary overall measure of fit, mostly because it makes intuitive sense and also because it can discriminate between later variations of the model.
Model fit for these runs was marginal (FIXME), (FIXME) for run2.

Looking at model classification shows why the fit is only marginal.
The model is able to correctly predict when a tag will not be used (CR=FIXME) for run1, CR= for run2).
However, if the model is to ever be fielded as a classifier, it must also be able to predict when a tag will be used.
And in this situation the model does only marginal (Hits=FIXME for run1, Hits= FIXME for run2).
That is, of all of the tags used in the 1,000 post tests, the model is only able to correctly predict those tags in about 10\% of cases.

\section{Discussion}

\subsection{Related Work}
This researcher recently discovered that predicting tags for the StackOverflow dataset has been worked on by at least one prior researcher (\cite{Kuo2011}).
\cite{Kuo2011} only looks at the Stack Overflow dataset briefly at the end of the technical report.
However he does mention that the co-occurrence model he used has about a 47\% classification accuracy for predicting StackOverflow tags, although it is unclear how exactly this accuracy is calculated.
I am considering sending this researcher an email, as I'm curious to know exactly how he measured classification accuracy for his co-occurrence model.

\subsection{ACT-R and Scale}

\subsection{Entropy and Stop Words}

\subsection{Future Work}

\subsubsection{User's Table}

\section{Acknowledgments}

\bibliographystyle{apacite}

\setlength{\bibleftmargin}{.125in}
\setlength{\bibindent}{-\bibleftmargin}

\bibliography{bibliography}

\end{document}
